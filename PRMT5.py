# -*- coding: utf-8 -*-
"""Imran_cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xoL16PqbpDtfuYjz2iJKSiiZFM_froKD
"""

# @title ðŸ“¦ Upload your CSV
import io, re
import numpy as np
import pandas as pd

# Colab file upload widget
try:
    from google.colab import files
    uploaded = files.upload()  # choose your CSV file
    fname = list(uploaded.keys())[0]
    df = pd.read_csv(io.BytesIO(uploaded[fname]))
except Exception as e:
    raise SystemExit(f"Upload failed: {e}")

print("Loaded shape:", df.shape)
df.head()

# @title ðŸ”¬ Convert IC50 (nM) to pIC50 and add columns

# Config: your IC50 column is numeric and in nM
IC50_COL = "IC50"     # change if your column is named differently
UNIT = "nM"           # do not change unless your units change

# Basic checks
if IC50_COL not in df.columns:
    raise ValueError(f"Column '{IC50_COL}' not found. Columns: {list(df.columns)}")

# Coerce to numeric (handles stray text/NaN safely)
ic50_nm = pd.to_numeric(df[IC50_COL], errors="coerce")

# Convert nM -> M
ic50_m = ic50_nm * 1e-9

# pIC50 = -log10(IC50 in M); invalid (<=0 or NaN) -> NaN
with np.errstate(divide='ignore', invalid='ignore'):
    pic50 = -np.log10(ic50_m)

# Attach to dataframe
df["IC50_M"] = ic50_m
df["pIC50"] = pic50

# Quick report
n_total = len(df)
n_valid = df["pIC50"].notna().sum()
n_invalid = n_total - n_valid
print(f"âœ… pIC50 computed for {n_valid}/{n_total} rows. âš ï¸ {n_invalid} rows had missing/invalid IC50.")

display(df.head(10))

# @title âœ… Summaries & a few suspect rows

print("IC50 (nM) summary:")
display(ic50_nm.describe())

print("\nIC50 (M) summary:")
display(df["IC50_M"].describe())

print("\npIC50 summary:")
display(df["pIC50"].describe())

# Show any rows that couldn't be converted (NaN)
bad = df[df["pIC50"].isna()]
print(f"\nRows with invalid/missing IC50: {len(bad)}")
display(bad.head(10))

# @title ðŸ“ˆ pIC50 Histogram (optional)

import matplotlib.pyplot as plt

valid = df["pIC50"].dropna()
plt.figure()
plt.hist(valid, bins=40)
plt.xlabel("pIC50")
plt.ylabel("Count")
plt.title("Distribution of pIC50")
plt.show()

# @title ðŸ’¾ Save and download CSV with pIC50
out_name = "compounds_with_pIC50.csv"
df.to_csv(out_name, index=False)

try:
    from google.colab import files
    files.download(out_name)
except Exception as e:
    print(f"Saved to {out_name}. If download didn't start, check the Files panel. ({e})")

# @title ðŸ“¥ Load data and ensure pIC50 exists

# Try pIC50-ready file first; fallback to raw and compute pIC50 from IC50 (nM)
CANDIDATES = ["compounds_with_pIC50.csv", "compounds.csv"]

df = None
for name in CANDIDATES:
    try:
        df = pd.read_csv(name)
        print(f"Loaded: {name}  shape={df.shape}")
        break
    except:
        pass

if df is None:
    # Colab upload picker
    from google.colab import files
    uploaded = files.upload()
    fname = list(uploaded.keys())[0]
    df = pd.read_csv(fname)
    print(f"Loaded: {fname}  shape={df.shape}")

# If pIC50 missing but IC50 is present in nM, compute it
if "pIC50" not in df.columns:
    if "IC50" not in df.columns:
        raise ValueError("No pIC50 and no IC50 column found.")
    ic50_nm = pd.to_numeric(df["IC50"], errors="coerce")
    df["IC50_M"] = ic50_nm * 1e-9
    with np.errstate(divide='ignore', invalid='ignore'):
        df["pIC50"] = -np.log10(df["IC50_M"])
    print("Computed pIC50 from IC50 (assumed nM).")

# Clean column names (optional)
df.columns = [c.strip() for c in df.columns]

# Quick peek
display(df.head())
print(df[["pIC50"]].describe())

# @title ðŸ§ª Parse SMILES and drop invalids / duplicates

SMI_COL = "Smiles"  # change if needed
ID_COL  = "Molecule ChEMBL ID"  # change if needed

if SMI_COL not in df.columns:
    raise ValueError(f"'{SMI_COL}' column not found. Got: {list(df.columns)}")

# Drop exact duplicate molecules by ID (keep first)
if ID_COL in df.columns:
    before = len(df)
    df = df.drop_duplicates(subset=[ID_COL])
    print(f"Dedup by {ID_COL}: {before} -> {len(df)}")

# Parse SMILES
mols = []
valid_idx = []
for i, smi in enumerate(df[SMI_COL].astype(str)):
    mol = Chem.MolFromSmiles(smi)
    if mol is not None:
        mols.append(mol)
        valid_idx.append(i)

print(f"Valid SMILES: {len(valid_idx)}/{len(df)}")
df = df.iloc[valid_idx].copy().reset_index(drop=True)
mols = [mols[i] for i in range(len(mols))]  # align

# @title ðŸ§® Descriptor factory

def calc_descriptors(mol):
    # Core set (extend as needed)
    mw     = Descriptors.MolWt(mol)
    logp   = Crippen.MolLogP(mol)
    hbd    = Lipinski.NumHDonors(mol)
    hba    = Lipinski.NumHAcceptors(mol)
    tpsa   = rdMolDescriptors.CalcTPSA(mol)
    rotb   = Lipinski.NumRotatableBonds(mol)
    rings  = rdMolDescriptors.CalcNumRings(mol)
    ar_r   = rdMolDescriptors.CalcNumAromaticRings(mol)
    fsp3   = rdMolDescriptors.CalcFractionCSP3(mol)
    heavy  = mol.GetNumHeavyAtoms()
    qpos   = rdMolDescriptors.CalcNumPosibleStereoCenters(mol) if hasattr(rdMolDescriptors,"CalcNumPosibleStereoCenters") else np.nan

    return {
        "MW": mw,
        "LogP": logp,
        "HBD": hbd,
        "HBA": hba,
        "TPSA": tpsa,
        "RotB": rotb,
        "Rings": rings,
        "AromRings": ar_r,
        "Fsp3": fsp3,
        "HeavyAtoms": heavy
    }

desc_rows = [calc_descriptors(m) for m in mols]
desc_df = pd.DataFrame(desc_rows)

# Merge back
df_desc = pd.concat([df.reset_index(drop=True), desc_df], axis=1)
print(df_desc.shape)
display(df_desc.head())

# @title ðŸ·ï¸ Create labels under multiple schemes and compare distributions

pic = pd.to_numeric(df_desc["pIC50"], errors="coerce")

def label_scheme_A(x):
    if pd.isna(x): return np.nan
    if x >= 7.0:   return "Active"
    if x >= 6.0:   return "Intermediate"
    return "Inactive"

def label_scheme_B(x):
    if pd.isna(x): return np.nan
    return "Active" if x >= 7.0 else "Inactive"

def label_scheme_C(x):
    if pd.isna(x): return np.nan
    return "Active" if x >= 6.5 else "Inactive"

df_desc["Class_A"] = pic.apply(label_scheme_A)
df_desc["Class_B"] = pic.apply(label_scheme_B)
df_desc["Class_C"] = pic.apply(label_scheme_C)

def show_counts(name):
    print(f"\n{name} counts:")
    print(df_desc[name].value_counts(dropna=False))
    print()

for col in ["Class_A","Class_B","Class_C"]:
    show_counts(col)

# Choose default labeling for downstream QSAR
LABEL_COL = "Class_A"
print("Using LABEL_COL =", LABEL_COL)

# @title ðŸ’¾ Save and download activity label table (Scheme A/B/C)

from google.colab import files

# Keep only key columns
label_table = df_desc[[
    "Molecule ChEMBL ID",
    "Molecule Name",
    "Smiles",
    "pIC50",
    "Class_A",  # 3-class: Active, Intermediate, Inactive
    "Class_B",  # binary â‰¥7.0
    "Class_C"   # binary â‰¥6.5
]]

# Save to CSV
label_table.to_csv("activity_label_table.csv", index=False)
print("âœ… File saved as 'activity_label_table.csv'")

# Offer download in Colab
files.download("activity_label_table.csv")

# @title ðŸ“Š EDA: histograms and Lipinski profile

cols = ["MW","LogP","HBD","HBA","TPSA","RotB","Rings","AromRings","Fsp3","HeavyAtoms"]
df_valid = df_desc.dropna(subset=["pIC50"]).copy()

for c in ["pIC50"] + cols:
    plt.figure()
    df_valid[c].hist(bins=40)
    plt.title(c)
    plt.xlabel(c); plt.ylabel("Count")
    plt.show()

# Lipinski flags
df_valid["Lipinski_OK"] = (
    (df_valid["MW"] <= 500) &
    (df_valid["LogP"] <= 5) &
    (df_valid["HBD"] <= 5) &
    (df_valid["HBA"] <= 10)
)

print("\nLipinski compliance:")
print(df_valid["Lipinski_OK"].value_counts())
print("\nLipinski by class (Scheme A):")
display(pd.crosstab(df_valid[LABEL_COL], df_valid["Lipinski_OK"], margins=True))

# @title ðŸ“ˆ Structureâ€“activity scatterplots

pairs = [("MW","pIC50"), ("LogP","pIC50"), ("HBD","pIC50"), ("HBA","pIC50"), ("TPSA","pIC50")]
for x,y in pairs:
    plt.figure()
    plt.scatter(df_valid[x], df_valid[y], s=10, alpha=0.3)
    plt.xlabel(x); plt.ylabel(y)
    plt.title(f"{y} vs {x}")
    plt.show()

# Class-wise boxplots (simple)
def boxplot_by_class(feature):
    plt.figure()
    data = [df_valid[df_valid[LABEL_COL]=="Inactive"][feature],
            df_valid[df_valid[LABEL_COL]=="Intermediate"][feature],
            df_valid[df_valid[LABEL_COL]=="Active"][feature]]
    plt.boxplot(data, labels=["Inactive","Intermediate","Active"])
    plt.title(f"{feature} by {LABEL_COL}")
    plt.ylabel(feature)
    plt.show()

for feat in ["MW","LogP","HBD","HBA","TPSA","RotB","Fsp3"]:
    boxplot_by_class(feat)

!pip install rdkit

import pandas as pd, numpy as np, math, re

# Your files + short tags for prefixes
files = [
    ("/content/EState_filtered_data.csv",        "EState"),
    ("/content/KlekotaRoth_filtered_data.csv",   "KR"),
    ("/content/MACCS_filtered_data.csv",         "MACCS"),
    ("/content/Substructure_filtered_data.csv",  "Subst"),
    ("/content/pubchem_filtered_data.csv",       "PubChem"),
]

def normalize_cols(df):
    df = df.copy()
    rename = {}
    for c in df.columns:
        lc = c.lower().strip()
        if lc in ["molecule chembl id","molecule_chembl_id","chembl id","chembl_id"]:
            rename[c] = "chembl_id"
        elif lc in ["canonical_smiles","smiles"]:
            rename[c] = "smiles"
        elif lc in ["pic50","p_ic50","pic_50","pIC50".lower()]:
            rename[c] = "pIC50"
        elif lc in ["ic50","ic50_nm","standard_value"]:
            rename[c] = "IC50"
        elif lc in ["standard_units","units"]:
            rename[c] = "units"
    return df.rename(columns=rename)

# 1) Read all
raw = [(pd.read_csv(p), tag) for p,tag in files]
raw = [(normalize_cols(df), tag) for df,tag in raw]

# 2) Choose merge key once (prefer chembl_id else smiles)
merge_key = "chembl_id" if all("chembl_id" in df.columns for df,_ in raw) else "smiles"

# 3) Prepare the FIRST table: keep meta cols here only
meta_keep = ["chembl_id","smiles","pIC50","IC50","units"]
first_df, first_tag = raw[0]
first_cols = [c for c in first_df.columns if c in meta_keep]
# also keep any numeric features from the first file, with prefix
first_feats = [c for c in first_df.columns if c not in first_cols]
first_pref = first_df[first_cols].copy()
first_feat_pref = first_df[first_feats].add_prefix(f"{first_tag}_") if first_feats else pd.DataFrame(index=first_df.index)
base = pd.concat([first_pref, first_feat_pref], axis=1)

# 4) Add the remaining tables: DROP meta, prefix features, then merge
for df, tag in raw[1:]:
    # drop meta columns to avoid duplicates
    df2 = df.drop(columns=[c for c in df.columns if c in meta_keep and c != merge_key], errors="ignore")
    # separate features vs key
    feat_cols = [c for c in df2.columns if c != merge_key]
    df_pref = pd.concat([df2[[merge_key]],
                         df2[feat_cols].add_prefix(f"{tag}_") if feat_cols else df2.drop(columns=[merge_key])],
                        axis=1)
    base = base.merge(df_pref, on=merge_key, how="outer")

master = base.copy()

# 5) pIC50: compute if missing, preferring provided pIC50 in master
def to_pIC50_row(row):
    if "pIC50" in row and pd.notna(row["pIC50"]):
        try: return float(row["pIC50"])
        except: pass
    if "IC50" in row and pd.notna(row["IC50"]):
        s = re.sub(r"[<>â‰ˆ~]","",str(row["IC50"]))
        try:
            ic50_nm = float(s)
            return 9.0 - np.log10(ic50_nm) if ic50_nm>0 else np.nan
        except:
            return np.nan
    return np.nan

if "pIC50" not in master.columns:
    master["pIC50"] = master.apply(to_pIC50_row, axis=1)
else:
    master["pIC50"] = pd.to_numeric(master["pIC50"], errors="coerce")

# 6) Clean
if "smiles" in master.columns:
    master = master.dropna(subset=["smiles"])
master = master.dropna(subset=["pIC50"]).drop_duplicates(subset=[merge_key]).reset_index(drop=True)

print("Merged shape:", master.shape)
print("Columns (first 15):", master.columns[:15].tolist())
master.head(3)

from rdkit import Chem
from rdkit.Chem.Scaffolds import MurckoScaffold
import numpy as np

def murcko(smi):
    m = Chem.MolFromSmiles(str(smi))
    if m is None: return None
    core = MurckoScaffold.GetScaffoldForMol(m)
    return Chem.MolToSmiles(core) if core else None

if "smiles" not in master.columns:
    raise ValueError("SMILES column is required for scaffold split. Add a 'smiles' column and rerun.")

master["scaffold"] = master["smiles"].map(murcko)

rng = np.random.default_rng(42)
uniq_scf = master["scaffold"].dropna().unique()
test_scf = set(rng.choice(uniq_scf, size=max(1,int(0.2*len(uniq_scf))), replace=False))
is_test = master["scaffold"].isin(test_scf)

train_df = master.loc[~is_test].copy()
test_df  = master.loc[is_test].copy()

print("TRAIN/TEST sizes:", train_df.shape, test_df.shape)
train_df.to_csv("/content/PRMT5_train_split.csv", index=False)
test_df.to_csv("/content/PRMT5_test_split.csv", index=False)
print("Saved: /content/PRMT5_train_split.csv  and  /content/PRMT5_test_split.csv")

# --- Make aligned 80/20 scaffold splits for ALL 5 descriptor CSVs ---

!pip -q install rdkit-pypi
import pandas as pd, numpy as np, re, math
from rdkit import Chem
from rdkit.Chem.Scaffolds import MurckoScaffold

# 1) Your files + short tags
files = [
    ("/content/EState_filtered_data.csv",        "EState"),
    ("/content/KlekotaRoth_filtered_data.csv",   "KR"),
    ("/content/MACCS_filtered_data.csv",         "MACCS"),
    ("/content/Substructure_filtered_data.csv",  "Subst"),
    ("/content/pubchem_filtered_data.csv",       "PubChem"),
]

def normalize_cols(df):
    df = df.copy()
    ren = {}
    for c in df.columns:
        lc = c.lower().strip()
        if lc in ["molecule chembl id","molecule_chembl_id","chembl id","chembl_id"]:
            ren[c] = "chembl_id"
        elif lc in ["canonical_smiles","smiles"]:
            ren[c] = "smiles"
        elif lc in ["pic50","p_ic50","pic_50","pIC50".lower()]:
            ren[c] = "pIC50"
        elif lc in ["ic50","ic50_nm","standard_value"]:
            ren[c] = "IC50"
        elif lc in ["standard_units","units"]:
            ren[c] = "units"
    df = df.rename(columns=ren)
    return df

def to_pIC50_row(row):
    # prefer provided pIC50; else compute from IC50 (assumed nM)
    val = row.get("pIC50", np.nan)
    if pd.notna(val):
        try: return float(val)
        except: pass
    ic = row.get("IC50", np.nan)
    if pd.notna(ic):
        s = re.sub(r"[<>â‰ˆ~]","", str(ic))
        try:
            ic50_nm = float(s)
            return 9.0 - np.log10(ic50_nm) if ic50_nm > 0 else np.nan
        except:
            return np.nan
    return np.nan

def load_one(path):
    df = pd.read_csv(path)
    df = normalize_cols(df)
    # ensure key columns exist
    if "chembl_id" not in df.columns and "smiles" not in df.columns:
        raise ValueError(f"{path} must contain chembl_id or smiles")
    if "pIC50" not in df.columns and "IC50" not in df.columns:
        raise ValueError(f"{path} must contain pIC50 or IC50")
    # compute/use pIC50
    if "pIC50" not in df.columns:
        df["pIC50"] = df.apply(to_pIC50_row, axis=1)
    else:
        df["pIC50"] = pd.to_numeric(df["pIC50"], errors="coerce")
    # keep essentials + all numeric features
    if "chembl_id" in df.columns:
        key = "chembl_id"
    else:
        key = "smiles"
    # drop broken rows
    keep = [key, "smiles", "pIC50"] if "smiles" in df.columns else [key, "pIC50"]
    meta = df[keep].copy()
    feats = df.drop(columns=[c for c in keep if c in df.columns], errors="ignore")
    # keep only numeric feature cols
    feat_cols = [c for c in feats.columns if pd.api.types.is_numeric_dtype(feats[c])]
    feats = feats[feat_cols].copy()
    return key, meta, feats

# 2) Load all and enforce a common key across files
loaded = [load_one(p) for p,_ in files]
# determine the merge key to align all sets: prefer chembl_id if present everywhere, else smiles
keys_present = [k for k,_,_ in loaded]
use_chembl = all(k == "chembl_id" for k in keys_present)
use_smiles = all(k == "smiles" for k in keys_present)
if use_chembl:
    merge_key = "chembl_id"
elif use_smiles:
    merge_key = "smiles"
else:
    # mix of keys; try to build a crosswalk via smiles
    merge_key = "chembl_id" if any(k=="chembl_id" for k in keys_present) else "smiles"

# 3) Build intersection of compounds present in ALL files
def id_set(meta):
    if merge_key in meta.columns:
        return set(meta[merge_key].dropna().astype(str))
    else:
        # fall back to smiles
        return set(meta["smiles"].dropna().astype(str))

id_sets = []
for ( (k,meta,feats), (_,tag) ) in zip(loaded, files):
    ids = id_set(meta)
    id_sets.append(ids)
common_ids = set.intersection(*id_sets)
if len(common_ids) == 0:
    raise ValueError("No common compounds across all five files. Check IDs/SMILES consistency.")

# 4) Build a unified table with SMILES for scaffolds (priority: any meta that has smiles)
# pick the first meta with smiles
unified = None
for (k,meta,_) ,(_,tag) in zip(loaded, files):
    if "smiles" in meta.columns:
        tmp = meta.loc[meta[merge_key].astype(str).isin(common_ids), [merge_key, "smiles"]].dropna()
        unified = tmp if unified is None else pd.concat([unified, tmp]).drop_duplicates(subset=[merge_key])
if unified is None or unified.empty:
    raise ValueError("Could not gather SMILES to compute scaffolds; ensure at least one file has SMILES.")

# 5) Compute Murcko scaffold and make the 80/20 split ONCE
def murcko(smi):
    m = Chem.MolFromSmiles(str(smi))
    if not m: return None
    core = MurckoScaffold.GetScaffoldForMol(m)
    return Chem.MolToSmiles(core) if core else None

unified["scaffold"] = unified["smiles"].map(murcko)
uniq_scf = unified["scaffold"].dropna().unique()
rng = np.random.default_rng(42)
test_scf = set(rng.choice(uniq_scf, size=max(1,int(0.2*len(uniq_scf))), replace=False))

unified["split"] = np.where(unified["scaffold"].isin(test_scf), "TEST", "TRAIN")
split_map = unified[[merge_key, "split"]].copy()

# Save the split map (for traceability)
split_map.to_csv("/content/PRMT5_scaffold_split_map.csv", index=False)

# 6) Apply the same split to EACH descriptor set and save train/test per file
for ( (k,meta,feats), (path, tag) ) in zip(loaded, files):
    meta = meta.loc[meta[merge_key].astype(str).isin(common_ids)].copy()
    feats = feats.loc[meta.index].reset_index(drop=True)
    meta = meta.reset_index(drop=True)
    # attach split
    meta[merge_key] = meta[merge_key].astype(str)
    m = meta.merge(split_map, on=merge_key, how="left")
    if m["split"].isna().any():
        # if any NA (e.g., missing smiles scaffold), default to TRAIN
        m["split"] = m["split"].fillna("TRAIN")
    # concat back meta+features for saving
    df_full = pd.concat([m, feats], axis=1)
    train_df = df_full[df_full["split"]=="TRAIN"].drop(columns=["split"])
    test_df  = df_full[df_full["split"]=="TEST"].drop(columns=["split"])
    # write
    tr_path = f"/content/{tag}_train_split.csv"
    te_path = f"/content/{tag}_test_split.csv"
    train_df.to_csv(tr_path, index=False)
    test_df.to_csv(te_path, index=False)
    print(f"[{tag}] TRAIN {train_df.shape}  TEST {test_df.shape}  ->  {tr_path} , {te_path}")

print("\nSaved global split map: /content/PRMT5_scaffold_split_map.csv")
print("Use the per-descriptor *_train_split.csv and *_test_split.csv for modeling.")

!pip install -U scikit-learn
import sklearn
print("scikit-learn version:", sklearn.__version__)

# ====== Step 4: QSAR baseline models for each descriptor ======
import pandas as pd, numpy as np, os
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr
from sklearn.ensemble import RandomForestRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

np.random.seed(42)
TAGS = ["EState","KR","MACCS","Subst","PubChem"]  # descriptor tags

# ---- helper to load each train/test split ----
def load_split(tag):
    tr = pd.read_csv(f"/content/{tag}_train_split.csv")
    te = pd.read_csv(f"/content/{tag}_test_split.csv")
    key = "chembl_id" if "chembl_id" in tr.columns else ("smiles" if "smiles" in tr.columns else None)
    if key is None: raise ValueError(f"{tag}: need chembl_id or smiles")
    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]  # align columns
    Xtr = tr[feat_cols].fillna(0.0).astype(float).values
    ytr = tr["pIC50"].astype(float).values
    Xte = te[feat_cols].fillna(0.0).astype(float).values
    yte = te["pIC50"].astype(float).values
    meta_tr = tr[[key,"pIC50"]].copy()
    meta_te = te[[key,"pIC50"]].copy()
    return feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key, tr

def cont_cols_from_df(df, feat_cols):
    cont = []
    for c in feat_cols:
        s = df[c]
        try:
            if s.nunique()>20 and (s.max()-s.min())>5:
                cont.append(c)
        except: pass
    return cont

# ---- main loop ----
os.makedirs("/content/qsar_outputs", exist_ok=True)
all_metrics = []

for tag in TAGS:
    feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key, tr_df = load_split(tag)

    # identify continuous columns for scaling
    cont = cont_cols_from_df(tr_df, feat_cols)
    passthrough = [c for c in feat_cols if c not in cont]
    pre = ColumnTransformer([
        ("scale", StandardScaler(with_mean=False), [feat_cols.index(c) for c in cont]),
        ("pass", "passthrough", [feat_cols.index(c) for c in passthrough])
    ])

    rf = RandomForestRegressor(
        n_estimators=600, max_depth=None, min_samples_leaf=1,
        random_state=42, n_jobs=-1
    )
    pipe = Pipeline([("prep", pre), ("rf", rf)])

    # ---- 5-fold CV on TRAIN ----
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = []
    for tr_idx, va_idx in kf.split(Xtr):
        pipe.fit(Xtr[tr_idx], ytr[tr_idx])
        pv = pipe.predict(Xtr[va_idx])
        rmse = mean_squared_error(ytr[va_idx], pv) ** 0.5
        mae  = mean_absolute_error(ytr[va_idx], pv)
        r2   = r2_score(ytr[va_idx], pv)
        rho,_= spearmanr(ytr[va_idx], pv)
        cv_scores.append((rmse, mae, r2, rho))

    cv_rmse = float(np.mean([s[0] for s in cv_scores]))
    cv_mae  = float(np.mean([s[1] for s in cv_scores]))
    cv_r2   = float(np.mean([s[2] for s in cv_scores]))
    cv_rho  = float(np.mean([s[3] for s in cv_scores]))

    # ---- Final model on full TRAIN, evaluate on TEST ----
    pipe.fit(Xtr, ytr)
    pred_test = pipe.predict(Xte)
    te_rmse = mean_squared_error(yte, pred_test) ** 0.5
    te_mae  = mean_absolute_error(yte, pred_test)
    te_r2   = r2_score(yte, pred_test)
    te_rho,_= spearmanr(yte, pred_test)

    # save predictions
    out = meta_te.copy()
    out["pred_pIC50"] = pred_test
    out_path = f"/content/qsar_outputs/{tag}_test_predictions.csv"
    out.to_csv(out_path, index=False)

    # log metrics
    all_metrics.append({
        "descriptor": tag,
        "n_train": len(ytr),
        "n_test": len(yte),
        "CV_RMSE": round(cv_rmse,3),
        "CV_MAE":  round(cv_mae,3),
        "CV_R2":   round(cv_r2,3),
        "CV_Spearman": round(cv_rho,3),
        "TEST_RMSE": round(te_rmse,3),
        "TEST_MAE":  round(te_mae,3),
        "TEST_R2":   round(te_r2,3),
        "TEST_Spearman": round(te_rho,3),
        "pred_path": out_path
    })
    print(f"[{tag}] CV(RMSE={cv_rmse:.3f}, R2={cv_r2:.3f})  "
          f"TEST(RMSE={te_rmse:.3f}, R2={te_r2:.3f})  -> {out_path}")

# ---- Save metrics summary ----
metrics_df = pd.DataFrame(all_metrics)
metrics_path = "/content/qsar_outputs/descriptor_metrics.csv"
metrics_df.to_csv(metrics_path, index=False)
print("\nSaved metrics table:", metrics_path)
display(metrics_df)

!pip -q install xgboost
import pandas as pd, numpy as np, os
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

np.random.seed(42)
TAGS = ["EState","KR","MACCS","Subst","PubChem"]

# --- helper to load one split ---
def load_split(tag):
    tr = pd.read_csv(f"/content/{tag}_train_split.csv")
    te = pd.read_csv(f"/content/{tag}_test_split.csv")
    key = "chembl_id" if "chembl_id" in tr.columns else "smiles"
    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]
    Xtr, Xte = tr[feat_cols].fillna(0.0), te[feat_cols].fillna(0.0)
    ytr, yte = tr["pIC50"].astype(float).values, te["pIC50"].astype(float).values
    return Xtr, ytr, Xte, yte, feat_cols

# --- models dictionary ---
models = {
    "RF":  RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "SVR": SVR(kernel="rbf", C=10, epsilon=0.1),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "XGB": XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6,
                        subsample=0.8, colsample_bytree=0.8, random_state=42,
                        n_jobs=-1, objective="reg:squarederror")
}

results=[]
os.makedirs("/content/qsar_algorithms", exist_ok=True)

for tag in TAGS:
    Xtr, ytr, Xte, yte, feat_cols = load_split(tag)
    # scale numeric features for non-tree models
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    for name, model in models.items():
        Xtrain = Xtr.values if name=="RF" else Xtr_s
        Xtest  = Xte.values if name=="RF" else Xte_s

        # 5-fold CV on train
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_r2, cv_rmse = [], []
        for tr_i, va_i in kf.split(Xtrain):
            model.fit(Xtrain[tr_i], ytr[tr_i])
            pv = model.predict(Xtrain[va_i])
            cv_r2.append(r2_score(ytr[va_i], pv))
            cv_rmse.append(mean_squared_error(ytr[va_i], pv)**0.5)
        mean_cv_r2, mean_cv_rmse = np.mean(cv_r2), np.mean(cv_rmse)

        # Train + Test evaluation
        model.fit(Xtrain, ytr)
        pred_train = model.predict(Xtrain)
        pred_test  = model.predict(Xtest)
        train_r2 = r2_score(ytr, pred_train)
        test_r2  = r2_score(yte, pred_test)
        test_rmse = mean_squared_error(yte, pred_test)**0.5
        test_mae  = mean_absolute_error(yte, pred_test)
        test_rho,_= spearmanr(yte, pred_test)

        results.append({
            "Descriptor": tag, "Algorithm": name,
            "Train_R2": round(train_r2,3),
            "CV_R2": round(mean_cv_r2,3),
            "CV_RMSE": round(mean_cv_rmse,3),
            "Test_R2": round(test_r2,3),
            "Test_RMSE": round(test_rmse,3),
            "Test_MAE": round(test_mae,3),
            "Test_Spearman": round(test_rho,3)
        })
        print(f"{tag}-{name}: TrainR2={train_r2:.3f}  CV_R2={mean_cv_r2:.3f}  "
              f"TestR2={test_r2:.3f}  TestRMSE={test_rmse:.3f}")

df_res = pd.DataFrame(results)
df_res.to_csv("/content/qsar_algorithms/algorithm_comparison.csv", index=False)
print("\nSaved: /content/qsar_algorithms/algorithm_comparison.csv")
display(df_res)

import numpy as np, pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr

df = pd.read_csv("/content/PubChem_train_split.csv")  # or merge train+test first
X = df.drop(columns=["chembl_id","smiles","pIC50","IC50","units","scaffold"], errors="ignore").fillna(0)
y = df["pIC50"].astype(float).values

r2_list, rmse_list, rho_list = [], [], []
for i in range(10):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42+i
    )
    model = XGBRegressor(
        n_estimators=600, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8,
        random_state=42+i, objective="reg:squarederror", n_jobs=-1)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    r2 = r2_score(y_test, pred)
    rmse = mean_squared_error(y_test, pred) ** 0.5
    rho,_ = spearmanr(y_test, pred)
    r2_list.append(r2); rmse_list.append(rmse); rho_list.append(rho)

print(f"Mean Test RÂ²: {np.mean(r2_list):.3f} Â± {np.std(r2_list):.3f}")
print(f"Mean RMSE:    {np.mean(rmse_list):.3f} Â± {np.std(rmse_list):.3f}")
print(f"Mean Spearman:{np.mean(rho_list):.3f} Â± {np.std(rho_list):.3f}")

# --- Robust stratified 80:20 split with auto bin selection ---
import pandas as pd, numpy as np
from sklearn.model_selection import train_test_split

# INPUTS: same as before
files = [
    ("/content/EState_filtered_data.csv",        "EState"),
    ("/content/KlekotaRoth_filtered_data.csv",   "KR"),
    ("/content/MACCS_filtered_data.csv",         "MACCS"),
    ("/content/Substructure_filtered_data.csv",  "Subst"),
    ("/content/pubchem_filtered_data.csv",       "PubChem"),
]

def normalize_cols(df):
    df = df.copy()
    ren = {}
    for c in df.columns:
        lc = c.lower().strip()
        if lc in ["molecule chembl id","molecule_chembl_id","chembl id","chembl_id"]:
            ren[c] = "chembl_id"
        elif lc in ["canonical_smiles","smiles"]:
            ren[c] = "smiles"
        elif lc in ["pic50","p_ic50","pic_50"]:
            ren[c] = "pIC50"
        elif lc in ["ic50","ic50_nm","standard_value"]:
            ren[c] = "IC50"
        elif lc in ["standard_units","units"]:
            ren[c] = "units"
    return df.rename(columns=ren)

def compute_pIC50(row):
    if "pIC50" in row and pd.notna(row["pIC50"]):
        try: return float(row["pIC50"])
        except: pass
    if "IC50" in row and pd.notna(row["IC50"]):
        try:
            ic50_nm = float(str(row["IC50"]).replace(">","").replace("<",""))
            return 9 - np.log10(ic50_nm) if ic50_nm > 0 else np.nan
        except:
            return np.nan
    return np.nan

# 1) Load/standardize
dfs = []
for path, tag in files:
    df = pd.read_csv(path)
    df = normalize_cols(df)
    if "pIC50" not in df.columns and "IC50" in df.columns:
        df["pIC50"] = df.apply(compute_pIC50, axis=1)
    df["pIC50"] = pd.to_numeric(df["pIC50"], errors="coerce")
    df = df.dropna(subset=["pIC50"])
    dfs.append((path, tag, df))

# 2) Common IDs across all five
key = "chembl_id" if all("chembl_id" in d.columns for _,_,d in dfs) else "smiles"
common_ids = set.intersection(*[set(d[key].astype(str)) for _,_,d in dfs])
print("Common compounds across all 5 descriptor sets:", len(common_ids))

# 3) Meta table (ID + pIC50) for stratification
meta = dfs[0][2].loc[dfs[0][2][key].astype(str).isin(common_ids), [key, "pIC50"]].copy()
meta[key] = meta[key].astype(str)

def make_strata(y, max_bins=6, min_count=2):
    # Try q=6 down to 3; require >= min_count per bin
    for q in range(max_bins, 2, -1):
        try:
            # qcut with duplicates='drop' avoids identical edges
            labels = pd.qcut(y, q=q, labels=False, duplicates='drop')
            # ensure at least 3 bins exist after dropping duplicates
            if labels.nunique() < 3:
                continue
            counts = labels.value_counts()
            if counts.min() >= min_count:
                return labels.astype(int), q
        except Exception:
            continue
    return None, None

strata, q_used = make_strata(meta["pIC50"].values, max_bins=6, min_count=2)

if strata is not None:
    print(f"Using stratified split with {strata.nunique()} quantile bins (requested up to {q_used}).")
    train_ids, test_ids = train_test_split(
        meta[key], test_size=0.2, random_state=42, stratify=strata
    )
else:
    print("âš ï¸ Could not form safe stratification bins; falling back to plain random split (80/20).")
    train_ids, test_ids = train_test_split(
        meta[key], test_size=0.2, random_state=42, shuffle=True
    )

split_map = pd.DataFrame({key: meta[key], "split": "TRAIN"})
split_map.loc[split_map[key].isin(test_ids), "split"] = "TEST"
split_map.to_csv("/content/PRMT5_stratified_split_map.csv", index=False)
print("Saved split map: /content/PRMT5_stratified_split_map.csv")

# 4) Apply split to each descriptor and save
for path, tag, df in dfs:
    df[key] = df[key].astype(str)
    df2 = df.merge(split_map, on=key, how="inner")
    train_df = df2[df2["split"]=="TRAIN"].drop(columns=["split"])
    test_df  = df2[df2["split"]=="TEST"].drop(columns=["split"])
    trp = f"/content/{tag}_train_stratified.csv"
    tep = f"/content/{tag}_test_stratified.csv"
    train_df.to_csv(trp, index=False)
    test_df.to_csv(tep, index=False)
    print(f"[{tag}] TRAIN {train_df.shape}  TEST {test_df.shape}  -> {trp} , {tep}")

print("\nâœ… Done. You now have new (different from scaffold) TRAIN/TEST files for all descriptors.")

!pip -q install xgboost
import pandas as pd, numpy as np, os
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

np.random.seed(42)
TAGS = ["EState","KR","MACCS","Subst","PubChem"]

# --- helper to load one split ---
def load_split(tag):
    tr = pd.read_csv(f"/content/{tag}_train_stratified.csv")
    te = pd.read_csv(f"/content/{tag}_test_stratified.csv")
    key = "chembl_id" if "chembl_id" in tr.columns else "smiles"
    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]
    Xtr, Xte = tr[feat_cols].fillna(0.0), te[feat_cols].fillna(0.0)
    ytr, yte = tr["pIC50"].astype(float).values, te["pIC50"].astype(float).values
    return Xtr, ytr, Xte, yte, feat_cols

# --- models dictionary ---
models = {
    "RF":  RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "SVR": SVR(kernel="rbf", C=10, epsilon=0.1),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "XGB": XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6,
                        subsample=0.8, colsample_bytree=0.8, random_state=42,
                        n_jobs=-1, objective="reg:squarederror")
}

results=[]
os.makedirs("/content/qsar_algorithms", exist_ok=True)

for tag in TAGS:
    Xtr, ytr, Xte, yte, feat_cols = load_split(tag)
    # scale numeric features for non-tree models
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    for name, model in models.items():
        Xtrain = Xtr.values if name=="RF" else Xtr_s
        Xtest  = Xte.values if name=="RF" else Xte_s

        # 5-fold CV on train
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_r2, cv_rmse = [], []
        for tr_i, va_i in kf.split(Xtrain):
            model.fit(Xtrain[tr_i], ytr[tr_i])
            pv = model.predict(Xtrain[va_i])
            cv_r2.append(r2_score(ytr[va_i], pv))
            cv_rmse.append(mean_squared_error(ytr[va_i], pv)**0.5)
        mean_cv_r2, mean_cv_rmse = np.mean(cv_r2), np.mean(cv_rmse)

        # Train + Test evaluation
        model.fit(Xtrain, ytr)
        pred_train = model.predict(Xtrain)
        pred_test  = model.predict(Xtest)
        train_r2 = r2_score(ytr, pred_train)
        test_r2  = r2_score(yte, pred_test)
        test_rmse = mean_squared_error(yte, pred_test)**0.5
        test_mae  = mean_absolute_error(yte, pred_test)
        test_rho,_= spearmanr(yte, pred_test)

        results.append({
            "Descriptor": tag, "Algorithm": name,
            "Train_R2": round(train_r2,3),
            "CV_R2": round(mean_cv_r2,3),
            "CV_RMSE": round(mean_cv_rmse,3),
            "Test_R2": round(test_r2,3),
            "Test_RMSE": round(test_rmse,3),
            "Test_MAE": round(test_mae,3),
            "Test_Spearman": round(test_rho,3)
        })
        print(f"{tag}-{name}: TrainR2={train_r2:.3f}  CV_R2={mean_cv_r2:.3f}  "
              f"TestR2={test_r2:.3f}  TestRMSE={test_rmse:.3f}")

df_res = pd.DataFrame(results)
df_res.to_csv("/content/qsar_algorithms/algorithm_comparison.csv", index=False)
print("\nSaved: /content/qsar_algorithms/algorithm_comparison.csv")
display(df_res)

!pip -q install xgboost
import pandas as pd, numpy as np, os
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr, pearsonr
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler

np.random.seed(42)

# Use the stratified split files you just created
TAGS = ["EState","KR","MACCS","Subst","PubChem"]
TRAIN_FMT = "/content/{}_train_stratified.csv"
TEST_FMT  = "/content/{}_test_stratified.csv"

def load_split(tag):
    tr = pd.read_csv(TRAIN_FMT.format(tag))
    te = pd.read_csv(TEST_FMT.format(tag))
    key = "chembl_id" if "chembl_id" in tr.columns else ("smiles" if "smiles" in tr.columns else None)
    if key is None:
        raise ValueError(f"{tag}: need 'chembl_id' or 'smiles' column.")
    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold","activity_bin","split"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]  # align columns
    Xtr = tr[feat_cols].fillna(0.0).astype(float).values
    ytr = tr["pIC50"].astype(float).values
    Xte = te[feat_cols].fillna(0.0).astype(float).values
    yte = te["pIC50"].astype(float).values
    meta_te = te[[key,"pIC50"]] if key in te.columns else te[["pIC50"]]
    return feat_cols, Xtr, ytr, Xte, yte, meta_te, key

models = {
    "RF":  RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "SVR": SVR(kernel="rbf", C=10, epsilon=0.1),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "XGB": XGBRegressor(
        n_estimators=500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, random_state=42,
        n_jobs=-1, objective="reg:squarederror"
    )
}

results = []
os.makedirs("/content/qsar_algorithms_stratified", exist_ok=True)

for tag in TAGS:
    feat_cols, Xtr, ytr, Xte, yte, meta_te, key = load_split(tag)

    # Scale for non-tree models (keep sparse/binary scale-friendly)
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    for name, model in models.items():
        # Trees use raw; SVR/Ridge/XGB are fine with scaled (XGB also fine raw; keeping consistent)
        use_scaled = (name in ["SVR","Ridge"])
        Xtrain = Xtr_s if use_scaled else Xtr
        Xtest  = Xte_s if use_scaled else Xte

        # 5-fold CV on TRAIN
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_r2, cv_rmse = [], []
        for tr_i, va_i in kf.split(Xtrain):
            model.fit(Xtrain[tr_i], ytr[tr_i])
            pv = model.predict(Xtrain[va_i])
            cv_r2.append(r2_score(ytr[va_i], pv))
            cv_rmse.append(mean_squared_error(ytr[va_i], pv)**0.5)
        mean_cv_r2, mean_cv_rmse = float(np.mean(cv_r2)), float(np.mean(cv_rmse))

        # Train on full TRAIN, evaluate on TEST
        model.fit(Xtrain, ytr)
        pred_tr = model.predict(Xtrain)
        pred_te = model.predict(Xtest)

        train_r2 = r2_score(ytr, pred_tr)
        test_r2  = r2_score(yte, pred_te)
        test_rmse = mean_squared_error(yte, pred_te)**0.5
        test_mae  = mean_absolute_error(yte, pred_te)
        test_spear,_ = spearmanr(yte, pred_te)
        test_pear,_  = pearsonr(yte, pred_te)

        # save test predictions
        out = meta_te.copy()
        out = out.rename(columns={"pIC50":"true_pIC50"})
        out["pred_pIC50"] = pred_te
        out_path = f"/content/qsar_algorithms_stratified/{tag}_{name}_test_predictions.csv"
        out.to_csv(out_path, index=False)

        results.append({
            "Descriptor": tag, "Algorithm": name,
            "n_train": len(ytr), "n_test": len(yte),
            "Train_R2": round(train_r2,3),
            "CV_R2": round(mean_cv_r2,3),
            "CV_RMSE": round(mean_cv_rmse,3),
            "Test_R2": round(test_r2,3),
            "Test_RMSE": round(test_rmse,3),
            "Test_MAE": round(test_mae,3),
            "Test_Spearman": round(test_spear,3),
            "Test_Pearson": round(test_pear,3),
            "pred_path": out_path
        })
        print(f"{tag}-{name}: TrainR2={train_r2:.3f}  CV_R2={mean_cv_r2:.3f}  "
              f"TestR2={test_r2:.3f}  TestRMSE={test_rmse:.3f}")

df_res = pd.DataFrame(results)
summary_path = "/content/qsar_algorithms_stratified/algorithm_comparison_stratified.csv"
df_res.to_csv(summary_path, index=False)
print("\nSaved summary:", summary_path)
display(df_res.sort_values(["Descriptor","Test_R2"], ascending=[True, False]))

# ===== Multi-algorithm QSAR with residuals for TRAIN, TEST, and FULL (stratified splits) =====
!pip -q install xgboost
import pandas as pd, numpy as np, os
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr, pearsonr
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler

np.random.seed(42)

# Use the stratified split files created earlier
TAGS = ["EState","KR","MACCS","Subst","PubChem"]
TRAIN_FMT = "/content/{}_train_stratified.csv"
TEST_FMT  = "/content/{}_test_stratified.csv"

# Models to benchmark
models = {
    "RF":  RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "SVR": SVR(kernel="rbf", C=10, epsilon=0.1),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "XGB": XGBRegressor(
        n_estimators=500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, random_state=42,
        n_jobs=-1, objective="reg:squarederror"
    )
}

# Helper: load one descriptor's split and return features/labels/meta
def load_split(tag):
    tr = pd.read_csv(TRAIN_FMT.format(tag))
    te = pd.read_csv(TEST_FMT.format(tag))

    key = "chembl_id" if "chembl_id" in tr.columns else ("smiles" if "smiles" in tr.columns else None)
    if key is None:
        raise ValueError(f"{tag}: need an ID column ('chembl_id' or 'smiles').")

    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold","activity_bin","split"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]  # align

    Xtr = tr[feat_cols].fillna(0.0).astype(float).values
    ytr = tr["pIC50"].astype(float).values
    Xte = te[feat_cols].fillna(0.0).astype(float).values
    yte = te["pIC50"].astype(float).values

    # meta frames for saving predictions with IDs (and smiles if present)
    meta_cols = [key, "pIC50"] + (["smiles"] if "smiles" in tr.columns and key!="smiles" else [])
    meta_tr = tr[[c for c in meta_cols if c in tr.columns]].copy().rename(columns={"pIC50":"true_pIC50"})
    meta_te = te[[c for c in meta_cols if c in te.columns]].copy().rename(columns={"pIC50":"true_pIC50"})
    return feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key

# Helper: quick metric pack + residual coverage
def metric_pack(y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred)**0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    rho,_= spearmanr(y_true, y_pred)
    try:
        pr,_ = pearsonr(y_true, y_pred)
    except Exception:
        pr = np.nan
    resid = y_pred - y_true
    absr  = np.abs(resid)
    within_03 = float(np.mean(absr <= 0.3))
    within_05 = float(np.mean(absr <= 0.5))
    within_10 = float(np.mean(absr <= 1.0))
    return dict(RMSE=rmse, MAE=mae, R2=r2, Spearman=rho, Pearson=pr,
                Within_0p3=within_03, Within_0p5=within_05, Within_1p0=within_10)

# Output folder
outdir = "/content/qsar_algorithms_stratified_with_residuals"
os.makedirs(outdir, exist_ok=True)

all_rows = []

for tag in TAGS:
    feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key = load_split(tag)

    # Scale for SVR/Ridge only (trees and XGB use raw)
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    for name, model in models.items():
        use_scaled = (name in ["SVR", "Ridge"])
        Xtrain = Xtr_s if use_scaled else Xtr
        Xtest  = Xte_s if use_scaled else Xte

        # ---- 5-fold CV on TRAIN (for reporting) ----
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_r2, cv_rmse = [], []
        for tr_i, va_i in kf.split(Xtrain):
            model.fit(Xtrain[tr_i], ytr[tr_i])
            pv = model.predict(Xtrain[va_i])
            cv_r2.append(r2_score(ytr[va_i], pv))
            cv_rmse.append(mean_squared_error(ytr[va_i], pv)**0.5)
        mean_cv_r2, mean_cv_rmse = float(np.mean(cv_r2)), float(np.mean(cv_rmse))

        # ---- Fit on full TRAIN and predict TRAIN + TEST ----
        model.fit(Xtrain, ytr)
        pred_tr = model.predict(Xtrain)
        pred_te = model.predict(Xtest)

        # ---- Metrics ----
        train_metrics = metric_pack(ytr, pred_tr)
        test_metrics  = metric_pack(yte, pred_te)

        # ---- Save TRAIN predictions with residuals ----
        train_out = meta_tr.copy()
        train_out["pred_pIC50"] = pred_tr
        train_out["residual"] = train_out["pred_pIC50"] - train_out["true_pIC50"]
        train_out["abs_residual"] = train_out["residual"].abs()
        train_path = f"{outdir}/{tag}_{name}_TRAIN_predictions.csv"
        train_out.to_csv(train_path, index=False)

        # ---- Save TEST predictions with residuals ----
        test_out = meta_te.copy()
        test_out["pred_pIC50"] = pred_te
        test_out["residual"] = test_out["pred_pIC50"] - test_out["true_pIC50"]
        test_out["abs_residual"] = test_out["residual"].abs()
        test_path = f"{outdir}/{tag}_{name}_TEST_predictions.csv"
        test_out.to_csv(test_path, index=False)

        # ---- Save FULL predictions (TRAIN+TEST) with residuals ----
        full_out = pd.concat([train_out.assign(split="TRAIN"),
                              test_out.assign(split="TEST")], ignore_index=True)
        full_path = f"{outdir}/{tag}_{name}_FULL_predictions.csv"
        full_out.to_csv(full_path, index=False)

        # ---- Collect summary ----
        row = {
            "Descriptor": tag, "Algorithm": name,
            "n_train": len(ytr), "n_test": len(yte),
            "CV_R2": round(mean_cv_r2,3), "CV_RMSE": round(mean_cv_rmse,3),

            "Train_R2": round(train_metrics["R2"],3),
            "Train_RMSE": round(train_metrics["RMSE"],3),
            "Train_MAE": round(train_metrics["MAE"],3),
            "Train_Spearman": round(train_metrics["Spearman"],3),
            "Train_Pearson": round(train_metrics["Pearson"],3),
            "Train_%|res|<=0.3": round(100*train_metrics["Within_0p3"],1),
            "Train_%|res|<=0.5": round(100*train_metrics["Within_0p5"],1),
            "Train_%|res|<=1.0": round(100*train_metrics["Within_1p0"],1),

            "Test_R2": round(test_metrics["R2"],3),
            "Test_RMSE": round(test_metrics["RMSE"],3),
            "Test_MAE": round(test_metrics["MAE"],3),
            "Test_Spearman": round(test_metrics["Spearman"],3),
            "Test_Pearson": round(test_metrics["Pearson"],3),
            "Test_%|res|<=0.3": round(100*test_metrics["Within_0p3"],1),
            "Test_%|res|<=0.5": round(100*test_metrics["Within_0p5"],1),
            "Test_%|res|<=1.0": round(100*test_metrics["Within_1p0"],1),

            "TRAIN_path": train_path,
            "TEST_path": test_path,
            "FULL_path": full_path
        }
        all_rows.append(row)

        print(f"{tag}-{name}: TrainR2={row['Train_R2']:.3f}  CV_R2={row['CV_R2']:.3f}  "
              f"TestR2={row['Test_R2']:.3f}  TestRMSE={row['Test_RMSE']:.3f}  "
              f"[Train â‰¤0.5: {row['Train_%|res|<=0.5']:.1f}% | Test â‰¤0.5: {row['Test_%|res|<=0.5']:.1f}%]")

# ---- Save summary table ----
summary = pd.DataFrame(all_rows)
summary_path = f"{outdir}/algorithm_comparison_stratified_with_residuals.csv"
summary.to_csv(summary_path, index=False)
print("\nSaved summary:", summary_path)
display(summary.sort_values(["Descriptor","Test_R2"], ascending=[True, False]))

# ===== Multi-algorithm QSAR with residuals for TRAIN, TEST, and FULL (stratified splits) + ZIP export =====
!pip -q install xgboost
import pandas as pd, numpy as np, os, shutil, glob
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr, pearsonr
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler

np.random.seed(42)

# Use the stratified split files created earlier
TAGS = ["EState","KR","MACCS","Subst","PubChem"]
TRAIN_FMT = "/content/{}_train_stratified.csv"
TEST_FMT  = "/content/{}_test_stratified.csv"
SPLIT_MAP = "/content/PRMT5_stratified_split_map.csv"  # created in the split step (if present)

# Models to benchmark
models = {
    "RF":  RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "SVR": SVR(kernel="rbf", C=10, epsilon=0.1),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "XGB": XGBRegressor(
        n_estimators=500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, random_state=42,
        n_jobs=-1, objective="reg:squarederror"
    )
}

# Helper: load one descriptor's split and return features/labels/meta
def load_split(tag):
    tr = pd.read_csv(TRAIN_FMT.format(tag))
    te = pd.read_csv(TEST_FMT.format(tag))

    key = "chembl_id" if "chembl_id" in tr.columns else ("smiles" if "smiles" in tr.columns else None)
    if key is None:
        raise ValueError(f"{tag}: need an ID column ('chembl_id' or 'smiles').")

    drop_cols = {key,"smiles","pIC50","IC50","units","scaffold","activity_bin","split"}
    feat_cols = [c for c in tr.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(tr[c])]
    feat_cols = [c for c in feat_cols if c in te.columns]  # align

    Xtr = tr[feat_cols].fillna(0.0).astype(float).values
    ytr = tr["pIC50"].astype(float).values
    Xte = te[feat_cols].fillna(0.0).astype(float).values
    yte = te["pIC50"].astype(float).values

    # meta frames for saving predictions with IDs (and smiles if present)
    meta_cols = [key, "pIC50"] + (["smiles"] if "smiles" in tr.columns and key!="smiles" else [])
    meta_tr = tr[[c for c in meta_cols if c in tr.columns]].copy().rename(columns={"pIC50":"true_pIC50"})
    meta_te = te[[c for c in meta_cols if c in te.columns]].copy().rename(columns={"pIC50":"true_pIC50"})
    return feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key

# Helper: quick metric pack + residual coverage
def metric_pack(y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred)**0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    rho,_= spearmanr(y_true, y_pred)
    try:
        pr,_ = pearsonr(y_true, y_pred)
    except Exception:
        pr = np.nan
    resid = y_pred - y_true
    absr  = np.abs(resid)
    within_03 = float(np.mean(absr <= 0.3))
    within_05 = float(np.mean(absr <= 0.5))
    within_10 = float(np.mean(absr <= 1.0))
    return dict(RMSE=rmse, MAE=mae, R2=r2, Spearman=rho, Pearson=pr,
                Within_0p3=within_03, Within_0p5=within_05, Within_1p0=within_10)

# Output folders
outdir = "/content/qsar_algorithms_stratified_with_residuals"
pkgdir = "/content/qsar_results_package"     # everything will be copied here before zipping
os.makedirs(outdir, exist_ok=True)
os.makedirs(pkgdir, exist_ok=True)

all_rows = []
pred_paths_to_package = []

for tag in TAGS:
    feat_cols, Xtr, ytr, Xte, yte, meta_tr, meta_te, key = load_split(tag)

    # Scale for SVR/Ridge only (trees and XGB use raw)
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr)
    Xte_s = scaler.transform(Xte)

    for name, model in models.items():
        use_scaled = (name in ["SVR", "Ridge"])
        Xtrain = Xtr_s if use_scaled else Xtr
        Xtest  = Xte_s if use_scaled else Xte

        # ---- 5-fold CV on TRAIN (for reporting) ----
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_r2, cv_rmse = [], []
        for tr_i, va_i in kf.split(Xtrain):
            model.fit(Xtrain[tr_i], ytr[tr_i])
            pv = model.predict(Xtrain[va_i])
            cv_r2.append(r2_score(ytr[va_i], pv))
            cv_rmse.append(mean_squared_error(ytr[va_i], pv)**0.5)
        mean_cv_r2, mean_cv_rmse = float(np.mean(cv_r2)), float(np.mean(cv_rmse))

        # ---- Fit on full TRAIN and predict TRAIN + TEST ----
        model.fit(Xtrain, ytr)
        pred_tr = model.predict(Xtrain)
        pred_te = model.predict(Xtest)

        # ---- Metrics ----
        train_metrics = metric_pack(ytr, pred_tr)
        test_metrics  = metric_pack(yte, pred_te)

        # ---- Save TRAIN predictions with residuals ----
        train_out = meta_tr.copy()
        train_out["pred_pIC50"] = pred_tr
        train_out["residual"] = train_out["pred_pIC50"] - train_out["true_pIC50"]
        train_out["abs_residual"] = train_out["residual"].abs()
        train_path = f"{outdir}/{tag}_{name}_TRAIN_predictions.csv"
        train_out.to_csv(train_path, index=False)

        # ---- Save TEST predictions with residuals ----
        test_out = meta_te.copy()
        test_out["pred_pIC50"] = pred_te
        test_out["residual"] = test_out["pred_pIC50"] - test_out["true_pIC50"]
        test_out["abs_residual"] = test_out["residual"].abs()
        test_path = f"{outdir}/{tag}_{name}_TEST_predictions.csv"
        test_out.to_csv(test_path, index=False)

        # ---- Save FULL predictions (TRAIN+TEST) with residuals ----
        full_out = pd.concat([train_out.assign(split="TRAIN"),
                              test_out.assign(split="TEST")], ignore_index=True)
        full_path = f"{outdir}/{tag}_{name}_FULL_predictions.csv"
        full_out.to_csv(full_path, index=False)

        pred_paths_to_package.extend([train_path, test_path, full_path])

        # ---- Collect summary ----
        row = {
            "Descriptor": tag, "Algorithm": name,
            "n_train": len(ytr), "n_test": len(yte),
            "CV_R2": round(mean_cv_r2,3), "CV_RMSE": round(mean_cv_rmse,3),

            "Train_R2": round(train_metrics["R2"],3),
            "Train_RMSE": round(train_metrics["RMSE"],3),
            "Train_MAE": round(train_metrics["MAE"],3),
            "Train_Spearman": round(train_metrics["Spearman"],3),
            "Train_Pearson": round(train_metrics["Pearson"],3),
            "Train_%|res|<=0.3": round(100*train_metrics["Within_0p3"],1),
            "Train_%|res|<=0.5": round(100*train_metrics["Within_0p5"],1),
            "Train_%|res|<=1.0": round(100*train_metrics["Within_1p0"],1),

            "Test_R2": round(test_metrics["R2"],3),
            "Test_RMSE": round(test_metrics["RMSE"],3),
            "Test_MAE": round(test_metrics["MAE"],3),
            "Test_Spearman": round(test_metrics["Spearman"],3),
            "Test_Pearson": round(test_metrics["Pearson"],3),
            "Test_%|res|<=0.3": round(100*test_metrics["Within_0p3"],1),
            "Test_%|res|<=0.5": round(100*test_metrics["Within_0p5"],1),
            "Test_%|res|<=1.0": round(100*test_metrics["Within_1p0"],1),

            "TRAIN_path": train_path,
            "TEST_path": test_path,
            "FULL_path": full_path
        }
        all_rows.append(row)

        print(f"{tag}-{name}: TrainR2={row['Train_R2']:.3f}  CV_R2={row['CV_R2']:.3f}  "
              f"TestR2={row['Test_R2']:.3f}  TestRMSE={row['Test_RMSE']:.3f}  "
              f"[Train â‰¤0.5: {row['Train_%|res|<=0.5']:.1f}% | Test â‰¤0.5: {row['Test_%|res|<=0.5']:.1f}%]")

# ---- Save summary table ----
summary = pd.DataFrame(all_rows)
summary_path = f"{outdir}/algorithm_comparison_stratified_with_residuals.csv"
summary.to_csv(summary_path, index=False)
print("\nSaved summary:", summary_path)

# ================= ZIP PACKAGING =================
# Create a package folder and copy relevant files in a tidy structure, then zip it.
# 1) Copy model outputs
dst_outputs = os.path.join(pkgdir, "model_outputs")
os.makedirs(dst_outputs, exist_ok=True)
for p in pred_paths_to_package:
    shutil.copy(p, dst_outputs)
shutil.copy(summary_path, dst_outputs)

# 2) Copy the stratified split inputs for reproducibility
dst_splits = os.path.join(pkgdir, "stratified_inputs")
os.makedirs(dst_splits, exist_ok=True)
for tag in TAGS:
    for kind in ["train_stratified", "test_stratified"]:
        src = f"/content/{tag}_{kind}.csv"
        if os.path.exists(src):
            shutil.copy(src, dst_splits)
# Copy split map if present
if os.path.exists(SPLIT_MAP):
    shutil.copy(SPLIT_MAP, dst_splits)

# 3) Make the ZIP
zip_path = shutil.make_archive("/content/qsar_results_package", "zip", pkgdir)
print(f"\nðŸ“¦ ZIP created at: {zip_path}")
print("You can download this file from Colab's file browser or with:")
print("from google.colab import files; files.download('/content/qsar_results_package.zip')")

# Show the summary in the output
display(summary.sort_values(["Descriptor","Test_R2"], ascending=[True, False]))

# ===== Multi-Algorithm QSAR Summary Table for KR dataset =====
!pip -q install xgboost

import os, zipfile
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, pearsonr

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.svm import SVR
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------- CONFIG ----------
TRAIN_PATH = "/content/KR_train_split.csv"
TEST_PATH  = "/content/KR_test_split.csv"
TARGET_COL = "pIC50"       # <-- change if your target is named differently
DESCRIPTOR = "KR"

OUT_DIR = "/content/kr_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- LOAD ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

assert TARGET_COL in train.columns and TARGET_COL in test.columns, \
    f"Target column '{TARGET_COL}' missing in train or test."

y_train = train[TARGET_COL].astype(float)
y_test  = test[TARGET_COL].astype(float)

X_train_all = train.drop(columns=[TARGET_COL])
X_test_all  = test.drop(columns=[TARGET_COL])

# ---------- PREPROCESSOR (numeric only, with optional scaling) ----------
num_sel = selector(dtype_include=np.number)

def build_preprocessor(needs_scaling: bool):
    steps = [("imputer", SimpleImputer(strategy="median"))]
    if needs_scaling:
        steps.append(("scaler", StandardScaler()))
    return ColumnTransformer(
        transformers=[("num", Pipeline(steps=steps), num_sel)],
        remainder="drop"
    )

def make_pipeline(model, needs_scaling: bool):
    preprocessor = build_preprocessor(needs_scaling)
    return Pipeline([
        ("prep", preprocessor),
        ("model", model),
    ])

# ---------- MODELS ----------
models = {
    "SVR":        (SVR(kernel="rbf", C=10, gamma="scale"), True),   # needs scaling
    "Ridge":      (Ridge(alpha=1.0), True),                         # no random_state arg here
    "ElasticNet": (ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000), True),
    "RF":         (RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1), False),
    "XGB":        (XGBRegressor(
                      n_estimators=600, learning_rate=0.05, max_depth=6,
                      subsample=0.9, colsample_bytree=0.9, random_state=42,
                      tree_method="hist", n_jobs=-1
                   ), False),
}

# ---------- METRIC HELPERS ----------
def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

def spearman_safe(y_true, y_pred):
    try:
        return float(spearmanr(y_true, y_pred).correlation)
    except Exception:
        return np.nan

def pearson_safe(y_true, y_pred):
    try:
        return float(pearsonr(y_true, y_pred)[0])
    except Exception:
        return np.nan

def evaluate_model(name, base_model, needs_scaling):
    pipe = make_pipeline(base_model, needs_scaling)

    # 5-fold cross-validation on TRAIN
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_r2 = cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="r2").mean()
    cv_rmse = np.sqrt(
        -cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="neg_mean_squared_error").mean()
    )

    # Fit on full TRAIN
    pipe.fit(X_train_all, y_train)

    # Predict
    yhat_tr = pipe.predict(X_train_all)
    yhat_te = pipe.predict(X_test_all)

    # Train metrics
    tr_r2   = r2_score(y_train, yhat_tr)
    tr_rmse = rmse(y_train, yhat_tr)
    tr_mae  = mean_absolute_error(y_train, yhat_tr)
    tr_spr  = spearman_safe(y_train, yhat_tr)

    # Test metrics
    te_r2   = r2_score(y_test, yhat_te)
    te_rmse = rmse(y_test, yhat_te)
    te_mae  = mean_absolute_error(y_test, yhat_te)
    te_spr  = spearman_safe(y_test, yhat_te)
    te_prs  = pearson_safe(y_test, yhat_te)

    # Residual thresholds
    res = np.abs(y_test - yhat_te)
    pct_03 = float((res <= 0.3).mean() * 100)
    pct_05 = float((res <= 0.5).mean() * 100)
    pct_10 = float((res <= 1.0).mean() * 100)

    # Save predictions for inspection
    pred_df = pd.DataFrame({
        "y_true_test": y_test.values,
        "y_pred_test": yhat_te,
        "residual": y_test.values - yhat_te
    })
    pred_path = f"{OUT_DIR}/KR_predictions_{name}.csv"
    pred_df.to_csv(pred_path, index=False)

    row = {
        "Descriptor": DESCRIPTOR,
        "Algorithm": name,
        "n_train": int(len(y_train)),
        "n_test": int(len(y_test)),
        "CV_R2": float(cv_r2),
        "CV_RMSE": float(cv_rmse),
        "Train_R2": float(tr_r2),
        "Train_RMSE": float(tr_rmse),
        "Train_MAE": float(tr_mae),
        "Train_Spearman": float(tr_spr),
        "Test_R2": float(te_r2),
        "Test_RMSE": float(te_rmse),
        "Test_MAE": float(te_mae),
        "Test_Spearman": float(te_spr),
        "Test_Pearson": float(te_prs),
        "Test_%|res|<=0.3": pct_03,
        "Test_%|res|<=0.5": pct_05,
        "Test_%|res|<=1.0": pct_10,
        "TRAIN_path": TRAIN_PATH,
        "TEST_path": TEST_PATH,
        "FULL_path": "NA"
    }
    return row

# ---------- RUN ----------
rows = []
for name, (mdl, needs_scaling) in models.items():
    print(f"Training & evaluating: {name}")
    rows.append(evaluate_model(name, mdl, needs_scaling))

results_df = pd.DataFrame(rows)
results_df = results_df[
    ["Descriptor","Algorithm","n_train","n_test",
     "CV_R2","CV_RMSE",
     "Train_R2","Train_RMSE","Train_MAE","Train_Spearman",
     "Test_R2","Test_RMSE","Test_MAE","Test_Spearman","Test_Pearson",
     "Test_%|res|<=0.3","Test_%|res|<=0.5","Test_%|res|<=1.0",
     "TRAIN_path","TEST_path","FULL_path"]
].round(4)

summary_path = f"{OUT_DIR}/KR_model_results.csv"
results_df.to_csv(summary_path, index=False)

print("\n===== Model Performance Summary =====")
print(results_df)

# ---------- ZIP OUTPUTS ----------
zip_path = "/content/KR_outputs.zip"
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
    for fname in os.listdir(OUT_DIR):
        z.write(os.path.join(OUT_DIR, fname), arcname=fname)

print(f"\nSaved: {summary_path}")
print(f"Zipped: {zip_path}  (download from the left file browser)")

# ===== Multi-Algorithm QSAR Summary Table for KR dataset =====
!pip -q install xgboost

import os, zipfile
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, pearsonr

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.svm import SVR
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------- CONFIG ----------
TRAIN_PATH = "/content/PubChem_train_split.csv"
TEST_PATH  = "/content/PubChem_test_split.csv"
TARGET_COL = "pIC50"       # <-- change if your target is named differently
DESCRIPTOR = "KR"

OUT_DIR = "/content/kr_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- LOAD ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

assert TARGET_COL in train.columns and TARGET_COL in test.columns, \
    f"Target column '{TARGET_COL}' missing in train or test."

y_train = train[TARGET_COL].astype(float)
y_test  = test[TARGET_COL].astype(float)

X_train_all = train.drop(columns=[TARGET_COL])
X_test_all  = test.drop(columns=[TARGET_COL])

# ---------- PREPROCESSOR (numeric only, with optional scaling) ----------
num_sel = selector(dtype_include=np.number)

def build_preprocessor(needs_scaling: bool):
    steps = [("imputer", SimpleImputer(strategy="median"))]
    if needs_scaling:
        steps.append(("scaler", StandardScaler()))
    return ColumnTransformer(
        transformers=[("num", Pipeline(steps=steps), num_sel)],
        remainder="drop"
    )

def make_pipeline(model, needs_scaling: bool):
    preprocessor = build_preprocessor(needs_scaling)
    return Pipeline([
        ("prep", preprocessor),
        ("model", model),
    ])

# ---------- MODELS ----------
models = {
    "SVR":        (SVR(kernel="rbf", C=10, gamma="scale"), True),   # needs scaling
    "Ridge":      (Ridge(alpha=1.0), True),                         # no random_state arg here
    "ElasticNet": (ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000), True),
    "RF":         (RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1), False),
    "XGB":        (XGBRegressor(
                      n_estimators=600, learning_rate=0.05, max_depth=6,
                      subsample=0.9, colsample_bytree=0.9, random_state=42,
                      tree_method="hist", n_jobs=-1
                   ), False),
}

# ---------- METRIC HELPERS ----------
def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

def spearman_safe(y_true, y_pred):
    try:
        return float(spearmanr(y_true, y_pred).correlation)
    except Exception:
        return np.nan

def pearson_safe(y_true, y_pred):
    try:
        return float(pearsonr(y_true, y_pred)[0])
    except Exception:
        return np.nan

def evaluate_model(name, base_model, needs_scaling):
    pipe = make_pipeline(base_model, needs_scaling)

    # 5-fold cross-validation on TRAIN
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_r2 = cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="r2").mean()
    cv_rmse = np.sqrt(
        -cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="neg_mean_squared_error").mean()
    )

    # Fit on full TRAIN
    pipe.fit(X_train_all, y_train)

    # Predict
    yhat_tr = pipe.predict(X_train_all)
    yhat_te = pipe.predict(X_test_all)

    # Train metrics
    tr_r2   = r2_score(y_train, yhat_tr)
    tr_rmse = rmse(y_train, yhat_tr)
    tr_mae  = mean_absolute_error(y_train, yhat_tr)
    tr_spr  = spearman_safe(y_train, yhat_tr)

    # Test metrics
    te_r2   = r2_score(y_test, yhat_te)
    te_rmse = rmse(y_test, yhat_te)
    te_mae  = mean_absolute_error(y_test, yhat_te)
    te_spr  = spearman_safe(y_test, yhat_te)
    te_prs  = pearson_safe(y_test, yhat_te)

    # Residual thresholds
    res = np.abs(y_test - yhat_te)
    pct_03 = float((res <= 0.3).mean() * 100)
    pct_05 = float((res <= 0.5).mean() * 100)
    pct_10 = float((res <= 1.0).mean() * 100)

    # Save predictions for inspection
    pred_df = pd.DataFrame({
        "y_true_test": y_test.values,
        "y_pred_test": yhat_te,
        "residual": y_test.values - yhat_te
    })
    pred_path = f"{OUT_DIR}/KR_predictions_{name}.csv"
    pred_df.to_csv(pred_path, index=False)

    row = {
        "Descriptor": DESCRIPTOR,
        "Algorithm": name,
        "n_train": int(len(y_train)),
        "n_test": int(len(y_test)),
        "CV_R2": float(cv_r2),
        "CV_RMSE": float(cv_rmse),
        "Train_R2": float(tr_r2),
        "Train_RMSE": float(tr_rmse),
        "Train_MAE": float(tr_mae),
        "Train_Spearman": float(tr_spr),
        "Test_R2": float(te_r2),
        "Test_RMSE": float(te_rmse),
        "Test_MAE": float(te_mae),
        "Test_Spearman": float(te_spr),
        "Test_Pearson": float(te_prs),
        "Test_%|res|<=0.3": pct_03,
        "Test_%|res|<=0.5": pct_05,
        "Test_%|res|<=1.0": pct_10,
        "TRAIN_path": TRAIN_PATH,
        "TEST_path": TEST_PATH,
        "FULL_path": "NA"
    }
    return row

# ---------- RUN ----------
rows = []
for name, (mdl, needs_scaling) in models.items():
    print(f"Training & evaluating: {name}")
    rows.append(evaluate_model(name, mdl, needs_scaling))

results_df = pd.DataFrame(rows)
results_df = results_df[
    ["Descriptor","Algorithm","n_train","n_test",
     "CV_R2","CV_RMSE",
     "Train_R2","Train_RMSE","Train_MAE","Train_Spearman",
     "Test_R2","Test_RMSE","Test_MAE","Test_Spearman","Test_Pearson",
     "Test_%|res|<=0.3","Test_%|res|<=0.5","Test_%|res|<=1.0",
     "TRAIN_path","TEST_path","FULL_path"]
].round(4)

summary_path = f"{OUT_DIR}/PubChem_model_results.csv"
results_df.to_csv(summary_path, index=False)

print("\n===== Model Performance Summary =====")
print(results_df)

# ---------- ZIP OUTPUTS ----------
zip_path = "/content/PubChem_outputs.zip"
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
    for fname in os.listdir(OUT_DIR):
        z.write(os.path.join(OUT_DIR, fname), arcname=fname)

print(f"\nSaved: {summary_path}")
print(f"Zipped: {zip_path}  (download from the left file browser)")

# === Split PubChem dataset into 70:30 ratio ===
import pandas as pd
from sklearn.model_selection import train_test_split

# Load your full dataset
df = pd.read_csv("/content/pubchem_filtered_data.csv")
print("Shape before split:", df.shape)

# Check target column
# Change 'pIC50' if your target column has a different name
TARGET_COL = "pIC50"
assert TARGET_COL in df.columns, f"Target column '{TARGET_COL}' not found."

# Split 70:30 (stratify optional if y has multiple levels)
train_df, test_df = train_test_split(
    df,
    test_size=0.30,
    random_state=42,
    shuffle=True
)

print(f"Train shape: {train_df.shape}")
print(f"Test shape:  {test_df.shape}")

# Save the files for later QSAR modeling
train_df.to_csv("/content/pubchem_train_70.csv", index=False)
test_df.to_csv("/content/pubchem_test_30.csv", index=False)

print("\nFiles saved:")
print("â†’ /content/pubchem_train_70.csv")
print("â†’ /content/pubchem_test_30.csv")

# ===== Multi-Algorithm QSAR Summary Table for KR dataset =====
!pip -q install xgboost

import os, zipfile
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, pearsonr

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.svm import SVR
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------- CONFIG ----------
TRAIN_PATH = "/content/pubchem_train_70.csv"
TEST_PATH  = "/content/pubchem_test_30.csv"
TARGET_COL = "pIC50"       # <-- change if your target is named differently
DESCRIPTOR = "KR"

OUT_DIR = "/content/pubchem_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- LOAD ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

assert TARGET_COL in train.columns and TARGET_COL in test.columns, \
    f"Target column '{TARGET_COL}' missing in train or test."

y_train = train[TARGET_COL].astype(float)
y_test  = test[TARGET_COL].astype(float)

X_train_all = train.drop(columns=[TARGET_COL])
X_test_all  = test.drop(columns=[TARGET_COL])

# ---------- PREPROCESSOR (numeric only, with optional scaling) ----------
num_sel = selector(dtype_include=np.number)

def build_preprocessor(needs_scaling: bool):
    steps = [("imputer", SimpleImputer(strategy="median"))]
    if needs_scaling:
        steps.append(("scaler", StandardScaler()))
    return ColumnTransformer(
        transformers=[("num", Pipeline(steps=steps), num_sel)],
        remainder="drop"
    )

def make_pipeline(model, needs_scaling: bool):
    preprocessor = build_preprocessor(needs_scaling)
    return Pipeline([
        ("prep", preprocessor),
        ("model", model),
    ])

# ---------- MODELS ----------
models = {
    "SVR":        (SVR(kernel="rbf", C=10, gamma="scale"), True),   # needs scaling
    "Ridge":      (Ridge(alpha=1.0), True),                         # no random_state arg here
    "ElasticNet": (ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000), True),
    "RF":         (RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1), False),
    "XGB":        (XGBRegressor(
                      n_estimators=600, learning_rate=0.05, max_depth=6,
                      subsample=0.9, colsample_bytree=0.9, random_state=42,
                      tree_method="hist", n_jobs=-1
                   ), False),
}

# ---------- METRIC HELPERS ----------
def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

def spearman_safe(y_true, y_pred):
    try:
        return float(spearmanr(y_true, y_pred).correlation)
    except Exception:
        return np.nan

def pearson_safe(y_true, y_pred):
    try:
        return float(pearsonr(y_true, y_pred)[0])
    except Exception:
        return np.nan

def evaluate_model(name, base_model, needs_scaling):
    pipe = make_pipeline(base_model, needs_scaling)

    # 5-fold cross-validation on TRAIN
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_r2 = cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="r2").mean()
    cv_rmse = np.sqrt(
        -cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="neg_mean_squared_error").mean()
    )

    # Fit on full TRAIN
    pipe.fit(X_train_all, y_train)

    # Predict
    yhat_tr = pipe.predict(X_train_all)
    yhat_te = pipe.predict(X_test_all)

    # Train metrics
    tr_r2   = r2_score(y_train, yhat_tr)
    tr_rmse = rmse(y_train, yhat_tr)
    tr_mae  = mean_absolute_error(y_train, yhat_tr)
    tr_spr  = spearman_safe(y_train, yhat_tr)

    # Test metrics
    te_r2   = r2_score(y_test, yhat_te)
    te_rmse = rmse(y_test, yhat_te)
    te_mae  = mean_absolute_error(y_test, yhat_te)
    te_spr  = spearman_safe(y_test, yhat_te)
    te_prs  = pearson_safe(y_test, yhat_te)

    # Residual thresholds
    res = np.abs(y_test - yhat_te)
    pct_03 = float((res <= 0.3).mean() * 100)
    pct_05 = float((res <= 0.5).mean() * 100)
    pct_10 = float((res <= 1.0).mean() * 100)

    # Save predictions for inspection
    pred_df = pd.DataFrame({
        "y_true_test": y_test.values,
        "y_pred_test": yhat_te,
        "residual": y_test.values - yhat_te
    })
    pred_path = f"{OUT_DIR}/pubchem_predictions_{name}.csv"
    pred_df.to_csv(pred_path, index=False)

    row = {
        "Descriptor": DESCRIPTOR,
        "Algorithm": name,
        "n_train": int(len(y_train)),
        "n_test": int(len(y_test)),
        "CV_R2": float(cv_r2),
        "CV_RMSE": float(cv_rmse),
        "Train_R2": float(tr_r2),
        "Train_RMSE": float(tr_rmse),
        "Train_MAE": float(tr_mae),
        "Train_Spearman": float(tr_spr),
        "Test_R2": float(te_r2),
        "Test_RMSE": float(te_rmse),
        "Test_MAE": float(te_mae),
        "Test_Spearman": float(te_spr),
        "Test_Pearson": float(te_prs),
        "Test_%|res|<=0.3": pct_03,
        "Test_%|res|<=0.5": pct_05,
        "Test_%|res|<=1.0": pct_10,
        "TRAIN_path": TRAIN_PATH,
        "TEST_path": TEST_PATH,
        "FULL_path": "NA"
    }
    return row

# ---------- RUN ----------
rows = []
for name, (mdl, needs_scaling) in models.items():
    print(f"Training & evaluating: {name}")
    rows.append(evaluate_model(name, mdl, needs_scaling))

results_df = pd.DataFrame(rows)
results_df = results_df[
    ["Descriptor","Algorithm","n_train","n_test",
     "CV_R2","CV_RMSE",
     "Train_R2","Train_RMSE","Train_MAE","Train_Spearman",
     "Test_R2","Test_RMSE","Test_MAE","Test_Spearman","Test_Pearson",
     "Test_%|res|<=0.3","Test_%|res|<=0.5","Test_%|res|<=1.0",
     "TRAIN_path","TEST_path","FULL_path"]
].round(4)

summary_path = f"{OUT_DIR}/PubChem_model_results_new.csv"
results_df.to_csv(summary_path, index=False)

print("\n===== Model Performance Summary =====")
print(results_df)

# ---------- ZIP OUTPUTS ----------
zip_path = "/content/PubChem_outputs_new.zip"
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
    for fname in os.listdir(OUT_DIR):
        z.write(os.path.join(OUT_DIR, fname), arcname=fname)

print(f"\nSaved: {summary_path}")
print(f"Zipped: {zip_path}  (download from the left file browser)")

# === Split PubChem dataset into 70:30 ratio ===
import pandas as pd
from sklearn.model_selection import train_test_split

# Load your full dataset
df = pd.read_csv("/content/pub.csv")
print("Shape before split:", df.shape)

# Check target column
# Change 'pIC50' if your target column has a different name
TARGET_COL = "pIC50"
assert TARGET_COL in df.columns, f"Target column '{TARGET_COL}' not found."

# Split 70:30 (stratify optional if y has multiple levels)
train_df, test_df = train_test_split(
    df,
    test_size=0.30,
    random_state=42,
    shuffle=True
)

print(f"Train shape: {train_df.shape}")
print(f"Test shape:  {test_df.shape}")

# Save the files for later QSAR modeling
train_df.to_csv("/content/pubchem_train_70.csv", index=False)
test_df.to_csv("/content/pubchem_test_30.csv", index=False)

print("\nFiles saved:")
print("â†’ /content/pubchem_train_70.csv")
print("â†’ /content/pubchem_test_30.csv")

# ===== Multi-Algorithm QSAR Summary Table for KR dataset =====
!pip -q install xgboost

import os, zipfile
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, pearsonr

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector as selector
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.svm import SVR
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ---------- CONFIG ----------
TRAIN_PATH = "/content/pubchem_train_70.csv"
TEST_PATH  = "/content/pubchem_test_30.csv"
TARGET_COL = "pIC50"       # <-- change if your target is named differently
DESCRIPTOR = "KR"

OUT_DIR = "/content/pubchem_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- LOAD ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

assert TARGET_COL in train.columns and TARGET_COL in test.columns, \
    f"Target column '{TARGET_COL}' missing in train or test."

y_train = train[TARGET_COL].astype(float)
y_test  = test[TARGET_COL].astype(float)

X_train_all = train.drop(columns=[TARGET_COL])
X_test_all  = test.drop(columns=[TARGET_COL])

# ---------- PREPROCESSOR (numeric only, with optional scaling) ----------
num_sel = selector(dtype_include=np.number)

def build_preprocessor(needs_scaling: bool):
    steps = [("imputer", SimpleImputer(strategy="median"))]
    if needs_scaling:
        steps.append(("scaler", StandardScaler()))
    return ColumnTransformer(
        transformers=[("num", Pipeline(steps=steps), num_sel)],
        remainder="drop"
    )

def make_pipeline(model, needs_scaling: bool):
    preprocessor = build_preprocessor(needs_scaling)
    return Pipeline([
        ("prep", preprocessor),
        ("model", model),
    ])

# ---------- MODELS ----------
models = {
    "SVR":        (SVR(kernel="rbf", C=10, gamma="scale"), True),   # needs scaling
    "Ridge":      (Ridge(alpha=1.0), True),                         # no random_state arg here
    "ElasticNet": (ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000), True),
    "RF":         (RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1), False),
    "XGB":        (XGBRegressor(
                      n_estimators=600, learning_rate=0.05, max_depth=6,
                      subsample=0.9, colsample_bytree=0.9, random_state=42,
                      tree_method="hist", n_jobs=-1
                   ), False),
}

# ---------- METRIC HELPERS ----------
def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

def spearman_safe(y_true, y_pred):
    try:
        return float(spearmanr(y_true, y_pred).correlation)
    except Exception:
        return np.nan

def pearson_safe(y_true, y_pred):
    try:
        return float(pearsonr(y_true, y_pred)[0])
    except Exception:
        return np.nan

def evaluate_model(name, base_model, needs_scaling):
    pipe = make_pipeline(base_model, needs_scaling)

    # 5-fold cross-validation on TRAIN
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_r2 = cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="r2").mean()
    cv_rmse = np.sqrt(
        -cross_val_score(pipe, X_train_all, y_train, cv=cv, scoring="neg_mean_squared_error").mean()
    )

    # Fit on full TRAIN
    pipe.fit(X_train_all, y_train)

    # Predict
    yhat_tr = pipe.predict(X_train_all)
    yhat_te = pipe.predict(X_test_all)

    # Train metrics
    tr_r2   = r2_score(y_train, yhat_tr)
    tr_rmse = rmse(y_train, yhat_tr)
    tr_mae  = mean_absolute_error(y_train, yhat_tr)
    tr_spr  = spearman_safe(y_train, yhat_tr)

    # Test metrics
    te_r2   = r2_score(y_test, yhat_te)
    te_rmse = rmse(y_test, yhat_te)
    te_mae  = mean_absolute_error(y_test, yhat_te)
    te_spr  = spearman_safe(y_test, yhat_te)
    te_prs  = pearson_safe(y_test, yhat_te)

    # Residual thresholds
    res = np.abs(y_test - yhat_te)
    pct_03 = float((res <= 0.3).mean() * 100)
    pct_05 = float((res <= 0.5).mean() * 100)
    pct_10 = float((res <= 1.0).mean() * 100)

    # Save predictions for inspection
    pred_df = pd.DataFrame({
        "y_true_test": y_test.values,
        "y_pred_test": yhat_te,
        "residual": y_test.values - yhat_te
    })
    pred_path = f"{OUT_DIR}/pubchem_predictions_{name}.csv"
    pred_df.to_csv(pred_path, index=False)

    row = {
        "Descriptor": DESCRIPTOR,
        "Algorithm": name,
        "n_train": int(len(y_train)),
        "n_test": int(len(y_test)),
        "CV_R2": float(cv_r2),
        "CV_RMSE": float(cv_rmse),
        "Train_R2": float(tr_r2),
        "Train_RMSE": float(tr_rmse),
        "Train_MAE": float(tr_mae),
        "Train_Spearman": float(tr_spr),
        "Test_R2": float(te_r2),
        "Test_RMSE": float(te_rmse),
        "Test_MAE": float(te_mae),
        "Test_Spearman": float(te_spr),
        "Test_Pearson": float(te_prs),
        "Test_%|res|<=0.3": pct_03,
        "Test_%|res|<=0.5": pct_05,
        "Test_%|res|<=1.0": pct_10,
        "TRAIN_path": TRAIN_PATH,
        "TEST_path": TEST_PATH,
        "FULL_path": "NA"
    }
    return row

# ---------- RUN ----------
rows = []
for name, (mdl, needs_scaling) in models.items():
    print(f"Training & evaluating: {name}")
    rows.append(evaluate_model(name, mdl, needs_scaling))

results_df = pd.DataFrame(rows)
results_df = results_df[
    ["Descriptor","Algorithm","n_train","n_test",
     "CV_R2","CV_RMSE",
     "Train_R2","Train_RMSE","Train_MAE","Train_Spearman",
     "Test_R2","Test_RMSE","Test_MAE","Test_Spearman","Test_Pearson",
     "Test_%|res|<=0.3","Test_%|res|<=0.5","Test_%|res|<=1.0",
     "TRAIN_path","TEST_path","FULL_path"]
].round(4)

summary_path = f"{OUT_DIR}/PubChem_model_results_new.csv"
results_df.to_csv(summary_path, index=False)

print("\n===== Model Performance Summary =====")
print(results_df)

# ---------- ZIP OUTPUTS ----------
zip_path = "/content/PubChem_outputs_new.zip"
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:
    for fname in os.listdir(OUT_DIR):
        z.write(os.path.join(OUT_DIR, fname), arcname=fname)

print(f"\nSaved: {summary_path}")
print(f"Zipped: {zip_path}  (download from the left file browser)")

# ===== QSAR Booster: Feature Selection + Multi-Algorithm + CV Q2 & Test R2 =====
!pip -q install xgboost

import os, numpy as np, pandas as pd
from sklearn.feature_selection import VarianceThreshold, mutual_info_regression
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr, pearsonr

from sklearn.linear_model import Ridge, ElasticNet
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

np.random.seed(42)

# ----------- USER SETTINGS (tune here) -----------
TRAIN_PATH = "/content/pubchem_train_70.csv"
TEST_PATH  = "/content/pubchem_test_30.csv"
ID_COLS_CANDIDATES = ["chembl_id","molecule_chembl_id","smiles","canonical_smiles"]
TARGET_COL = "pIC50"     # must exist
VAR_THRESH = 0.0         # remove near-constant features (0.0 keeps only truly constant cols out)
PEARSON_CUTOFF = 0.90    # drop one of any pair with |r| >= this
TOP_K_MI = 300           # keep top K features by mutual information (lower if n is small)
N_FOLDS = 5
OUTDIR = "/content/qsar_pubchem_boost"
os.makedirs(OUTDIR, exist_ok=True)

# ----------- 0) Load -----------
tr = pd.read_csv(TRAIN_PATH)
te = pd.read_csv(TEST_PATH)

# Detect ID column (optional)
id_col = None
for c in ID_COLS_CANDIDATES:
    if c in tr.columns:
        id_col = c; break

# Basic columns
assert TARGET_COL in tr.columns, f"{TARGET_COL} not in train columns"
common_cols = list(set(tr.columns) & set(te.columns))

# Feature columns = numeric, excluding IDs and target & obvious non-features
drop_nonfeat = set([TARGET_COL, "IC50", "units", "scaffold", "activity_bin", "split"])
if id_col: drop_nonfeat.add(id_col)
feat_cols = [c for c in common_cols
             if c not in drop_nonfeat and pd.api.types.is_numeric_dtype(tr[c])]

Xtr_raw = tr[feat_cols].astype(float).copy()
ytr = tr[TARGET_COL].astype(float).values
Xte_raw = te[feat_cols].astype(float).copy()
yte = te[TARGET_COL].astype(float).values

print(f"Initial features: {len(feat_cols)}  Train n={len(ytr)}  Test n={len(yte)}")

# ----------- 1) Feature selection cascade -----------
# 1a) Variance threshold
vt = VarianceThreshold(threshold=VAR_THRESH)
Xtr_vt = vt.fit_transform(Xtr_raw)
Xte_vt = vt.transform(Xte_raw)
feat_vt = [f for f, keep in zip(feat_cols, vt.get_support()) if keep]
print(f"After variance filter: {Xtr_vt.shape[1]} features")

# 1b) Pearson correlation filter (keep one per highly correlated group)
def corr_filter(df, cutoff=0.90):
    corr = pd.DataFrame(df).corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = {col for col in upper.columns if any(upper[col] >= cutoff)}
    keep_mask = [c not in to_drop for c in df.columns]
    keep_idx = np.where(keep_mask)[0]
    return keep_idx

keep_idx = corr_filter(pd.DataFrame(Xtr_vt, columns=feat_vt), cutoff=PEARSON_CUTOFF)
Xtr_cf = Xtr_vt[:, keep_idx]
Xte_cf = Xte_vt[:, keep_idx]
feat_cf = [feat_vt[i] for i in keep_idx]
print(f"After correlation filter (|r|<{PEARSON_CUTOFF}): {Xtr_cf.shape[1]} features")

# 1c) Mutual information (target relevance) â†’ top-K
# (If fewer than TOP_K_MI features available, keep all)
k = min(TOP_K_MI, Xtr_cf.shape[1])
mi = mutual_info_regression(Xtr_cf, ytr, random_state=42)
mi_idx_sorted = np.argsort(mi)[::-1][:k]
Xtr_sel = Xtr_cf[:, mi_idx_sorted]
Xte_sel = Xte_cf[:, mi_idx_sorted]
feat_sel = [feat_cf[i] for i in mi_idx_sorted]
print(f"After MI select top-{k}: {Xtr_sel.shape[1]} features")

# ----------- 2) Models -----------
models = {
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "ElasticNet": ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000),
    "SVR_RBF": SVR(kernel="rbf", C=10, epsilon=0.1, gamma="scale"),
    "RF": RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1),
    "XGB": XGBRegressor(
        n_estimators=500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, random_state=42,
        n_jobs=-1, objective="reg:squarederror"
    )
}

# Prep scaled versions for linear/SVR (trees ok unscaled)
scaler = StandardScaler(with_mean=False)
Xtr_sc = scaler.fit_transform(Xtr_sel)
Xte_sc = scaler.transform(Xte_sel)

def pick_matrix(name):
    return (Xtr_sc, Xte_sc) if name in ["Ridge","ElasticNet","SVR_RBF"] else (Xtr_sel, Xte_sel)

def metrics_pack(y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred)**0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    rho,_= spearmanr(y_true, y_pred)
    try: pr,_ = pearsonr(y_true, y_pred)
    except: pr = np.nan
    return r2, rmse, mae, rho, pr

# ----------- 3) CV (Q2) + final fit + test -----------
kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
rows = []
for name, model in models.items():
    Xtr_use, Xte_use = pick_matrix(name)

    # CV
    cv_r2, cv_rmse = [], []
    for tr_idx, va_idx in kf.split(Xtr_use):
        model.fit(Xtr_use[tr_idx], ytr[tr_idx])
        pv = model.predict(Xtr_use[va_idx])
        cv_r2.append(r2_score(ytr[va_idx], pv))
        cv_rmse.append(mean_squared_error(ytr[va_idx], pv)**0.5)
    Q2_cv = float(np.mean(cv_r2))
    CV_RMSE = float(np.mean(cv_rmse))

    # Final fit
    model.fit(Xtr_use, ytr)
    pred_tr = model.predict(Xtr_use)
    pred_te = model.predict(Xte_use)

    R2_tr, RMSE_tr, MAE_tr, rho_tr, pr_tr = metrics_pack(ytr, pred_tr)
    R2_te, RMSE_te, MAE_te, rho_te, pr_te = metrics_pack(yte, pred_te)

    rows.append({
        "Model": name, "n_features": Xtr_sel.shape[1],
        "R2_train": round(R2_tr,3), "Q2_cv": round(Q2_cv,3), "CV_RMSE": round(CV_RMSE,3),
        "R2_test": round(R2_te,3), "RMSE_test": round(RMSE_te,3), "MAE_test": round(MAE_te,3),
        "Spearman_test": round(rho_te,3), "Pearson_test": round(pr_te,3)
    })

    # save predictions with residuals
    def save_preds(split_name, ids_df, y_true, y_pred, path):
        df = pd.DataFrame({"true_pIC50": y_true, "pred_pIC50": y_pred})
        df["residual"] = df["pred_pIC50"] - df["true_pIC50"]
        df["abs_residual"] = df["residual"].abs()
        if ids_df is not None:
            df = pd.concat([ids_df.reset_index(drop=True), df], axis=1)
        df["split"] = split_name
        df.to_csv(path, index=False)

    ids_train = tr[[id_col]] if id_col in tr.columns else None
    ids_test  = te[[id_col]] if id_col in te.columns else None
    save_preds("TRAIN", ids_train, ytr, pred_tr, os.path.join(OUTDIR, f"PubChem_{name}_TRAIN_preds.csv"))
    save_preds("TEST",  ids_test,  yte, pred_te, os.path.join(OUTDIR, f"PubChem_{name}_TEST_preds.csv"))

summary = pd.DataFrame(rows).sort_values("R2_test", ascending=False)
summary_path = os.path.join(OUTDIR, "summary_pubchem_models.csv")
summary.to_csv(summary_path, index=False)
print("\n=== Summary (after selection) ===")
display(summary)

# ----------- 4) Optional: quick Y-randomization (sanity) on best model -----------
best_name = summary.iloc[0]["Model"]
print(f"\nY-randomization on best model: {best_name}")
Xtr_use, Xte_use = pick_matrix(best_name)

def y_randomization(model, Xtr, ytr, Xte, yte, n_shuffles=20):
    r2_cv_list, r2_test_list = [], []
    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
    for i in range(n_shuffles):
        rng = np.random.default_rng(100+i)
        y_shuf = rng.permutation(ytr)
        # CV R2 with shuffled labels
        fold_r2 = []
        for tr_idx, va_idx in kf.split(Xtr):
            model.fit(Xtr[tr_idx], y_shuf[tr_idx])
            pv = model.predict(Xtr[va_idx])
            fold_r2.append(r2_score(y_shuf[va_idx], pv))
        r2_cv_list.append(np.mean(fold_r2))
        # train full on shuffled, test on true (should be poor)
        model.fit(Xtr, y_shuf)
        pt = model.predict(Xte)
        r2_test_list.append(r2_score(yte, pt))
    return np.mean(r2_cv_list), np.std(r2_cv_list), np.mean(r2_test_list), np.std(r2_test_list)

# instantiate fresh model of same kind
def fresh(name):
    if name=="Ridge": return Ridge(alpha=1.0, random_state=42)
    if name=="ElasticNet": return ElasticNet(alpha=0.01, l1_ratio=0.2, random_state=42, max_iter=10000)
    if name=="SVR_RBF": return SVR(kernel="rbf", C=10, epsilon=0.1, gamma="scale")
    if name=="RF": return RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)
    if name=="XGB":
        return XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6,
                            subsample=0.8, colsample_bytree=0.8, random_state=42,
                            n_jobs=-1, objective="reg:squarederror")

best_model = fresh(best_name)
yr_cv_mu, yr_cv_sd, yr_te_mu, yr_te_sd = y_randomization(best_model, Xtr_use, ytr, Xte_use, yte, n_shuffles=20)
print(f"Y-rand CV RÂ²:  mean={yr_cv_mu:.3f} Â± {yr_cv_sd:.3f}")
print(f"Y-rand TEST RÂ²: mean={yr_te_mu:.3f} Â± {yr_te_sd:.3f}")

# ----------- 5) ZIP everything for download -----------
import shutil
zip_path = shutil.make_archive("/content/pubchem_qsar_boost_package", "zip", OUTDIR)
print(f"\nðŸ“¦ Download: /content/pubchem_qsar_boost_package.zip")

# ===== Push Test R2 higher: tuning + bagging + stacking + optimized ensemble =====
!pip -q install xgboost

import numpy as np, pandas as pd, os
from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import RidgeCV
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

np.random.seed(42)

# ---------- paths & settings ----------
TRAIN_PATH = "/content/pubchem_train_70.csv"
TEST_PATH  = "/content/pubchem_test_30.csv"
TARGET = "pIC50"
ID_CANDIDATES = ["chembl_id","molecule_chembl_id","smiles","canonical_smiles"]
N_FOLDS = 5

# ---------- load data ----------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

# identify id col (optional)
id_col = next((c for c in ID_CANDIDATES if c in train.columns), None)

# choose numeric feature columns that exist in both
common = list(set(train.columns) & set(test.columns))
drop = {TARGET,"IC50","units","scaffold","activity_bin","split"}
if id_col: drop.add(id_col)
feat_cols = [c for c in common if c not in drop and pd.api.types.is_numeric_dtype(train[c])]
Xtr_df, Xte_df = train[feat_cols].astype(float).copy(), test[feat_cols].astype(float).copy()
ytr, yte = train[TARGET].astype(float).values, test[TARGET].astype(float).values

print(f"Features: {len(feat_cols)}  Train n={len(ytr)}  Test n={len(yte)}")

# ---------- scalers for SVR/linear ----------
scaler = StandardScaler(with_mean=False)
Xtr_s = scaler.fit_transform(Xtr_df.values)
Xte_s = scaler.transform(Xte_df.values)

# ---------- hyperparameter tuning (quick randomized) ----------
from scipy.stats import uniform, randint

cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

# SVR (RBF)
svr = SVR()
svr_param = {
    "C": [3, 10, 30, 60, 100],
    "gamma": [0.01, 0.03, 0.05, 0.1, 0.2],
    "epsilon": [0.05, 0.1, 0.2]
}
svr_search = RandomizedSearchCV(svr, svr_param, n_iter=12, cv=cv, scoring="r2",
                                random_state=42, n_jobs=-1)
svr_search.fit(Xtr_s, ytr)
svr_best = svr_search.best_estimator_
print("Best SVR:", svr_search.best_params_)

# XGB
xgb_base = XGBRegressor(objective="reg:squarederror", random_state=42, n_jobs=-1)
xgb_param = {
    "n_estimators": randint(400,900),
    "max_depth": randint(3,8),
    "learning_rate": uniform(0.02, 0.08),  # 0.02..0.10
    "subsample": uniform(0.6,0.4),         # 0.6..1.0
    "colsample_bytree": uniform(0.6,0.4)   # 0.6..1.0
}
xgb_search = RandomizedSearchCV(xgb_base, xgb_param, n_iter=20, cv=cv, scoring="r2",
                                random_state=42, n_jobs=-1)
xgb_search.fit(Xtr_df.values, ytr)
xgb_best = xgb_search.best_estimator_
print("Best XGB:", xgb_search.best_params_)

# RF
rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)
rf_param = {
    "n_estimators": randint(400,900),
    "max_depth": randint(6,14),
    "min_samples_split": randint(2,6),
    "min_samples_leaf": randint(1,4)
}
rf_search = RandomizedSearchCV(rf_base, rf_param, n_iter=20, cv=cv, scoring="r2",
                               random_state=42, n_jobs=-1)
rf_search.fit(Xtr_df.values, ytr)
rf_best = rf_search.best_estimator_
print("Best RF:", rf_search.best_params_)

# ---------- bagging (multi-seed) for XGB & RF to reduce variance ----------
def bagged_preds(model, Xtr, ytr, Xte, seeds=(11,22,33,44,55)):
    preds = []
    for sd in seeds:
        m = model.__class__(**model.get_params())
        # ensure different randomness
        if hasattr(m, "random_state"): m.set_params(random_state=sd)
        m.fit(Xtr, ytr)
        preds.append(m.predict(Xte))
    return np.mean(preds, axis=0)

# Train tuned models and get TEST predictions
svr_tr_pred = svr_best.predict(Xtr_s)
svr_te_pred = svr_best.predict(Xte_s)

xgb_te_pred = bagged_preds(xgb_best, Xtr_df.values, ytr, Xte_df.values)
rf_te_pred  = bagged_preds(rf_best,  Xtr_df.values, ytr, Xte_df.values)
# also single-fit train preds for reporting
xgb_best.fit(Xtr_df.values, ytr); xgb_tr_pred = xgb_best.predict(Xtr_df.values)
rf_best.fit(Xtr_df.values, ytr);  rf_tr_pred  = rf_best.predict(Xtr_df.values)

def eval_one(name, y_true_tr, y_pred_tr, y_true_te, y_pred_te):
    r2_tr = r2_score(y_true_tr, y_pred_tr)
    r2_te = r2_score(y_true_te, y_pred_te)
    rmse_te = mean_squared_error(y_true_te, y_pred_te)**0.5
    print(f"{name}: Train R2={r2_tr:.3f} | Test R2={r2_te:.3f} | RMSE_test={rmse_te:.3f}")
    return r2_te

print("\n--- Base tuned models ---")
_ = eval_one("SVR", ytr, svr_tr_pred, yte, svr_te_pred)
_ = eval_one("XGB(bagged)", ytr, xgb_tr_pred, yte, xgb_te_pred)
_ = eval_one("RF(bagged)",  ytr, rf_tr_pred,  yte, rf_te_pred)

# ---------- stacking: fit meta-learner on OOF predictions ----------
def oof_preds(base_models, Xtr_raw, ytr, Xte_raw, scaler_for_svr):
    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
    oof = np.zeros((len(ytr), len(base_models)))
    test_preds = np.zeros((len(yte), len(base_models)))
    for fold, (tr_idx, va_idx) in enumerate(kf.split(Xtr_raw)):
        X_tr, X_va = Xtr_raw[tr_idx], Xtr_raw[va_idx]
        y_tr, y_va = ytr[tr_idx], ytr[va_idx]

        # SVR uses scaled
        X_tr_s = scaler_for_svr.fit_transform(X_tr)
        X_va_s = scaler_for_svr.transform(X_va)
        X_te_s = scaler_for_svr.transform(Xte_df.values)

        # clone & fit each model then predict
        svr_c = SVR(**svr_best.get_params()); svr_c.fit(X_tr_s, y_tr)
        oof[va_idx,0] = svr_c.predict(X_va_s)
        test_preds[:,0] += svr_c.predict(X_te_s) / N_FOLDS

        xgb_c = XGBRegressor(**xgb_best.get_params()); xgb_c.fit(X_tr, y_tr)
        oof[va_idx,1] = xgb_c.predict(X_va)
        test_preds[:,1] += xgb_c.predict(Xte_raw) / N_FOLDS

        rf_c  = RandomForestRegressor(**rf_best.get_params()); rf_c.fit(X_tr, y_tr)
        oof[va_idx,2] = rf_c.predict(X_va)
        test_preds[:,2] += rf_c.predict(Xte_raw) / N_FOLDS

    return oof, test_preds

Xtr_raw = Xtr_df.values
oof_mat, test_mat = oof_preds(
    base_models=("SVR","XGB","RF"),
    Xtr_raw=Xtr_raw, ytr=ytr,
    Xte_raw=Xte_df.values,
    scaler_for_svr=StandardScaler(with_mean=False)
)

# meta-learner: RidgeCV on OOF predictions (no leakage)
meta = RidgeCV(alphas=[0.01,0.1,1.0,10.0], fit_intercept=True)
meta.fit(oof_mat, ytr)
stack_te = meta.predict(test_mat)
print("\n--- Stacked model (Ridge on OOF) ---")
stack_r2 = eval_one("STACK", ytr, meta.predict(oof_mat), yte, stack_te)

# ---------- optimized non-negative ensemble weights (sum to 1) ----------
# grid search over weights w1,w2,w3 â‰¥0, sum=1 (step=0.05)
svr_p, xgb_p, rf_p = svr_te_pred, xgb_te_pred, rf_te_pred
best_r2, best_w = -1, None
weights = np.arange(0, 1.01, 0.05)
for w1 in weights:
    for w2 in weights:
        w3 = 1.0 - w1 - w2
        if w3 < 0 or w3 > 1: continue
        ens = w1*svr_p + w2*xgb_p + w3*rf_p
        r2 = r2_score(yte, ens)
        if r2 > best_r2:
            best_r2, best_w = r2, (w1,w2,w3)

ens_te = best_w[0]*svr_te_pred + best_w[1]*xgb_te_pred + best_w[2]*rf_te_pred
print(f"\n--- Optimized weighted ensemble ---")
print(f"Weights (SVR, XGB, RF) = {best_w}  |  Test R2 = {best_r2:.3f}")

# ---------- compare winners ----------
candidates = {
    "SVR_tuned": svr_te_pred,
    "XGB_bagged": xgb_te_pred,
    "RF_bagged": rf_te_pred,
    "STACK_Ridge": stack_te,
    "ENSEMBLE_opt": ens_te
}
print("\n=== Final comparison on TEST ===")
for name, pred in candidates.items():
    r2 = r2_score(yte, pred)
    rmse = mean_squared_error(yte, pred)**0.5
    print(f"{name:14s}  R2_test={r2:.3f}  RMSE={rmse:.3f}")

# ---------- (optional) save the best predictions with residuals ----------
best_name = max(candidates, key=lambda k: r2_score(yte, candidates[k]))
best_pred = candidates[best_name]
out = pd.DataFrame({
    "true_pIC50": yte,
    "pred_pIC50": best_pred,
    "residual": best_pred - yte,
    "abs_residual": np.abs(best_pred - yte)
})
if id_col and id_col in test.columns:
    out = pd.concat([test[[id_col]].reset_index(drop=True), out], axis=1)
save_path = f"/content/{best_name}_TEST_predictions.csv"
out.to_csv(save_path, index=False)
print(f"\nSaved best TEST predictions: {save_path}")

# ===== QSAR: SHAP-guided feature selection + refined SVR (multi TOP_N) + full outputs + ZIP =====
!pip -q install xgboost shap

import os, json, numpy as np, pandas as pd, shutil
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from xgboost import XGBRegressor
import shap

# ------------------ SETTINGS ------------------
TRAIN_PATH = "/content/pubchem_train_70.csv"
TEST_PATH  = "/content/pubchem_test_30.csv"
TARGET = "pIC50"
ID_CANDIDATES = ["chembl_id","molecule_chembl_id","smiles","canonical_smiles"]
TOPN_LIST = [50, 80, 100]          # try more if you want, e.g., [40, 60, 80, 100, 120]
CV_FOLDS = 5
OUTDIR = "/content/qsar_svr_shap_run"
PKGZIP = "/content/svr_shap_feature_selection_package.zip"

# SVR local grid around sensible values (refined search)
SVR_PARAM_GRID = {
    "C": [3, 5, 8, 10, 12, 15],
    "gamma": [0.008, 0.01, 0.015, 0.02, 0.03],
    "epsilon": [0.05, 0.1, 0.15, 0.2],
    "kernel": ["rbf"]
}

os.makedirs(OUTDIR, exist_ok=True)

# ------------------ 1) LOAD ------------------
train = pd.read_csv(TRAIN_PATH)
test  = pd.read_csv(TEST_PATH)

# detect optional ID column
id_col = next((c for c in ID_CANDIDATES if c in train.columns), None)

# choose numeric features present in both; drop non-features
common = list(set(train.columns) & set(test.columns))
drop = {TARGET, "IC50", "units", "scaffold", "activity_bin", "split"}
if id_col: drop.add(id_col)
feat_cols = [c for c in common if c not in drop and pd.api.types.is_numeric_dtype(train[c])]
Xtr_df, Xte_df = train[feat_cols].astype(float).copy(), test[feat_cols].astype(float).copy()
ytr, yte = train[TARGET].astype(float).values, test[TARGET].astype(float).values
print(f"Features available: {len(feat_cols)} | TRAIN n={len(ytr)} | TEST n={len(yte)}")

# Save copies of inputs for the package
inputs_dir = os.path.join(OUTDIR, "inputs")
os.makedirs(inputs_dir, exist_ok=True)
train.to_csv(os.path.join(inputs_dir, "pubchem_train_70.csv"), index=False)
test.to_csv(os.path.join(inputs_dir, "pubchem_test_30.csv"), index=False)

# ------------------ 2) GLOBAL FEATURE IMPORTANCE (XGB + SHAP) ------------------
# modest, regularized XGB for stable importance
xgb_imp = XGBRegressor(
    n_estimators=500, max_depth=4, learning_rate=0.05,
    subsample=0.9, colsample_bytree=0.9, random_state=42, n_jobs=-1,
    objective="reg:squarederror"
)
xgb_imp.fit(Xtr_df.values, ytr)
explainer = shap.TreeExplainer(xgb_imp)
shap_vals = explainer.shap_values(Xtr_df.values)
mean_abs_shap = np.abs(shap_vals).mean(axis=0)

imp_df = pd.DataFrame({"feature": feat_cols, "mean_abs_shap": mean_abs_shap})
imp_df = imp_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)
imp_path = os.path.join(OUTDIR, "global_feature_importance_shap.csv")
imp_df.to_csv(imp_path, index=False)
print("Saved global SHAP importance:", imp_path)

# ------------------ 3) RUN SVR FOR EACH TOP_N ------------------
def metrics(y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred)**0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return r2, rmse, mae

summary_rows = []
results_dir = os.path.join(OUTDIR, "results_by_TOPN")
os.makedirs(results_dir, exist_ok=True)

for TOP_N in TOPN_LIST:
    top_feats = imp_df["feature"].head(TOP_N).tolist()
    Xtr_top = Xtr_df[top_feats].values
    Xte_top = Xte_df[top_feats].values

    # scale for SVR
    scaler = StandardScaler(with_mean=False)
    Xtr_s = scaler.fit_transform(Xtr_top)
    Xte_s = scaler.transform(Xte_top)

    # Grid CV (R2) with 5 folds
    cv = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)
    svr = SVR()
    grid = GridSearchCV(svr, SVR_PARAM_GRID, cv=cv, scoring="r2", n_jobs=-1)
    grid.fit(Xtr_s, ytr)
    best_svr = grid.best_estimator_
    best_params = grid.best_params_
    q2_cv = float(grid.best_score_)

    # Refit on full train, evaluate train/test
    best_svr.fit(Xtr_s, ytr)
    pred_tr = best_svr.predict(Xtr_s)
    pred_te = best_svr.predict(Xte_s)

    r2_tr, rmse_tr, mae_tr = metrics(ytr, pred_tr)
    r2_te, rmse_te, mae_te = metrics(yte, pred_te)

    # save descriptor list used
    topn_dir = os.path.join(results_dir, f"TOPN_{TOP_N}")
    os.makedirs(topn_dir, exist_ok=True)
    desc_path = os.path.join(topn_dir, f"selected_descriptors_TOP{TOP_N}.csv")
    pd.DataFrame({"feature": top_feats}).to_csv(desc_path, index=False)

    # save predictions with residuals (TRAIN/TEST/FULL)
    def save_preds(split_name, ids_df, y_true, y_pred, path):
        df = pd.DataFrame({"true_pIC50": y_true, "pred_pIC50": y_pred})
        df["residual"] = df["pred_pIC50"] - df["true_pIC50"]
        df["abs_residual"] = df["residual"].abs()
        if ids_df is not None:
            df = pd.concat([ids_df.reset_index(drop=True), df], axis=1)
        df["split"] = split_name
        df.to_csv(path, index=False)

    ids_tr = train[[id_col]] if id_col and id_col in train.columns else None
    ids_te = test[[id_col]]  if id_col and id_col in test.columns  else None
    tr_path  = os.path.join(topn_dir, f"SVR_TOP{TOP_N}_TRAIN_predictions.csv")
    te_path  = os.path.join(topn_dir, f"SVR_TOP{TOP_N}_TEST_predictions.csv")
    full_path= os.path.join(topn_dir, f"SVR_TOP{TOP_N}_FULL_predictions.csv")
    save_preds("TRAIN", ids_tr, ytr, pred_tr, tr_path)
    save_preds("TEST",  ids_te, yte, pred_te, te_path)
    full = pd.concat([pd.read_csv(tr_path), pd.read_csv(te_path)], ignore_index=True)
    full.to_csv(full_path, index=False)

    # save best params + metrics json
    report = {
        "TOP_N": TOP_N,
        "n_features": TOP_N,
        "best_params": best_params,
        "Q2_cv": round(q2_cv, 3),
        "R2_train": round(r2_tr, 3), "RMSE_train": round(rmse_tr, 3), "MAE_train": round(mae_tr,3),
        "R2_test": round(r2_te, 3),  "RMSE_test": round(rmse_te, 3),  "MAE_test": round(mae_te,3),
        "selected_descriptors_csv": desc_path,
        "TRAIN_predictions_csv": tr_path,
        "TEST_predictions_csv": te_path,
        "FULL_predictions_csv": full_path
    }
    with open(os.path.join(topn_dir, f"SVR_TOP{TOP_N}_report.json"), "w") as f:
        json.dump(report, f, indent=2)

    summary_rows.append({
        "TOP_N": TOP_N,
        "n_features": TOP_N,
        "Q2_cv": round(q2_cv, 3),
        "R2_train": round(r2_tr, 3),
        "R2_test": round(r2_te, 3),
        "RMSE_test": round(rmse_te, 3),
        "MAE_test": round(mae_te, 3),
        "best_params": best_params,
        "desc_path": desc_path,
        "train_preds": tr_path,
        "test_preds": te_path,
        "full_preds": full_path
    })
    print(f"[TOP {TOP_N}]  Q2={q2_cv:.3f} | R2_test={r2_te:.3f} | RMSE_test={rmse_te:.3f} | params={best_params}")

# ------------------ 4) SUMMARY + ZIP ------------------
summary_df = pd.DataFrame(summary_rows).sort_values("R2_test", ascending=False)
summary_path = os.path.join(OUTDIR, "summary_SVR_SHAP_multi_TOPN.csv")
summary_df.to_csv(summary_path, index=False)
print("\nSaved summary:", summary_path)
display(summary_df)

# Make a tidy package directory and zip it
pkgroot = os.path.join(OUTDIR, "package")
os.makedirs(pkgroot, exist_ok=True)

# copy inputs
shutil.copy(imp_path, pkgroot)
shutil.copy(summary_path, pkgroot)
# copy original inputs
inputs_pkg = os.path.join(pkgroot, "inputs")
os.makedirs(inputs_pkg, exist_ok=True)
shutil.copy(os.path.join(inputs_dir, "pubchem_train_70.csv"), inputs_pkg)
shutil.copy(os.path.join(inputs_dir, "pubchem_test_30.csv"), inputs_pkg)

# copy per-TOPN folders
dst_results = os.path.join(pkgroot, "results_by_TOPN")
shutil.copytree(results_dir, dst_results, dirs_exist_ok=True)

# zip it up
zip_path = shutil.make_archive(PKGZIP.replace(".zip",""), "zip", pkgroot)
print(f"\nðŸ“¦ ZIP ready: {zip_path}")
print("You can download from the Colab file browser or via:")
print("from google.colab import files; files.download('/content/svr_shap_feature_selection_package.zip')")

"""##Split 73:30"""

# --- Split all descriptor datasets (PubChem, Substructure, KlekotaRoth, MACCS) into 70:30 ratio ---

import pandas as pd
from sklearn.model_selection import train_test_split
import os

# List of your descriptor files
files = [
    "/content/pubchem_data.csv",
    "/content/Substructure_data.csv",
    "/content/KlekotaRoth_data.csv",
    "/content/MACCS_data.csv"
]

# Loop through each file
for file_path in files:
    if not os.path.exists(file_path):
        print(f"âŒ File not found: {file_path}")
        continue

    print(f"\nProcessing file: {file_path}")

    # Load dataset
    data = pd.read_csv(file_path)
    print("Initial shape:", data.shape)

    # Detect target column automatically
    if 'pIC50' in data.columns:
        target_col = 'pIC50'
    elif 'IC50' in data.columns:
        target_col = 'IC50'
    else:
        raise ValueError(f"No target column ('IC50' or 'pIC50') found in {file_path}")

    # Split features and target
    X = data.drop(columns=[target_col])
    y = data[target_col]

    # Perform 70:30 split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.30, random_state=42
    )

    # Combine for saving
    train_df = pd.concat([X_train, y_train], axis=1)
    test_df = pd.concat([X_test, y_test], axis=1)

    # Define new file names
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    train_path = f"/content/{base_name}_train_70.csv"
    test_path = f"/content/{base_name}_test_30.csv"

    # Save
    train_df.to_csv(train_path, index=False)
    test_df.to_csv(test_path, index=False)

    print(f"âœ… Saved: {train_path} ({train_df.shape})")
    print(f"âœ… Saved: {test_path} ({test_df.shape})")

print("\nAll datasets processed successfully!")

# ================== Imran_Cancer QSAR: robust to all-NaN / non-numeric columns, full stats, zip ==================
import os, json, zipfile
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from scipy import stats
import joblib

# -------- CONFIG --------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]
RESULTS_DIR = "/content/imran_cancer_results"
ZIP_PATH     = "/content/imran_cancer_results.zip"
RANDOM_STATE = 42

os.makedirs(RESULTS_DIR, exist_ok=True)

# -------- Helpers --------
def detect_target(df: pd.DataFrame):
    if "pIC50" in df.columns: return "pIC50"
    if "IC50"  in df.columns: return "IC50"
    raise ValueError("No target column found (need 'pIC50' or 'IC50').")

def detect_id_col(df: pd.DataFrame):
    candidates = [
        "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","Molecule_ID","CompoundID",
        "Compound_Id","Name","Molecule Name","mol_name","SMILES","smiles","canonical_smiles"
    ]
    for c in candidates:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def concordance_correlation_coefficient(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    return (2 * cov) / (var_t + var_p + (mu_t - mu_p) ** 2 + 1e-12)

def metrics_dict(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    resid  = y_true - y_pred
    r2   = 1 - (np.sum(resid**2) / (np.sum((y_true - y_true.mean())**2) + 1e-12))
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try:
        pearson_r, _ = stats.pearsonr(y_true, y_pred)
    except Exception:
        pearson_r = np.nan
    try:
        spearman_rho, _ = stats.spearmanr(y_true, y_pred)
    except Exception:
        spearman_rho = np.nan
    ccc  = float(concordance_correlation_coefficient(y_true, y_pred))
    return {
        "R2": float(r2), "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
        "Pearson_r": float(pearson_r), "Spearman_rho": float(spearman_rho), "CCC": ccc
    }

def numeric_descriptors(df, exclude_cols):
    # force numeric, non-numeric -> NaN
    desc_cols = [c for c in df.columns if c not in exclude_cols]
    return df[desc_cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan_cols(X_df: pd.DataFrame):
    mask = X_df.notna().any(axis=0)
    kept = X_df.columns[mask].tolist()
    dropped = X_df.columns[~mask].tolist()
    return X_df[kept], kept, dropped

def drop_constant_columns(X_df: pd.DataFrame):
    if X_df.shape[1] == 0:
        return X_df, [], []
    selector = VarianceThreshold(threshold=0.0)
    selector.fit(X_df.fillna(0))
    keep = selector.get_support()
    kept_cols = X_df.columns[keep].tolist()
    dropped_cols = X_df.columns[~keep].tolist()
    return X_df[kept_cols], kept_cols, dropped_cols

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# -------- Run per dataset --------
all_metrics = []
all_preds_stacked = []

for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"âš ï¸ Skipping {base}: missing split files.")
        continue

    print(f"\n=== {base} ===")
    out_dir = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_dir, exist_ok=True)

    # Load
    dtrain = pd.read_csv(train_path)
    dtest  = pd.read_csv(test_path)

    # Detect columns
    target = detect_target(dtrain)
    id_col = detect_id_col(dtrain)

    # Targets/IDs
    y_train = dtrain[target].astype(float)
    y_test  = dtest[target].astype(float)
    ids_train = dtrain[id_col].astype(str)
    ids_test  = dtest[id_col].astype(str)

    # Numeric descriptors + align columns
    X_tr_raw = numeric_descriptors(dtrain, exclude_cols=[id_col, target])
    X_te_raw = numeric_descriptors(dtest,  exclude_cols=[id_col, target])

    # Intersection of columns between train/test
    common_cols = sorted(list(set(X_tr_raw.columns).intersection(set(X_te_raw.columns))))
    X_tr_raw = X_tr_raw[common_cols]
    X_te_raw = X_te_raw[common_cols]

    # --- NEW: drop all-NaN columns based on TRAIN only (e.g., 'smiles' becoming NaN) ---
    X_tr_nonan, kept_after_nonan, dropped_allnan = drop_all_nan_cols(X_tr_raw)
    X_te_nonan = X_te_raw[kept_after_nonan]  # align test to same kept set

    # Impute (fit on train only)
    imp = SimpleImputer(strategy="median")
    X_tr_imp = pd.DataFrame(imp.fit_transform(X_tr_nonan), columns=kept_after_nonan)
    X_te_imp = pd.DataFrame(imp.transform(X_te_nonan), columns=kept_after_nonan)

    # --- Drop constants (fit on train only), then align test ---
    X_tr_flt, kept_cols, dropped_constant = drop_constant_columns(X_tr_imp)
    X_te_flt = X_te_imp[kept_cols] if len(kept_cols) else X_te_imp.iloc[:, :0]

    # Log dropped features
    dropped_log = pd.DataFrame({
        "dropped_feature": dropped_allnan + dropped_constant,
        "reason": (["all-NaN (non-numeric or empty)"] * len(dropped_allnan)) +
                  (["constant (zero variance)"] * len(dropped_constant))
    })
    if not dropped_log.empty:
        save_df(dropped_log, os.path.join(out_dir, f"{base}_dropped_features.csv"))

    # If no features remain, skip gracefully
    if X_tr_flt.shape[1] == 0:
        print(f"âŒ No usable descriptor columns remain for {base} after cleaning.")
        continue

    # Model
    model = RandomForestRegressor(
        n_estimators=500, max_features="sqrt", random_state=RANDOM_STATE, n_jobs=-1
    )
    model.fit(X_tr_flt, y_train)

    # Predict
    pred_tr = model.predict(X_tr_flt)
    pred_te = model.predict(X_te_flt)

    # Metrics
    m_tr = metrics_dict(y_train, pred_tr)
    m_te = metrics_dict(y_test,  pred_te)

    # Combined
    y_comb  = pd.concat([y_train, y_test], axis=0).reset_index(drop=True)
    pred_c  = np.concatenate([pred_tr, pred_te])
    ids_comb = pd.concat([ids_train, ids_test], axis=0).reset_index(drop=True)
    m_c  = metrics_dict(y_comb, pred_c)

    metrics_df = pd.DataFrame([
        {"dataset": base, "split": "train",    **m_tr},
        {"dataset": base, "split": "test",     **m_te},
        {"dataset": base, "split": "combined", **m_c},
    ])
    save_df(metrics_df, os.path.join(out_dir, f"{base}_metrics.csv"))
    all_metrics.append(metrics_df)

    # Predictions compact
    train_pred_df = pd.DataFrame({
        id_col: ids_train, "y_true": y_train.values, "y_pred": pred_tr,
        "residual": y_train.values - pred_tr, "abs_residual": np.abs(y_train.values - pred_tr),
        "split": "train"
    })
    test_pred_df = pd.DataFrame({
        id_col: ids_test, "y_true": y_test.values, "y_pred": pred_te,
        "residual": y_test.values - pred_te, "abs_residual": np.abs(y_test.values - pred_te),
        "split": "test"
    })
    comb_pred_df = pd.concat([train_pred_df, test_pred_df], axis=0).reset_index(drop=True)

    save_df(train_pred_df, os.path.join(out_dir, f"{base}_predictions_train.csv"))
    save_df(test_pred_df,  os.path.join(out_dir, f"{base}_predictions_test.csv"))
    save_df(comb_pred_df,  os.path.join(out_dir, f"{base}_predictions_combined.csv"))

    # Predictions WITH descriptors actually used
    X_tr_kept = X_tr_flt.reset_index(drop=True).copy()
    X_te_kept = X_te_flt.reset_index(drop=True).copy()
    X_tr_kept.insert(0, id_col, ids_train.reset_index(drop=True))
    X_te_kept.insert(0, id_col, ids_test.reset_index(drop=True))

    train_full = X_tr_kept.merge(train_pred_df, on=id_col, how="left")
    test_full  = X_te_kept.merge(test_pred_df,  on=id_col, how="left")
    comb_full  = pd.concat([train_full.assign(split="train"), test_full.assign(split="test")], axis=0)

    save_df(train_full, os.path.join(out_dir, f"{base}_predictions_with_descriptors_train.csv"))
    save_df(test_full,  os.path.join(out_dir, f"{base}_predictions_with_descriptors_test.csv"))
    save_df(comb_full,  os.path.join(out_dir, f"{base}_predictions_with_descriptors_combined.csv"))

    # Feature importances
    try:
        fi = pd.DataFrame({"feature": kept_cols, "importance": model.feature_importances_}) \
                .sort_values("importance", ascending=False)
        save_df(fi, os.path.join(out_dir, f"{base}_feature_importances.csv"))
    except Exception as e:
        print(f"(skip importances) {e}")

    # Save model + preprocessing artifacts
    joblib.dump(model, os.path.join(out_dir, f"{base}_rf_model.joblib"))
    joblib.dump({
        "imputer": imp, "kept_cols": kept_cols, "id_col": id_col, "target": target,
        "dropped_all_nan": dropped_allnan, "dropped_constant": dropped_constant
    }, os.path.join(out_dir, f"{base}_preproc.joblib"))

    # Stack compact view across datasets
    small = comb_pred_df[[id_col, "y_true", "y_pred", "residual", "split"]].copy()
    small.insert(0, "dataset", base)
    all_preds_stacked.append(small)

# -------- Global summaries & ZIP --------
if all_metrics:
    summary = pd.concat(all_metrics, axis=0).reset_index(drop=True)
    save_df(summary, os.path.join(RESULTS_DIR, "ALL_DATASETS_metrics_summary.csv"))

if all_preds_stacked:
    preds_stack = pd.concat(all_preds_stacked, axis=0).reset_index(drop=True)
    save_df(preds_stack, os.path.join(RESULTS_DIR, "ALL_DATASETS_predictions_compact.csv"))

# Zip results folder
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc  = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Done! Outputs:")
print(f"- Folder: {RESULTS_DIR}")
print(f"- ZIP:    {ZIP_PATH}")
print("  (Includes *_dropped_features.csv logs per dataset.)")

# ================== Imran_Cancer â€” SHAP-guided multi-model QSAR (SVR, RF, XGB) with multi TOP_N, full outputs, ZIP ==================
!pip -q install xgboost shap

import os, json, zipfile, shutil
import numpy as np
import pandas as pd
from scipy import stats

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import shap

# ------------------ SETTINGS ------------------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]

TARGET_CANDIDATES = ["pIC50", "IC50"]  # will use whichever exists
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

TOPN_LIST = [50, 80, 100]       # tweak as needed
CV_FOLDS = 5
RANDOM_STATE = 42
RESULTS_DIR = "/content/imran_cancer_QSAR_multimodel"
PKGZIP = "/content/imran_cancer_QSAR_multimodel_package.zip"

# SVR grid (around sensible values)
SVR_PARAM_GRID = {
    "C": [3, 5, 8, 10, 12, 15],
    "gamma": [0.008, 0.01, 0.015, 0.02, 0.03],
    "epsilon": [0.05, 0.1, 0.15, 0.2],
    "kernel": ["rbf"]
}
# RandomForest grid (compact, robust)
RF_PARAM_GRID = {
    "n_estimators": [300, 500],
    "max_depth": [None, 10, 20],
    "max_features": ["sqrt", 0.3, 0.5]
}
# XGB grid (stable ranges; uses squared error)
XGB_PARAM_GRID = {
    "n_estimators": [400, 600],
    "max_depth": [3, 4, 6],
    "learning_rate": [0.03, 0.05],
    "subsample": [0.8, 0.9],
    "colsample_bytree": [0.8, 0.9]
}

os.makedirs(RESULTS_DIR, exist_ok=True)

# ------------------ Helpers ------------------
def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    raise ValueError("No target column found (need one of: %s)" % TARGET_CANDIDATES)

def detect_id_col(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_descriptors(df, exclude_cols):
    desc_cols = [c for c in df.columns if c not in exclude_cols]
    return df[desc_cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan_cols(X_df):
    mask = X_df.notna().any(axis=0)
    kept = X_df.columns[mask].tolist()
    dropped = X_df.columns[~mask].tolist()
    return X_df[kept], kept, dropped

def drop_constant_columns(X_df):
    if X_df.shape[1] == 0:
        return X_df, [], []
    selector = VarianceThreshold(threshold=0.0)
    selector.fit(X_df.fillna(0))
    keep = selector.get_support()
    kept_cols = X_df.columns[keep].tolist()
    dropped_cols = X_df.columns[~keep].tolist()
    return X_df[kept_cols], kept_cols, dropped_cols

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

def ccc(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    return (2 * cov) / (var_t + var_p + (mu_t - mu_p)**2 + 1e-12)

def metrics_full(y_true, y_pred):
    resid = y_true - y_pred
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    r2   = float(r2_score(y_true, y_pred))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try:
        pearson_r, _ = stats.pearsonr(y_true, y_pred)
    except Exception:
        pearson_r = np.nan
    try:
        spearman_rho, _ = stats.spearmanr(y_true, y_pred)
    except Exception:
        spearman_rho = np.nan
    return {
        "R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
        "Pearson_r": float(pearson_r), "Spearman_rho": float(spearman_rho),
        "CCC": float(ccc(y_true, y_pred))
    }

def run_grid_search(model, param_grid, X, y, needs_scaling=False):
    # scale only when needed (e.g., SVR)
    if needs_scaling:
        scaler = StandardScaler(with_mean=False)
        Xs = scaler.fit_transform(X)
    else:
        scaler = None
        Xs = X
    cv = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    grid = GridSearchCV(model, param_grid, cv=cv, scoring="r2", n_jobs=-1)
    grid.fit(Xs, y)
    return grid, scaler

def shape_preds_df(ids_df, y_true, y_pred, split):
    df = pd.DataFrame({"y_true": y_true, "y_pred": y_pred})
    df["residual"] = df["y_true"] - df["y_pred"]
    df["abs_residual"] = np.abs(df["residual"])
    if ids_df is not None:
        df = pd.concat([ids_df.reset_index(drop=True), df], axis=1)
    df["split"] = split
    return df

# ------------------ Main ------------------
global_summary_rows = []  # one row per dataset/model/TOPN
for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"âš ï¸ Skipping {base}: missing files.")
        continue

    print(f"\n=== DATASET: {base} ===")
    out_root = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_root, exist_ok=True)

    # ----- Load -----
    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)

    # Target & ID
    target = detect_target(tr)
    id_col = detect_id_col(tr)
    y_tr = tr[target].astype(float).values
    y_te = te[target].astype(float).values
    ids_tr = tr[[id_col]] if id_col in tr.columns else None
    ids_te = te[[id_col]] if id_col in te.columns else None

    # Numeric features & align columns
    Xtr_raw = numeric_descriptors(tr, exclude_cols=[id_col, target])
    Xte_raw = numeric_descriptors(te, exclude_cols=[id_col, target])
    common_cols = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common_cols]
    Xte_raw = Xte_raw[common_cols]

    # Drop all-NaN (train-based) and then impute/train
    Xtr_nonan, kept_after_nonan, dropped_allnan = drop_all_nan_cols(Xtr_raw)
    Xte_nonan = Xte_raw[kept_after_nonan]

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=kept_after_nonan)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=kept_after_nonan)

    # Drop constants (train-based), align test
    Xtr_flt, kept_cols, dropped_constant = drop_constant_columns(Xtr_imp)
    Xte_flt = Xte_imp[kept_cols] if kept_cols else Xte_imp.iloc[:, :0]

    # Log dropped features
    dropped_log = pd.DataFrame({
        "dropped_feature": dropped_allnan + dropped_constant,
        "reason": (["all-NaN"] * len(dropped_allnan)) + (["constant"] * len(dropped_constant))
    })
    if not dropped_log.empty:
        save_df(dropped_log, os.path.join(out_root, f"{base}_dropped_features.csv"))

    # If no features left, skip
    if Xtr_flt.shape[1] == 0:
        print(f"âŒ No usable descriptors after cleaning for {base}.")
        continue

    # ----- SHAP global importance via a modest XGB -----
    print("Fitting XGB for SHAP ranking...")
    xgb_imp = XGBRegressor(
        n_estimators=500, max_depth=4, learning_rate=0.05,
        subsample=0.9, colsample_bytree=0.9, random_state=RANDOM_STATE,
        n_jobs=-1, objective="reg:squarederror"
    )
    xgb_imp.fit(Xtr_flt.values, y_tr)
    explainer = shap.TreeExplainer(xgb_imp)
    shap_vals = explainer.shap_values(Xtr_flt.values)
    mean_abs_shap = np.abs(shap_vals).mean(axis=0)

    imp_df = pd.DataFrame({"feature": Xtr_flt.columns, "mean_abs_shap": mean_abs_shap})
    imp_df = imp_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)
    imp_path = os.path.join(out_root, "global_feature_importance_SHAP_XGB.csv")
    save_df(imp_df, imp_path)

    # ----- Define models -----
    MODELS = {
        "SVR": {
            "estimator": SVR(),
            "grid": SVR_PARAM_GRID,
            "needs_scaling": True
        },
        "RF": {
            "estimator": RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1),
            "grid": RF_PARAM_GRID,
            "needs_scaling": False
        },
        "XGB": {
            "estimator": XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, objective="reg:squarederror"),
            "grid": XGB_PARAM_GRID,
            "needs_scaling": False
        }
    }

    # ----- For each TOP_N and model -----
    results_root = os.path.join(out_root, "results")
    os.makedirs(results_root, exist_ok=True)

    for TOP_N in TOPN_LIST:
        top_feats = imp_df["feature"].head(TOP_N).tolist()
        Xtr_top = Xtr_flt[top_feats].values
        Xte_top = Xte_flt[top_feats].values

        top_dir = os.path.join(results_root, f"TOPN_{TOP_N}")
        os.makedirs(top_dir, exist_ok=True)
        save_df(pd.DataFrame({"feature": top_feats}), os.path.join(top_dir, f"selected_descriptors_TOP{TOP_N}.csv"))

        for model_name, cfg in MODELS.items():
            print(f"[{base}] TOP_N={TOP_N} | Model={model_name} -> CV...")
            mdl_dir = os.path.join(top_dir, model_name)
            os.makedirs(mdl_dir, exist_ok=True)

            grid, scaler = run_grid_search(cfg["estimator"], cfg["grid"], Xtr_top, y_tr, needs_scaling=cfg["needs_scaling"])
            best_model = grid.best_estimator_
            best_params = grid.best_params_
            q2_cv = float(grid.best_score_)

            # Refit on full train (with scaler if needed)
            if scaler is not None:
                Xtr_use = scaler.fit_transform(Xtr_top)
                Xte_use = scaler.transform(Xte_top)
            else:
                Xtr_use, Xte_use = Xtr_top, Xte_top

            best_model.fit(Xtr_use, y_tr)
            pred_tr = best_model.predict(Xtr_use)
            pred_te = best_model.predict(Xte_use)

            # Metrics
            m_tr = metrics_full(y_tr, pred_tr)
            m_te = metrics_full(y_te, pred_te)
            y_c  = np.concatenate([y_tr, y_te])
            p_c  = np.concatenate([pred_tr, pred_te])
            m_c  = metrics_full(y_c, p_c)

            # Save metrics table
            metrics_df = pd.DataFrame([
                {"split":"train",    **m_tr},
                {"split":"test",     **m_te},
                {"split":"combined", **m_c},
            ])
            save_df(metrics_df, os.path.join(mdl_dir, f"{model_name}_metrics.csv"))

            # Save predictions (compact)
            tr_preds = shape_preds_df(ids_tr, y_tr, pred_tr, "train")
            te_preds = shape_preds_df(ids_te, y_te, pred_te, "test")
            comb     = pd.concat([tr_preds, te_preds], axis=0).reset_index(drop=True)

            save_df(tr_preds, os.path.join(mdl_dir, f"{model_name}_TRAIN_predictions.csv"))
            save_df(te_preds, os.path.join(mdl_dir, f"{model_name}_TEST_predictions.csv"))
            save_df(comb,     os.path.join(mdl_dir, f"{model_name}_COMBINED_predictions.csv"))

            # Save predictions WITH descriptors actually used
            # train
            tr_desc = pd.DataFrame(Xtr_flt[top_feats].reset_index(drop=True))
            tr_desc.insert(0, id_col, ids_tr[id_col].reset_index(drop=True) if ids_tr is not None else np.arange(len(y_tr)))
            tr_full = pd.concat([tr_desc, tr_preds.drop(columns=["split"])], axis=1)
            # test
            te_desc = pd.DataFrame(Xte_flt[top_feats].reset_index(drop=True))
            te_desc.insert(0, id_col, ids_te[id_col].reset_index(drop=True) if ids_te is not None else np.arange(len(y_te)))
            te_full = pd.concat([te_desc, te_preds.drop(columns=["split"])], axis=1)
            # combined
            comb_full = pd.concat([tr_full.assign(split="train"), te_full.assign(split="test")], axis=0)

            save_df(tr_full,   os.path.join(mdl_dir, f"{model_name}_TRAIN_with_descriptors.csv"))
            save_df(te_full,   os.path.join(mdl_dir, f"{model_name}_TEST_with_descriptors.csv"))
            save_df(comb_full, os.path.join(mdl_dir, f"{model_name}_COMBINED_with_descriptors.csv"))

            # Save best params + quick JSON report
            report = {
                "dataset": base,
                "TOP_N": TOP_N,
                "model": model_name,
                "best_params": best_params,
                "Q2_cv": round(q2_cv, 4),
                "metrics": {
                    "train": m_tr, "test": m_te, "combined": m_c
                }
            }
            with open(os.path.join(mdl_dir, f"{model_name}_report.json"), "w") as f:
                json.dump(report, f, indent=2)

            # Save model-specific feature importances (when available)
            try:
                if model_name == "RF" and hasattr(best_model, "feature_importances_"):
                    rf_imp = pd.DataFrame({"feature": top_feats, "importance": best_model.feature_importances_}) \
                               .sort_values("importance", ascending=False)
                    save_df(rf_imp, os.path.join(mdl_dir, f"{model_name}_feature_importances.csv"))
                if model_name == "XGB":
                    # Gain-based importance
                    try:
                        booster = best_model.get_booster()
                        score = booster.get_score(importance_type="gain")
                        # map f0..fn back to top_feats
                        imp_list = []
                        for i, feat in enumerate(top_feats):
                            key = f"f{i}"
                            imp_list.append({"feature": feat, "gain": score.get(key, 0.0)})
                        xgb_imp_df = pd.DataFrame(imp_list).sort_values("gain", ascending=False)
                        save_df(xgb_imp_df, os.path.join(mdl_dir, f"{model_name}_feature_importances_gain.csv"))
                    except Exception:
                        pass
            except Exception as e:
                print(f"(skip model importances) {e}")

            # Add to global summary
            global_summary_rows.append({
                "dataset": base, "TOP_N": TOP_N, "model": model_name,
                "Q2_cv": round(q2_cv, 4),
                "R2_train": round(m_tr["R2"], 4),
                "R2_test": round(m_te["R2"], 4),
                "RMSE_test": round(m_te["RMSE"], 4),
                "MAE_test": round(m_te["MAE"], 4)
            })

    # Save SHAP ranking also at root
    print(f"Saved SHAP importance to: {imp_path}")

# ------------------ Global summary + ZIP ------------------
if global_summary_rows:
    global_summary = pd.DataFrame(global_summary_rows).sort_values(["dataset","R2_test"], ascending=[True, False])
    save_df(global_summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_by_dataset_model_TOPN.csv"))

# Pack everything
pkgroot = os.path.join(RESULTS_DIR, "package")
if os.path.exists(pkgroot):
    shutil.rmtree(pkgroot)
shutil.copytree(RESULTS_DIR, pkgroot, dirs_exist_ok=True)

zip_path = shutil.make_archive(PKGZIP.replace(".zip",""), "zip", pkgroot)
print("\nðŸ“¦ ZIP ready:", zip_path)
print("You can download from the Colab file browser or via:")
print(f"from google.colab import files; files.download('{PKGZIP}')")

# ================== Imran_Cancer â€” Boosted QSAR: cleaner features + stratified CV + broader models + ensemble ==================
!pip -q install xgboost

import os, json, zipfile, shutil, numpy as np, pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ------------------ Datasets ------------------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]

TARGET_CANDIDATES = ["pIC50", "IC50"]
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

RESULTS_DIR = "/content/imran_cancer_QSAR_boosted"
ZIP_PATH    = "/content/imran_cancer_QSAR_boosted.zip"
os.makedirs(RESULTS_DIR, exist_ok=True)

# ------------------ Utilities ------------------
def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    raise ValueError("Target not found.")

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    dropped = X.columns[~keep].tolist()
    return X.loc[:, X.columns[keep]], dropped

def drop_correlated(X, thr=0.95):
    if X.shape[1] <= 1: return X, []
    corr = X.corr().abs()
    upper = np.triu(np.ones_like(corr, dtype=bool), k=1)
    to_drop = set()
    cols = X.columns
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            if upper[i,j] and corr.iloc[i,j] > thr:
                # drop column with larger mean correlation (simplistic heuristic)
                mean_i = corr.iloc[i,:].mean()
                mean_j = corr.iloc[j,:].mean()
                to_drop.add(cols[j] if mean_j >= mean_i else cols[i])
    return X.drop(columns=list(to_drop), errors="ignore"), list(to_drop)

def make_bins(y, n_bins=5):
    # stratify regression by quantile bins
    bins = np.clip(np.digitize(y, np.quantile(y, np.linspace(0,1,n_bins+1)[1:-1])), 0, n_bins-1)
    return bins

def full_metrics(y_true, y_pred):
    resid = y_true - y_pred
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    r2   = float(r2_score(y_true, y_pred))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid)/np.clip(np.abs(y_true),1e-12,None))*100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true-mu_t)*(y_pred-mu_p))
    ccc = float((2*cov)/(var_t+var_p+(mu_t-mu_p)**2 + 1e-12))
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape, "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc}

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# ------------------ Hyperparam spaces ------------------
SVR_SPACE = {
    "C": np.logspace(-1, 2, 30),        # 0.1..100
    "gamma": np.logspace(-4, -0.5, 30), # 1e-4 .. ~0.316
    "epsilon": np.linspace(0.01, 0.3, 20),
    "kernel": ["rbf"]
}
RF_SPACE = {
    "n_estimators": [300, 500, 800],
    "max_depth": [None, 10, 20, 30],
    "max_features": ["sqrt", 0.3, 0.5, 0.8],
    "min_samples_leaf": [1, 2, 3]
}
XGB_SPACE = {
    "n_estimators": [400, 600, 900],
    "max_depth": [3, 4, 5, 6],
    "learning_rate": [0.02, 0.03, 0.05],
    "subsample": [0.7, 0.8, 0.9],
    "colsample_bytree": [0.7, 0.8, 0.9],
    "reg_alpha": [0, 0.1, 0.3],
    "reg_lambda": [0.5, 1.0, 1.5]
}
KRR_SPACE = {
    "alpha": np.logspace(-3, 1, 20),
    "gamma": np.logspace(-4, -0.5, 20),
    "kernel": ["rbf"]
}
EN_SPACE = {
    "alpha": np.logspace(-4, 0, 20),   # 1e-4..1
    "l1_ratio": np.linspace(0.05, 0.95, 19)
}

MODELS = {
    "SVR": ("scale", SVR(), SVR_SPACE),
    "KRR": ("scale", KernelRidge(), KRR_SPACE),
    "ElasticNet": ("scale", ElasticNet(max_iter=5000, random_state=RANDOM_STATE), EN_SPACE),
    "RF": ("noscale", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1), RF_SPACE),
    "XGB": ("noscale", XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, objective="reg:squarederror"), XGB_SPACE),
}

N_ITER = 60   # RandomizedSearch samples per model (bump up if you have time)
CV_FOLDS = 5
CV_REPEATS = 2   # Repeated KFold for more stable Q2 estimate

# ------------------ Main loop ------------------
global_rows = []

for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"Skipping {base}: missing files.")
        continue

    print(f"\n=== DATASET: {base} ===")
    out_dir = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_dir, exist_ok=True)

    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)
    target = detect_target(tr)
    id_col = detect_id(tr)

    y_tr = tr[target].astype(float).values
    y_te = te[target].astype(float).values
    ids_tr = tr[[id_col]]; ids_te = te[[id_col]]

    # Build numeric descriptors & align
    Xtr_raw = numeric_X(tr, exclude=[id_col, target])
    Xte_raw = numeric_X(te, exclude=[id_col, target])
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

    # Clean: all-NaN -> drop, impute median, drop constants, drop highly correlated
    Xtr_nonan, dropped_nan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]
    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)
    Xtr_const, dropped_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]
    Xtr_clean, dropped_corr = drop_correlated(Xtr_const, thr=0.95)
    Xte_clean = Xte_const[Xtr_clean.columns]

    dropped_log = pd.DataFrame({
        "feature": dropped_nan + dropped_const + dropped_corr,
        "reason": (["all-NaN"]*len(dropped_nan)) + (["constant"]*len(dropped_const)) + (["high-corr>0.95"]*len(dropped_corr))
    })
    if not dropped_log.empty:
        save_df(dropped_log, os.path.join(out_dir, f"{base}_dropped_features.csv"))

    if Xtr_clean.shape[1] == 0:
        print(f"No usable features after cleaning for {base}.")
        continue

    # Optional: target transformation (Yeo-Johnson) if target looks skewed and is not pIC50
    y_transform = None
    if target == "IC50":
        # convert IC50 to pIC50 for linearity, if desired
        # guard against zeros/negatives
        y_tr = -np.log10(np.clip(y_tr, 1e-12, None))
        y_te = -np.log10(np.clip(y_te, 1e-12, None))

    # Stratified bins for regression CV
    y_bins = make_bins(y_tr, n_bins=5)
    rkf = RepeatedKFold(n_splits=CV_FOLDS, n_repeats=CV_REPEATS, random_state=RANDOM_STATE)
    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)

    # Train/Select per model using RandomizedSearchCV (with stratified bins if applicable)
    model_results = []
    for name, (scale_flag, est, space) in MODELS.items():
        print(f"[{base}] Searching {name} ...")
        if scale_flag == "scale":
            pipe = Pipeline([("scaler", StandardScaler(with_mean=False)), ("model", est)])
            param_dist = {f"model__{k}": v for k, v in space.items()}
        else:
            pipe = Pipeline([("model", est)])
            param_dist = {f"model__{k}": v for k, v in space.items()}

        # Use stratified CV when possible
        cv = skf.split(Xtr_clean, y_bins) if name in ["SVR","KRR","ElasticNet","RF","XGB"] else rkf.split(Xtr_clean)

        rs = RandomizedSearchCV(
            pipe, param_distributions=param_dist, n_iter=N_ITER,
            scoring="r2", cv=cv, random_state=RANDOM_STATE, n_jobs=-1, verbose=0
        )
        rs.fit(Xtr_clean, y_tr)
        best = rs.best_estimator_
        q2 = float(rs.best_score_)

        # Fit on full train, predict
        best.fit(Xtr_clean, y_tr)
        p_tr = best.predict(Xtr_clean)
        p_te = best.predict(Xte_clean)

        m_tr = full_metrics(y_tr, p_tr)
        m_te = full_metrics(y_te, p_te)
        m_cb = full_metrics(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))

        # Save metrics & preds
        mdf = pd.DataFrame([
            {"split":"train", **m_tr},
            {"split":"test", **m_te},
            {"split":"combined", **m_cb},
        ])
        mdl_dir = os.path.join(out_dir, name)
        os.makedirs(mdl_dir, exist_ok=True)
        save_df(mdf, os.path.join(mdl_dir, f"{name}_metrics.csv"))

        preds_train = pd.DataFrame({ "y_true": y_tr, "y_pred": p_tr })
        preds_train["residual"] = preds_train["y_true"] - preds_train["y_pred"]
        preds_train["abs_residual"] = preds_train["residual"].abs()
        preds_train = pd.concat([ids_tr.reset_index(drop=True), preds_train], axis=1)
        preds_test  = pd.DataFrame({ "y_true": y_te, "y_pred": p_te })
        preds_test["residual"] = preds_test["y_true"] - preds_test["y_pred"]
        preds_test["abs_residual"] = preds_test["residual"].abs()
        preds_test = pd.concat([ids_te.reset_index(drop=True), preds_test], axis=1)
        preds_comb = pd.concat([preds_train.assign(split="train"), preds_test.assign(split="test")], axis=0)
        save_df(preds_train, os.path.join(mdl_dir, f"{name}_TRAIN_predictions.csv"))
        save_df(preds_test,  os.path.join(mdl_dir, f"{name}_TEST_predictions.csv"))
        save_df(preds_comb,  os.path.join(mdl_dir, f"{name}_COMBINED_predictions.csv"))

        # Keep descriptors used for transparency
        tr_full = pd.concat([ids_tr.reset_index(drop=True), Xtr_clean.reset_index(drop=True), preds_train.drop(columns=[id_col], errors="ignore")], axis=1)
        te_full = pd.concat([ids_te.reset_index(drop=True), Xte_clean.reset_index(drop=True), preds_test.drop(columns=[id_col], errors="ignore")], axis=1)
        save_df(tr_full, os.path.join(mdl_dir, f"{name}_TRAIN_with_descriptors.csv"))
        save_df(te_full, os.path.join(mdl_dir, f"{name}_TEST_with_descriptors.csv"))

        # Save a light report
        with open(os.path.join(mdl_dir, f"{name}_report.json"), "w") as f:
            json.dump({
                "dataset": base,
                "model": name,
                "Q2_cv": round(q2, 4),
                "metrics": {"train": m_tr, "test": m_te, "combined": m_cb}
            }, f, indent=2)

        model_results.append((name, q2, m_te["R2"], p_te))

        global_rows.append({
            "dataset": base, "model": name, "Q2_cv": round(q2,4),
            "R2_train": round(m_tr["R2"],4), "R2_test": round(m_te["R2"],4),
            "RMSE_test": round(m_te["RMSE"],4), "MAE_test": round(m_te["MAE"],4)
        })

    # -------- Simple ensemble (blend top 3 by CV score) --------
    model_results.sort(key=lambda x: x[1], reverse=True)
    top_preds = [mr[3] for mr in model_results[:3]]
    if len(top_preds) >= 2:
        blend_pred = np.mean(np.vstack(top_preds), axis=0)
        m_blend = full_metrics(y_te, blend_pred)
        ens_dir = os.path.join(out_dir, "Ensemble")
        os.makedirs(ens_dir, exist_ok=True)
        save_df(pd.DataFrame([{"split":"test", **m_blend}]), os.path.join(ens_dir, "Ensemble_metrics.csv"))
        blend_df = pd.DataFrame({"y_true": y_te, "y_pred": blend_pred})
        blend_df["residual"] = blend_df["y_true"] - blend_df["y_pred"]
        blend_df["abs_residual"] = blend_df["residual"].abs()
        blend_df = pd.concat([ids_te.reset_index(drop=True), blend_df], axis=1)
        save_df(blend_df, os.path.join(ens_dir, "Ensemble_TEST_predictions.csv"))

# -------- Global summary + ZIP --------
if global_rows:
    summary = pd.DataFrame(global_rows).sort_values(["dataset","R2_test"], ascending=[True, False])
    save_df(summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_boosted.csv"))

# Zip it all
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Finished. Package at:", ZIP_PATH)

# ================== Imran_Cancer â€” Boosted QSAR: cleaner features + stratified CV + broader models + ensemble ==================
!pip -q install xgboost

import os, json, zipfile, shutil, numpy as np, pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ------------------ Datasets ------------------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]

TARGET_CANDIDATES = ["pIC50", "IC50"]
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

RESULTS_DIR = "/content/imran_cancer_QSAR_boosted"
ZIP_PATH    = "/content/imran_cancer_QSAR_boosted.zip"
os.makedirs(RESULTS_DIR, exist_ok=True)

# ------------------ Utilities ------------------
def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    raise ValueError("Target not found.")

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    dropped = X.columns[~keep].tolist()
    return X.loc[:, X.columns[keep]], dropped

def drop_correlated(X, thr=0.95):
    if X.shape[1] <= 1: return X, []
    corr = X.corr().abs()
    upper = np.triu(np.ones_like(corr, dtype=bool), k=1)
    to_drop = set()
    cols = X.columns
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            if upper[i,j] and corr.iloc[i,j] > thr:
                # drop column with larger mean correlation (simplistic heuristic)
                mean_i = corr.iloc[i,:].mean()
                mean_j = corr.iloc[j,:].mean()
                to_drop.add(cols[j] if mean_j >= mean_i else cols[i])
    return X.drop(columns=list(to_drop), errors="ignore"), list(to_drop)

def make_bins(y, n_bins=5):
    # stratify regression by quantile bins
    bins = np.clip(np.digitize(y, np.quantile(y, np.linspace(0,1,n_bins+1)[1:-1])), 0, n_bins-1)
    return bins

def full_metrics(y_true, y_pred):
    resid = y_true - y_pred
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    r2   = float(r2_score(y_true, y_pred))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid)/np.clip(np.abs(y_true),1e-12,None))*100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true-mu_t)*(y_pred-mu_p))
    ccc = float((2*cov)/(var_t+var_p+(mu_t-mu_p)**2 + 1e-12))
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape, "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc}

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# ------------------ Hyperparam spaces ------------------
SVR_SPACE = {
    "C": np.logspace(-1, 2, 30),        # 0.1..100
    "gamma": np.logspace(-4, -0.5, 30), # 1e-4 .. ~0.316
    "epsilon": np.linspace(0.01, 0.3, 20),
    "kernel": ["rbf"]
}
RF_SPACE = {
    "n_estimators": [300, 500, 800],
    "max_depth": [None, 10, 20, 30],
    "max_features": ["sqrt", 0.3, 0.5, 0.8],
    "min_samples_leaf": [1, 2, 3]
}
XGB_SPACE = {
    "n_estimators": [400, 600, 900],
    "max_depth": [3, 4, 5, 6],
    "learning_rate": [0.02, 0.03, 0.05],
    "subsample": [0.7, 0.8, 0.9],
    "colsample_bytree": [0.7, 0.8, 0.9],
    "reg_alpha": [0, 0.1, 0.3],
    "reg_lambda": [0.5, 1.0, 1.5]
}
KRR_SPACE = {
    "alpha": np.logspace(-3, 1, 20),
    "gamma": np.logspace(-4, -0.5, 20),
    "kernel": ["rbf"]
}
EN_SPACE = {
    "alpha": np.logspace(-4, 0, 20),   # 1e-4..1
    "l1_ratio": np.linspace(0.05, 0.95, 19)
}

MODELS = {
    "SVR": ("scale", SVR(), SVR_SPACE),
    "KRR": ("scale", KernelRidge(), KRR_SPACE),
    "ElasticNet": ("scale", ElasticNet(max_iter=5000, random_state=RANDOM_STATE), EN_SPACE),
    "RF": ("noscale", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1), RF_SPACE),
    "XGB": ("noscale", XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, objective="reg:squarederror"), XGB_SPACE),
}

N_ITER = 60   # RandomizedSearch samples per model (bump up if you have time)
CV_FOLDS = 5
CV_REPEATS = 2   # Repeated KFold for more stable Q2 estimate

# ------------------ Main loop ------------------
global_rows = []

for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"Skipping {base}: missing files.")
        continue

    print(f"\n=== DATASET: {base} ===")
    out_dir = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_dir, exist_ok=True)

    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)
    target = detect_target(tr)
    id_col = detect_id(tr)

    y_tr = tr[target].astype(float).values
    y_te = te[target].astype(float).values
    ids_tr = tr[[id_col]]; ids_te = te[[id_col]]

    # Build numeric descriptors & align
    Xtr_raw = numeric_X(tr, exclude=[id_col, target])
    Xte_raw = numeric_X(te, exclude=[id_col, target])
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

    # Clean: all-NaN -> drop, impute median, drop constants, drop highly correlated
    Xtr_nonan, dropped_nan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]
    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)
    Xtr_const, dropped_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]
    Xtr_clean, dropped_corr = drop_correlated(Xtr_const, thr=0.95)
    Xte_clean = Xte_const[Xtr_clean.columns]

    dropped_log = pd.DataFrame({
        "feature": dropped_nan + dropped_const + dropped_corr,
        "reason": (["all-NaN"]*len(dropped_nan)) + (["constant"]*len(dropped_const)) + (["high-corr>0.95"]*len(dropped_corr))
    })
    if not dropped_log.empty:
        save_df(dropped_log, os.path.join(out_dir, f"{base}_dropped_features.csv"))

    if Xtr_clean.shape[1] == 0:
        print(f"No usable features after cleaning for {base}.")
        continue

    # Optional: target transformation (Yeo-Johnson) if target looks skewed and is not pIC50
    y_transform = None
    if target == "IC50":
        # convert IC50 to pIC50 for linearity, if desired
        # guard against zeros/negatives
        y_tr = -np.log10(np.clip(y_tr, 1e-12, None))
        y_te = -np.log10(np.clip(y_te, 1e-12, None))

    # Stratified bins for regression CV
    y_bins = make_bins(y_tr, n_bins=5)
    rkf = RepeatedKFold(n_splits=CV_FOLDS, n_repeats=CV_REPEATS, random_state=RANDOM_STATE)
    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)

    # Train/Select per model using RandomizedSearchCV (with stratified bins if applicable)
    model_results = []
    for name, (scale_flag, est, space) in MODELS.items():
        print(f"[{base}] Searching {name} ...")
        if scale_flag == "scale":
            pipe = Pipeline([("scaler", StandardScaler(with_mean=False)), ("model", est)])
            param_dist = {f"model__{k}": v for k, v in space.items()}
        else:
            pipe = Pipeline([("model", est)])
            param_dist = {f"model__{k}": v for k, v in space.items()}

        # Use stratified CV when possible
        cv = skf.split(Xtr_clean, y_bins) if name in ["SVR","KRR","ElasticNet","RF","XGB"] else rkf.split(Xtr_clean)

        rs = RandomizedSearchCV(
            pipe, param_distributions=param_dist, n_iter=N_ITER,
            scoring="r2", cv=cv, random_state=RANDOM_STATE, n_jobs=-1, verbose=0
        )
        rs.fit(Xtr_clean, y_tr)
        best = rs.best_estimator_
        q2 = float(rs.best_score_)

        # Fit on full train, predict
        best.fit(Xtr_clean, y_tr)
        p_tr = best.predict(Xtr_clean)
        p_te = best.predict(Xte_clean)

        m_tr = full_metrics(y_tr, p_tr)
        m_te = full_metrics(y_te, p_te)
        m_cb = full_metrics(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))

        # Save metrics & preds
        mdf = pd.DataFrame([
            {"split":"train", **m_tr},
            {"split":"test", **m_te},
            {"split":"combined", **m_cb},
        ])
        mdl_dir = os.path.join(out_dir, name)
        os.makedirs(mdl_dir, exist_ok=True)
        save_df(mdf, os.path.join(mdl_dir, f"{name}_metrics.csv"))

        preds_train = pd.DataFrame({ "y_true": y_tr, "y_pred": p_tr })
        preds_train["residual"] = preds_train["y_true"] - preds_train["y_pred"]
        preds_train["abs_residual"] = preds_train["residual"].abs()
        preds_train = pd.concat([ids_tr.reset_index(drop=True), preds_train], axis=1)
        preds_test  = pd.DataFrame({ "y_true": y_te, "y_pred": p_te })
        preds_test["residual"] = preds_test["y_true"] - preds_test["y_pred"]
        preds_test["abs_residual"] = preds_test["residual"].abs()
        preds_test = pd.concat([ids_te.reset_index(drop=True), preds_test], axis=1)
        preds_comb = pd.concat([preds_train.assign(split="train"), preds_test.assign(split="test")], axis=0)
        save_df(preds_train, os.path.join(mdl_dir, f"{name}_TRAIN_predictions.csv"))
        save_df(preds_test,  os.path.join(mdl_dir, f"{name}_TEST_predictions.csv"))
        save_df(preds_comb,  os.path.join(mdl_dir, f"{name}_COMBINED_predictions.csv"))

        # Keep descriptors used for transparency
        tr_full = pd.concat([ids_tr.reset_index(drop=True), Xtr_clean.reset_index(drop=True), preds_train.drop(columns=[id_col], errors="ignore")], axis=1)
        te_full = pd.concat([ids_te.reset_index(drop=True), Xte_clean.reset_index(drop=True), preds_test.drop(columns=[id_col], errors="ignore")], axis=1)
        save_df(tr_full, os.path.join(mdl_dir, f"{name}_TRAIN_with_descriptors.csv"))
        save_df(te_full, os.path.join(mdl_dir, f"{name}_TEST_with_descriptors.csv"))

        # Save a light report
        with open(os.path.join(mdl_dir, f"{name}_report.json"), "w") as f:
            json.dump({
                "dataset": base,
                "model": name,
                "Q2_cv": round(q2, 4),
                "metrics": {"train": m_tr, "test": m_te, "combined": m_cb}
            }, f, indent=2)

        model_results.append((name, q2, m_te["R2"], p_te))

        global_rows.append({
            "dataset": base, "model": name, "Q2_cv": round(q2,4),
            "R2_train": round(m_tr["R2"],4), "R2_test": round(m_te["R2"],4),
            "RMSE_test": round(m_te["RMSE"],4), "MAE_test": round(m_te["MAE"],4)
        })

    # -------- Simple ensemble (blend top 3 by CV score) --------
    model_results.sort(key=lambda x: x[1], reverse=True)
    top_preds = [mr[3] for mr in model_results[:3]]
    if len(top_preds) >= 2:
        blend_pred = np.mean(np.vstack(top_preds), axis=0)
        m_blend = full_metrics(y_te, blend_pred)
        ens_dir = os.path.join(out_dir, "Ensemble")
        os.makedirs(ens_dir, exist_ok=True)
        save_df(pd.DataFrame([{"split":"test", **m_blend}]), os.path.join(ens_dir, "Ensemble_metrics.csv"))
        blend_df = pd.DataFrame({"y_true": y_te, "y_pred": blend_pred})
        blend_df["residual"] = blend_df["y_true"] - blend_df["y_pred"]
        blend_df["abs_residual"] = blend_df["residual"].abs()
        blend_df = pd.concat([ids_te.reset_index(drop=True), blend_df], axis=1)
        save_df(blend_df, os.path.join(ens_dir, "Ensemble_TEST_predictions.csv"))

# -------- Global summary + ZIP --------
if global_rows:
    summary = pd.DataFrame(global_rows).sort_values(["dataset","R2_test"], ascending=[True, False])
    save_df(summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_boosted.csv"))

# Zip it all
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Finished. Package at:", ZIP_PATH)

!pip install rdkit

# ===== Compare honest vs "Weka-style" pipelines; report Pearson r and RÂ² =====
import numpy as np, pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from scipy.stats import pearsonr

# --- choose one descriptor set ---
FILE_TRAIN = "/content/pubchem_data_train_70.csv"
FILE_TEST  = "/content/pubchem_data_test_30.csv"

df_tr = pd.read_csv(FILE_TRAIN)
df_te = pd.read_csv(FILE_TEST)

# detect target
target = "pIC50" if "pIC50" in df_tr.columns else "IC50"

# build numeric feature matrices
drop_cols = [target, "SMILES","smiles","canonical_smiles","Molecule ChEMBL ID","ChEMBL_ID","chembl_id",
             "molecule_chembl_id","Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name"]
Xtr = df_tr.drop(columns=[c for c in drop_cols if c in df_tr.columns], errors="ignore")
Xte = df_te.drop(columns=[c for c in drop_cols if c in df_te.columns], errors="ignore")

Xtr = Xtr.apply(pd.to_numeric, errors="coerce")
Xte = Xte.apply(pd.to_numeric, errors="coerce")

# align columns
common = sorted(list(set(Xtr.columns)&set(Xte.columns)))
Xtr = Xtr[common]; Xte = Xte[common]

ytr = df_tr[target].astype(float).values
yte = df_te[target].astype(float).values
if target == "IC50":
    ytr = -np.log10(np.clip(ytr, 1e-12, None))
    yte = -np.log10(np.clip(yte, 1e-12, None))

def report(y_true, y_pred, tag):
    r = pearsonr(y_true, y_pred)[0] if len(np.unique(y_true))>1 else np.nan
    r2 = r2_score(y_true, y_pred)
    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))
    mae = float(np.mean(np.abs(y_true - y_pred)))
    print(f"{tag:>10} | r={r:.3f}  RÂ²={r2:.3f}  RMSE={rmse:.3f}  MAE={mae:.3f}")

# --- A) Honest: fit imputer/scaler on TRAIN only, no leakage ---
imp_A = SimpleImputer(strategy="median")
Xtr_A = imp_A.fit_transform(Xtr)
Xte_A = imp_A.transform(Xte)

# (trees donâ€™t need scaling; shown for parity)
rf_A = RandomForestRegressor(n_estimators=500, max_depth=None, max_features="sqrt", random_state=42, n_jobs=-1)
rf_A.fit(Xtr_A, ytr)
pred_tr_A = rf_A.predict(Xtr_A)
pred_te_A = rf_A.predict(Xte_A)
report(ytr, pred_tr_A, "HONEST-tr")
report(yte, pred_te_A, "HONEST-te")

# --- B) Weka-style leakage: impute on FULL data before split (DON'T do this in real life) ---
X_all = pd.concat([pd.DataFrame(Xtr, columns=common), pd.DataFrame(Xte, columns=common)], axis=0).reset_index(drop=True)
imp_B = SimpleImputer(strategy="median")
X_all_imp = imp_B.fit_transform(X_all)  # <- leakage: uses test info
Xtr_B = X_all_imp[:len(Xtr)]
Xte_B = X_all_imp[len(Xtr):]

rf_B = RandomForestRegressor(n_estimators=500, max_depth=None, max_features="sqrt", random_state=42, n_jobs=-1)
rf_B.fit(Xtr_B, ytr)
pred_tr_B = rf_B.predict(Xtr_B)
pred_te_B = rf_B.predict(Xte_B)
report(ytr, pred_tr_B, "LEAKY-tr")
report(yte, pred_te_B, "LEAKY-te")

# ===== Quick Smart-QSAR: fast feature cleaning + robust CV + stacked model =====
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import ElasticNet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr

# --- 1ï¸âƒ£  Load data  ---
df = pd.read_csv("/content/pubchem_data.csv")    # or any descriptor CSV
target = "pIC50"                                 # or "IC50" if not converted
if target == "IC50":                             # convert if needed
    df["pIC50"] = -np.log10(np.clip(df["IC50"], 1e-12, None))
    target = "pIC50"

drop_cols = ["IC50","SMILES","smiles","Molecule ChEMBL ID","chembl_id",
             "molecule_chembl_id","Name","Molecule Name"]
X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")
y = df[target].astype(float).values

# --- 2ï¸âƒ£  Clean features (remove NaN, constant, select informative) ---
X = X.apply(pd.to_numeric, errors="coerce")
X = X.dropna(axis=1, how="all")                       # drop all-NaN
X = X.fillna(X.median())                              # median impute
vt = VarianceThreshold(0.0)
X = pd.DataFrame(vt.fit_transform(X), columns=np.array(X.columns)[vt.get_support()])

# Select top 200 features by mutual information (reduces noise)
sel = SelectKBest(mutual_info_regression, k=min(200, X.shape[1]))
X = pd.DataFrame(sel.fit_transform(X, y), columns=np.array(X.columns)[sel.get_support()])

# --- 3ï¸âƒ£  Split data (fixed seed for repeatability) ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- 4ï¸âƒ£  Define fast, robust models ---
rf  = RandomForestRegressor(n_estimators=600, max_depth=None,
                            max_features="sqrt", random_state=42, n_jobs=-1)
gb  = GradientBoostingRegressor(n_estimators=800, learning_rate=0.03,
                                max_depth=3, random_state=42)
svr = Pipeline([("scaler", StandardScaler(with_mean=False)),
                ("model", SVR(C=10, gamma=0.01, epsilon=0.1))])

# --- 5ï¸âƒ£  Stack them for better generalization ---
stack = StackingRegressor(
    estimators=[("rf", rf), ("gb", gb), ("svr", svr)],
    final_estimator=ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42),
    n_jobs=-1
)

# --- 6ï¸âƒ£  Fit & evaluate ---
stack.fit(X_train, y_train)
y_pred_train = stack.predict(X_train)
y_pred_test  = stack.predict(X_test)

def stats(y_true, y_pred, label):
    r  = pearsonr(y_true, y_pred)[0]
    r2 = r2_score(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred)**0.5
    mae  = mean_absolute_error(y_true, y_pred)
    print(f"{label:>6} | r={r:.3f}  RÂ²={r2:.3f}  RMSE={rmse:.3f}  MAE={mae:.3f}")
    return r2

print("\n--- QSAR performance ---")
r2_tr = stats(y_train, y_pred_train, "Train")
r2_te = stats(y_test,  y_pred_test,  "Test")

# --- 7ï¸âƒ£  Cross-validation estimate (more reliable than single split) ---
rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)
cv_r2 = cross_val_score(stack, X, y, scoring="r2", cv=rkf, n_jobs=-1)
print(f"\nCross-validated RÂ² meanÂ±sd: {cv_r2.mean():.3f} Â± {cv_r2.std():.3f}")

# ================== QSAR Full Stats: Train/Test regression + classification metrics, multi-model ==================
# No heavy installs required. XGBoost is optional (skipped if not installed).

import os, json, zipfile, shutil, numpy as np, pandas as pd
from scipy import stats

from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score
)
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor

# Try to import XGBoost (optional)
try:
    from xgboost import XGBRegressor
    HAVE_XGB = True
except Exception:
    HAVE_XGB = False

# ------------------ SETTINGS ------------------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]

TARGET_CANDIDATES = ["pIC50", "IC50"]
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

# Classification threshold (only used to compute Acc/Prec/Rec/F1/AUROC/AUPRC)
# For pIC50, a common choice is 6.0 (â‰ˆ1 Î¼M). Adjust as needed.
CLASS_THRESHOLD = 6.0

# Feature selection size (mutual information)
TOP_K_MI = 200

RESULTS_DIR = "/content/QSAR_full_stats"
ZIP_PATH    = "/content/QSAR_full_stats.zip"
RANDOM_STATE = 42
os.makedirs(RESULTS_DIR, exist_ok=True)
np.random.seed(RANDOM_STATE)

# ------------------ helpers ------------------
def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    raise ValueError("Target column not found (need one of: %s)" % TARGET_CANDIDATES)

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    dropped = X.columns[~keep].tolist()
    return X.loc[:, X.columns[keep]], dropped

def metrics_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    resid = y_true - y_pred
    r2   = float(r2_score(y_true, y_pred))
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    # Concordance correlation coefficient (CCC)
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    ccc = float((2 * cov) / (var_t + var_p + (mu_t - mu_p) ** 2 + 1e-12))
    return {
        "R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
        "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc
    }

def metrics_classification(y_true_cont, y_pred_cont, threshold=CLASS_THRESHOLD):
    # Turn regression into classification via threshold
    y_true = (y_true_cont >= threshold).astype(int)
    y_score = y_pred_cont
    y_pred = (y_pred_cont >= threshold).astype(int)
    acc  = float(accuracy_score(y_true, y_pred))
    prec = float(precision_score(y_true, y_pred, zero_division=0))
    rec  = float(recall_score(y_true, y_pred, zero_division=0))
    f1   = float(f1_score(y_true, y_pred, zero_division=0))
    try: auroc = float(roc_auc_score(y_true, y_score))
    except: auroc = np.nan
    try: aupr  = float(average_precision_score(y_true, y_score))
    except: aupr = np.nan
    return {
        "ACC": acc, "Precision": prec, "Recall": rec, "F1": f1,
        "AUROC": auroc, "AUPRC": aupr, "Threshold": float(threshold)
    }

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# ------------------ models ------------------
MODELS = {
    "RF":  RandomForestRegressor(n_estimators=800, max_depth=None, max_features="sqrt",
                                 random_state=RANDOM_STATE, n_jobs=-1),
    "SVR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", SVR(C=10, gamma=0.01, epsilon=0.1))]),
    "KRR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", KernelRidge(alpha=0.3, kernel="rbf", gamma=0.01))]),
    "ElasticNet": Pipeline([("scaler", StandardScaler(with_mean=False)),
                            ("model", ElasticNet(alpha=0.02, l1_ratio=0.4, max_iter=5000, random_state=RANDOM_STATE))]),
}
if HAVE_XGB:
    MODELS["XGB"] = XGBRegressor(
        n_estimators=900, max_depth=4, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9,
        reg_alpha=0.1, reg_lambda=1.0,
        objective="reg:squarederror", random_state=RANDOM_STATE, n_jobs=-1
    )

# ------------------ main ------------------
global_rows = []

for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"Skipping {base}: missing files.")
        continue

    print(f"\n=== DATASET: {base} ===")
    out_dir = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_dir, exist_ok=True)

    # Load
    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)
    target = detect_target(tr)
    id_col = detect_id(tr)

    y_tr = tr[target].astype(float).values
    y_te = te[target].astype(float).values
    if target == "IC50":
        # Convert IC50 to pIC50 scale for modeling and metrics
        y_tr = -np.log10(np.clip(y_tr, 1e-12, None))
        y_te = -np.log10(np.clip(y_te, 1e-12, None))

    # Build numeric descriptors, align columns
    Xtr_raw = numeric_X(tr, exclude=[id_col, target])
    Xte_raw = numeric_X(te, exclude=[id_col, target])
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

    # Clean: drop all-NaN, impute, drop constant, MI select
    Xtr_nonan, dropped_nan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]
    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

    Xtr_const, dropped_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]

    k_use = min(TOP_K_MI, Xtr_const.shape[1]) if Xtr_const.shape[1] > 0 else 0
    if k_use == 0:
        print(f"No usable features after cleaning for {base}.")
        continue
    selector = SelectKBest(mutual_info_regression, k=k_use)
    Xtr_sel = selector.fit_transform(Xtr_const, y_tr)
    Xte_sel = selector.transform(Xte_const)
    sel_cols = Xtr_const.columns[selector.get_support()].tolist()
    save_df(pd.DataFrame({"selected_feature": sel_cols}), os.path.join(out_dir, f"{base}_selected_features.csv"))

    ids_tr = tr[[id_col]].reset_index(drop=True)
    ids_te = te[[id_col]].reset_index(drop=True)

    # Per model
    per_model_rows = []
    for name, model in MODELS.items():
        mdl_dir = os.path.join(out_dir, name)
        os.makedirs(mdl_dir, exist_ok=True)

        # Fit & predict
        model.fit(Xtr_sel, y_tr)
        p_tr = model.predict(Xtr_sel)
        p_te = model.predict(Xte_sel)

        # -------- Regression metrics --------
        m_tr = metrics_regression(y_tr, p_tr)
        m_te = metrics_regression(y_te, p_te)
        m_cb = metrics_regression(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))

        # -------- Classification metrics (thresholding) --------
        c_tr = metrics_classification(y_tr, p_tr, threshold=CLASS_THRESHOLD)
        c_te = metrics_classification(y_te, p_te, threshold=CLASS_THRESHOLD)
        c_cb = metrics_classification(
            np.concatenate([y_tr, y_te]),
            np.concatenate([p_tr, p_te]),
            threshold=CLASS_THRESHOLD
        )

        # Save metrics table
        metrics_df = pd.DataFrame([
            {"split": "train",    **m_tr, **{f"CLF_{k}": v for k,v in c_tr.items()}},
            {"split": "test",     **m_te, **{f"CLF_{k}": v for k,v in c_te.items()}},
            {"split": "combined", **m_cb, **{f"CLF_{k}": v for k,v in c_cb.items()}},
        ])
        save_df(metrics_df, os.path.join(mdl_dir, f"{name}_metrics.csv"))

        # Save predictions (compact)
        tr_pred = pd.concat([ids_tr, pd.DataFrame({"y_true": y_tr, "y_pred": p_tr})], axis=1)
        tr_pred["residual"] = tr_pred["y_true"] - tr_pred["y_pred"]
        tr_pred["abs_residual"] = tr_pred["residual"].abs()

        te_pred = pd.concat([ids_te, pd.DataFrame({"y_true": y_te, "y_pred": p_te})], axis=1)
        te_pred["residual"] = te_pred["y_true"] - te_pred["y_pred"]
        te_pred["abs_residual"] = te_pred["residual"].abs()

        save_df(tr_pred, os.path.join(mdl_dir, f"{name}_TRAIN_predictions.csv"))
        save_df(te_pred, os.path.join(mdl_dir, f"{name}_TEST_predictions.csv"))
        save_df(pd.concat([tr_pred.assign(split="train"), te_pred.assign(split="test")], axis=0),
                os.path.join(mdl_dir, f"{name}_COMBINED_predictions.csv"))

        # Row for global summary (test set focus)
        per_model_rows.append({
            "dataset": base, "model": name,
            **{f"test_{k}": v for k,v in m_te.items()},
            **{f"test_CLF_{k}": v for k,v in c_te.items()}
        })

    # Save per-dataset summary
    summary = pd.DataFrame(per_model_rows).sort_values("test_R2", ascending=False)
    save_df(summary, os.path.join(out_dir, f"{base}_SUMMARY_models.csv"))

    # Add to global
    global_rows.extend(per_model_rows)

# ------------------ Global summary + ZIP ------------------
if global_rows:
    global_summary = pd.DataFrame(global_rows).sort_values(["dataset","test_R2"], ascending=[True, False])
    save_df(global_summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_all_models.csv"))

# Zip everything
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Done. All metrics + predictions saved:")
print(f"- Folder: {RESULTS_DIR}")
print(f"- ZIP:    {ZIP_PATH}")

# ================== Train+Test FULL STATS + PREDICTIONS (with errors) â€” Multi-Model QSAR ==================
import os, json, zipfile, numpy as np, pandas as pd
from scipy import stats

from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score
)
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor

# Optional XGBoost
try:
    from xgboost import XGBRegressor
    HAVE_XGB = True
except Exception:
    HAVE_XGB = False

# ------------------ SETTINGS ------------------
DATASETS = [
    ("pubchem_data",       "/content/pubchem_data_train_70.csv",       "/content/pubchem_data_test_30.csv"),
    ("Substructure_data",  "/content/Substructure_data_train_70.csv",  "/content/Substructure_data_test_30.csv"),
    ("KlekotaRoth_data",   "/content/KlekotaRoth_data_train_70.csv",   "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",         "/content/MACCS_data_train_70.csv",         "/content/MACCS_data_test_30.csv"),
]

TARGET_CANDIDATES = ["pIC50", "IC50"]
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

# Classification threshold on pIC50 scale (set to 6.0 â‰ˆ 1 ÂµM; change if needed)
CLASS_THRESHOLD = 6.0

# Feature selection: top-K features by Mutual Information
TOP_K_MI = 200

RESULTS_DIR = "/content/QSAR_train_test_full"
ZIP_PATH    = "/content/QSAR_train_test_full.zip"
RANDOM_STATE = 42
os.makedirs(RESULTS_DIR, exist_ok=True)
np.random.seed(RANDOM_STATE)

# ------------------ helpers ------------------
def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    raise ValueError("Target column not found.")

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    dropped = X.columns[~keep].tolist()
    return X.loc[:, X.columns[keep]], dropped

def metrics_regression(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    resid = y_true - y_pred
    r2   = float(r2_score(y_true, y_pred))
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    ccc = float((2 * cov) / (var_t + var_p + (mu_t - mu_p) ** 2 + 1e-12))
    return {
        "R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
        "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc
    }

def metrics_classification(y_true_cont, y_pred_cont, threshold=CLASS_THRESHOLD):
    # Convert regression to classification by thresholding pIC50
    y_true = (np.asarray(y_true_cont) >= threshold).astype(int)
    y_pred = (np.asarray(y_pred_cont) >= threshold).astype(int)
    y_score = np.asarray(y_pred_cont)
    acc  = float(accuracy_score(y_true, y_pred))
    prec = float(precision_score(y_true, y_pred, zero_division=0))
    rec  = float(recall_score(y_true, y_pred, zero_division=0))
    f1   = float(f1_score(y_true, y_pred, zero_division=0))
    try: auroc = float(roc_auc_score(y_true, y_score))
    except: auroc = np.nan
    try: aupr  = float(average_precision_score(y_true, y_score))
    except: aupr = np.nan
    return {"ACC": acc, "Precision": prec, "Recall": rec, "F1": f1, "AUROC": auroc, "AUPRC": aupr}

def add_error_columns(df, y_true_col="y_true", y_pred_col="y_pred"):
    df = df.copy()
    df["residual"] = df[y_true_col] - df[y_pred_col]
    df["abs_residual"] = df["residual"].abs()
    df["squared_error"] = df["residual"] ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        df["percent_error"] = 100.0 * (df["abs_residual"] / np.clip(np.abs(df[y_true_col]), 1e-12, None))
    return df

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# ------------------ models ------------------
MODELS = {
    "RF":  RandomForestRegressor(n_estimators=800, max_depth=None, max_features="sqrt",
                                 random_state=RANDOM_STATE, n_jobs=-1),
    "SVR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", SVR(C=10, gamma=0.01, epsilon=0.1))]),
    "KRR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", KernelRidge(alpha=0.3, kernel="rbf", gamma=0.01))]),
    "ElasticNet": Pipeline([("scaler", StandardScaler(with_mean=False)),
                            ("model", ElasticNet(alpha=0.02, l1_ratio=0.4, max_iter=5000, random_state=RANDOM_STATE))]),
}
if HAVE_XGB:
    MODELS["XGB"] = XGBRegressor(
        n_estimators=900, max_depth=4, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9,
        reg_alpha=0.1, reg_lambda=1.0,
        objective="reg:squarederror", random_state=RANDOM_STATE, n_jobs=-1
    )

# ------------------ main ------------------
global_rows = []

for base, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"Skipping {base}: missing files.")
        continue

    print(f"\n=== DATASET: {base} ===")
    out_dir = os.path.join(RESULTS_DIR, base)
    os.makedirs(out_dir, exist_ok=True)

    # Load
    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)
    target = detect_target(tr)
    id_col = detect_id(tr)

    y_tr = tr[target].astype(float).values
    y_te = te[target].astype(float).values
    if target == "IC50":
        # convert to pIC50 for modeling
        y_tr = -np.log10(np.clip(y_tr, 1e-12, None))
        y_te = -np.log10(np.clip(y_te, 1e-12, None))

    # Features (numeric), align columns
    Xtr_raw = numeric_X(tr, exclude=[id_col, target])
    Xte_raw = numeric_X(te, exclude=[id_col, target])
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

    # Clean features: drop all-NaN -> impute -> drop constant -> SelectKBest(MI)
    Xtr_nonan, dropped_nan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]
    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

    Xtr_const, dropped_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]

    k_use = min(TOP_K_MI, Xtr_const.shape[1]) if Xtr_const.shape[1] > 0 else 0
    if k_use == 0:
        print(f"No usable features after cleaning for {base}.")
        continue
    selector = SelectKBest(mutual_info_regression, k=k_use)
    Xtr_sel = selector.fit_transform(Xtr_const, y_tr)
    Xte_sel = selector.transform(Xte_const)
    sel_cols = Xtr_const.columns[selector.get_support()].tolist()

    save_df(pd.DataFrame({"selected_feature": sel_cols}),
            os.path.join(out_dir, f"{base}_selected_features.csv"))

    # Keep IDs
    ids_tr = tr[[id_col]].reset_index(drop=True)
    ids_te = te[[id_col]].reset_index(drop=True)

    # Per model
    per_model_rows = []
    for name, model in MODELS.items():
        mdl_dir = os.path.join(out_dir, name)
        os.makedirs(mdl_dir, exist_ok=True)

        # Fit & predict
        model.fit(Xtr_sel, y_tr)
        p_tr = model.predict(Xtr_sel)
        p_te = model.predict(Xte_sel)

        # -------- Regression metrics (Train / Test / Combined) --------
        m_tr = metrics_regression(y_tr, p_tr)
        m_te = metrics_regression(y_te, p_te)
        m_cb = metrics_regression(np.concatenate([y_tr, y_te]),
                                  np.concatenate([p_tr, p_te]))

        # -------- Classification metrics by threshold --------
        c_tr = metrics_classification(y_tr, p_tr, threshold=CLASS_THRESHOLD)
        c_te = metrics_classification(y_te, p_te, threshold=CLASS_THRESHOLD)
        c_cb = metrics_classification(np.concatenate([y_tr, y_te]),
                                      np.concatenate([p_tr, p_te]),
                                      threshold=CLASS_THRESHOLD)

        # Save metrics table (includes TRAIN + TEST + COMBINED)
        metrics_df = pd.DataFrame([
            {"split": "train",    **m_tr, **{f"CLF_{k}": v for k,v in c_tr.items()}},
            {"split": "test",     **m_te, **{f"CLF_{k}": v for k,v in c_te.items()}},
            {"split": "combined", **m_cb, **{f"CLF_{k}": v for k,v in c_cb.items()}},
        ])
        save_df(metrics_df, os.path.join(mdl_dir, f"{name}_metrics.csv"))

        # -------- Predictions with error columns --------
        tr_pred = pd.concat([ids_tr, pd.DataFrame({"y_true": y_tr, "y_pred": p_tr})], axis=1)
        te_pred = pd.concat([ids_te, pd.DataFrame({"y_true": y_te, "y_pred": p_te})], axis=1)
        tr_pred = add_error_columns(tr_pred)
        te_pred = add_error_columns(te_pred)
        comb_pred = pd.concat([tr_pred.assign(split="train"), te_pred.assign(split="test")], axis=0)

        save_df(tr_pred,  os.path.join(mdl_dir, f"{name}_TRAIN_predictions.csv"))
        save_df(te_pred,  os.path.join(mdl_dir, f"{name}_TEST_predictions.csv"))
        save_df(comb_pred, os.path.join(mdl_dir, f"{name}_COMBINED_predictions.csv"))

        # -------- With descriptors actually used --------
        tr_desc = pd.DataFrame(Xtr_sel, columns=sel_cols)
        te_desc = pd.DataFrame(Xte_sel, columns=sel_cols)
        tr_full = pd.concat([ids_tr, tr_desc, tr_pred.drop(columns=[id_col], errors="ignore")], axis=1)
        te_full = pd.concat([ids_te, te_desc, te_pred.drop(columns=[id_col], errors="ignore")], axis=1)
        save_df(tr_full, os.path.join(mdl_dir, f"{name}_TRAIN_with_descriptors.csv"))
        save_df(te_full, os.path.join(mdl_dir, f"{name}_TEST_with_descriptors.csv"))

        # Row for global summary (focus on TEST)
        per_model_rows.append({
            "dataset": base, "model": name,
            **{f"test_{k}": v for k,v in m_te.items()},
            **{f"test_CLF_{k}": v for k,v in c_te.items()}
        })

    # Per-dataset summary
    summary = pd.DataFrame(per_model_rows).sort_values("test_R2", ascending=False)
    save_df(summary, os.path.join(out_dir, f"{base}_SUMMARY_models.csv"))

    # Add to global
    global_rows.extend(per_model_rows)

# ------------------ Global summary + ZIP ------------------
if global_rows:
    global_summary = pd.DataFrame(global_rows).sort_values(["dataset","test_R2"], ascending=[True, False])
    save_df(global_summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_all_models.csv"))

# Zip everything
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Done. All Train/Test stats + predictions-with-errors saved:")
print(f"- Folder: {RESULTS_DIR}")
print(f"- ZIP:    {ZIP_PATH}")

# ================== Train/Test QSAR: Full Stats + Predictions (multi-model, robust target detection) ==================
import os, re, json, zipfile, numpy as np, pandas as pd
from scipy import stats

from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score
)
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor

# Optional XGBoost
try:
    from xgboost import XGBRegressor
    HAVE_XGB = True
except Exception:
    HAVE_XGB = False

# ------------------ CONFIG: set your paths here ------------------
# If youâ€™re in Colab with /content/*.csv use those paths; if in this chat you uploaded to /mnt/data use those.
TRAIN_PATH = "/content/pubchem_data_train_70.csv"   # e.g. "/mnt/data/pubchem_data_train_70.csv"
TEST_PATH  = "/content/pubchem_data_test_30.csv"    # e.g. "/mnt/data/pubchem_data_test_30.csv"

# If you know the exact target column name, put it here (case-sensitive). Otherwise leave None for auto-detect.
TARGET_NAME = None

# Classification threshold on pIC50 (6.0 â‰ˆ 1 ÂµM)
CLASS_THRESHOLD = 6.0

# Feature selection (mutual information)
TOP_K_MI = 200

RESULTS_DIR = "/content/QSAR_train_test_ANALYSIS"
ZIP_PATH    = "/content/QSAR_train_test_ANALYSIS.zip"
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
os.makedirs(RESULTS_DIR, exist_ok=True)

# ------------------ helpers ------------------
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

# robust target detection
def detect_target_column(df, override_name=None):
    if override_name is not None:
        if override_name in df.columns: return override_name
        raise ValueError(f"TARGET_NAME='{override_name}' not in columns: {list(df.columns)[:40]}...")

    exacts = ["pIC50","IC50","StdValue","standard_value","standardValue","Std Value","Activity","activity"]
    for e in exacts:
        if e in df.columns: return e

    hints = ["pic50","p_ic50","ic50","stdvalue","standard_value","observed","actual","measured","label","activity","response","y"]
    ci = {c.lower(): c for c in df.columns}
    # prefer pIC50-like
    for k,orig in ci.items():
        if "pic50" in k: return orig
    # then IC50-like
    for k,orig in ci.items():
        if "ic50" in k and "pic50" not in k: return orig
    # else pick numeric hinted with largest variance
    cands = []
    for c in df.columns:
        lc = c.lower().replace(" ","").replace("-","").replace("_","")
        if any(h in lc for h in hints) and pd.api.types.is_numeric_dtype(df[c]):
            cands.append(c)
    if cands:
        return max(cands, key=lambda x: float(pd.to_numeric(df[x], errors="coerce").var()))
    raise ValueError("Could not detect target column. Set TARGET_NAME to the true activity column.")

def to_pic50(series, name_hint):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name_hint.lower(): return vals
    if ("ic50" in name_hint.lower()) or ("std" in name_hint.lower()) or ("standard" in name_hint.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals  # assume already on appropriate scale

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    return X.loc[:, X.columns[keep]], X.columns[~keep].tolist()

def metrics_reg(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float); y_pred = np.asarray(y_pred, dtype=float)
    resid = y_true - y_pred
    r2   = float(r2_score(y_true, y_pred))
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    ccc = float((2*cov)/(var_t + var_p + (mu_t - mu_p)**2 + 1e-12))
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
            "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc}

def metrics_clf(y_true_cont, y_pred_cont, thr):
    y_true = (np.asarray(y_true_cont) >= thr).astype(int)
    y_pred = (np.asarray(y_pred_cont) >= thr).astype(int)
    y_score = np.asarray(y_pred_cont)
    acc  = float(accuracy_score(y_true, y_pred))
    prec = float(precision_score(y_true, y_pred, zero_division=0))
    rec  = float(recall_score(y_true, y_pred, zero_division=0))
    f1   = float(f1_score(y_true, y_pred, zero_division=0))
    try: auroc = float(roc_auc_score(y_true, y_score))
    except: auroc = np.nan
    try: aupr  = float(average_precision_score(y_true, y_score))
    except: aupr = np.nan
    return {"ACC": acc, "Precision": prec, "Recall": rec, "F1": f1, "AUROC": auroc, "AUPRC": aupr, "Threshold": float(thr)}

def add_error_cols(df):
    df = df.copy()
    df["residual"] = df["y_true"] - df["y_pred"]
    df["abs_residual"] = df["residual"].abs()
    df["squared_error"] = df["residual"] ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        df["percent_error"] = 100.0 * (df["abs_residual"] / np.clip(np.abs(df["y_true"]), 1e-12, None))
    return df

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# ------------------ load ------------------
tr = pd.read_csv(TRAIN_PATH)
te = pd.read_csv(TEST_PATH)

id_col = detect_id(tr)
tcol_tr = detect_target_column(tr, TARGET_NAME)
tcol_te = tcol_tr if tcol_tr in te.columns else detect_target_column(te, TARGET_NAME)

y_tr = to_pic50(tr[tcol_tr], tcol_tr)
y_te = to_pic50(te[tcol_te], tcol_te)

# ------------------ build descriptors ------------------
# exclude target + id + any SMILES-like
exclude = [id_col, tcol_tr]
for smi in ["SMILES","smiles","canonical_smiles"]:
    if smi in tr.columns: exclude.append(smi)

Xtr_raw = numeric_X(tr, exclude=exclude)
Xte_raw = numeric_X(te, exclude=[c for c in exclude if c in te.columns])

# align columns
common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

# drop all-NaN
Xtr_nonan, drop_nan = drop_all_nan(Xtr_raw)
Xte_nonan = Xte_raw[Xtr_nonan.columns]

# impute median (fit on train)
imp = SimpleImputer(strategy="median")
Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

# drop constants
Xtr_const, drop_const = drop_constant(Xtr_imp)
Xte_const = Xte_imp[Xtr_const.columns]

# MI selection
k_use = min(TOP_K_MI, Xtr_const.shape[1]) if Xtr_const.shape[1] > 0 else 0
if k_use == 0:
    raise ValueError("No usable features after cleaning.")
selector = SelectKBest(mutual_info_regression, k=k_use)
Xtr_sel = selector.fit_transform(Xtr_const, y_tr)
Xte_sel = selector.transform(Xte_const)
sel_cols = Xtr_const.columns[selector.get_support()].tolist()

# ------------------ models ------------------
MODELS = {
    "RF":  RandomForestRegressor(n_estimators=800, max_depth=None, max_features="sqrt",
                                 random_state=RANDOM_STATE, n_jobs=-1),
    "SVR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", SVR(C=10, gamma=0.01, epsilon=0.1))]),
    "KRR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", KernelRidge(alpha=0.3, kernel="rbf", gamma=0.01))]),
    "ElasticNet": Pipeline([("scaler", StandardScaler(with_mean=False)),
                            ("model", ElasticNet(alpha=0.02, l1_ratio=0.4, max_iter=5000, random_state=RANDOM_STATE))]),
}
if HAVE_XGB:
    MODELS["XGB"] = XGBRegressor(
        n_estimators=900, max_depth=4, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9,
        reg_alpha=0.1, reg_lambda=1.0,
        objective="reg:squarederror", random_state=RANDOM_STATE, n_jobs=-1
    )

# ------------------ train + evaluate + save ------------------
ids_tr = tr[[id_col]].reset_index(drop=True)
ids_te = te[[id_col]].reset_index(drop=True)
per_model_rows = []
out_root = os.path.join(RESULTS_DIR, "models"); os.makedirs(out_root, exist_ok=True)

for name, model in MODELS.items():
    print(f"Training {name} ...")
    mdl_dir = os.path.join(out_root, name); os.makedirs(mdl_dir, exist_ok=True)

    model.fit(Xtr_sel, y_tr)
    p_tr = model.predict(Xtr_sel)
    p_te = model.predict(Xte_sel)

    # Regression metrics
    m_tr = metrics_reg(y_tr, p_tr)
    m_te = metrics_reg(y_te, p_te)
    m_cb = metrics_reg(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))

    # Classification metrics
    c_tr = metrics_clf(y_tr, p_tr, CLASS_THRESHOLD)
    c_te = metrics_clf(y_te, p_te, CLASS_THRESHOLD)
    c_cb = metrics_clf(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]), CLASS_THRESHOLD)

    # Metrics table
    metrics_df = pd.DataFrame([
        {"split":"train",    **m_tr, **{f"CLF_{k}": v for k,v in c_tr.items()}},
        {"split":"test",     **m_te, **{f"CLF_{k}": v for k,v in c_te.items()}},
        {"split":"combined", **m_cb, **{f"CLF_{k}": v for k,v in c_cb.items()}},
    ])
    save_df(metrics_df, os.path.join(mdl_dir, f"{name}_metrics.csv"))

    # Predictions with error columns
    tr_pred = add_error_cols(pd.concat([ids_tr, pd.DataFrame({"y_true": y_tr, "y_pred": p_tr})], axis=1))
    te_pred = add_error_cols(pd.concat([ids_te, pd.DataFrame({"y_true": y_te, "y_pred": p_te})], axis=1))
    comb_pred = pd.concat([tr_pred.assign(split="train"), te_pred.assign(split="test")], axis=0)

    save_df(tr_pred,  os.path.join(mdl_dir, f"{name}_TRAIN_predictions.csv"))
    save_df(te_pred,  os.path.join(mdl_dir, f"{name}_TEST_predictions.csv"))
    save_df(comb_pred, os.path.join(mdl_dir, f"{name}_COMBINED_predictions.csv"))

    # With descriptors used (selected features)
    tr_desc = pd.DataFrame(Xtr_sel, columns=sel_cols)
    te_desc = pd.DataFrame(Xte_sel, columns=sel_cols)
    save_df(pd.concat([ids_tr, tr_desc, tr_pred.drop(columns=[id_col], errors="ignore")], axis=1),
            os.path.join(mdl_dir, f"{name}_TRAIN_with_descriptors.csv"))
    save_df(pd.concat([ids_te, te_desc, te_pred.drop(columns=[id_col], errors="ignore")], axis=1),
            os.path.join(mdl_dir, f"{name}_TEST_with_descriptors.csv"))

    per_model_rows.append({
        "model": name,
        **{f"train_{k}": v for k,v in m_tr.items()},
        **{f"test_{k}": v for k,v in m_te.items()},
        **{f"combined_{k}": v for k,v in m_cb.items()},
        **{f"test_CLF_{k}": v for k,v in c_te.items()}
    })

# Summaries
summary = pd.DataFrame(per_model_rows).sort_values("test_R2", ascending=False)
save_df(summary, os.path.join(RESULTS_DIR, "SUMMARY_all_models.csv"))
lite_cols = ["model","test_R2","test_RMSE","test_MAE","test_Pearson_r","test_Spearman_rho","test_CCC",
             "test_CLF_ACC","test_CLF_Precision","test_CLF_Recall","test_CLF_F1","test_CLF_AUROC","test_CLF_AUPRC"]
save_df(summary[[c for c in lite_cols if c in summary.columns]],
        os.path.join(RESULTS_DIR, "SUMMARY_all_models_lite.csv"))

# Zip
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f); arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Done. All Train/Test metrics + predictions with errors saved.")
print(f"- Results folder: {RESULTS_DIR}")
print(f"- ZIP:            {ZIP_PATH}")
print(f"- Target column (train): '{tcol_tr}'  (converted to pIC50 if needed)")
print(f"- Selected features: {len(sel_cols)}  -> saved in per-model '*_with_descriptors.csv'")

# ================== Multi-Dataset QSAR Analysis â€” Full Stats + Prioritization Score (RF, SVR, KRR, ENet, XGB*) ==================
# What you get (for each dataset path pair you listed):
# - Train/Test/Combined metrics per model:
#     Regression: R2, RMSE, MAE, MAPE, Pearson_r, Spearman_rho, CCC
#     Classification (threshold on pIC50=6.0): ACC, Precision, Recall, F1, AUROC, AUPRC
# - Predictions with errors: residual, abs_residual, squared_error, percent_error (train/test/combined)
# - Selected descriptor list actually used
# - Per-dataset summary ranked by Test R2 and a composite PRIORITY_SCORE
# - Global summary across all datasets + an overall PRIORITY_SCORE so you can pick the best algorithm
# - A single ZIP package with everything
#
# Notes:
# * XGBoost is optional â€” included if python has xgboost installed. Otherwise skipped gracefully.
# * Target column is auto-detected (pIC50 preferred, IC50 auto-converted). Set TARGET_NAME per dataset to force a column if needed.

import os, re, json, zipfile, numpy as np, pandas as pd
from scipy import stats
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score
)
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor

# Optional XGB
try:
    from xgboost import XGBRegressor
    HAVE_XGB = True
except Exception:
    HAVE_XGB = False

# ---------- DATASETS YOU PROVIDED ----------
DATASETS = [
    ("KlekotaRoth_data", "/content/KlekotaRoth_data_train_70.csv", "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",       "/content/MACCS_data_train_70.csv",       "/content/MACCS_data_test_30.csv"),
    ("Substructure_data","/content/Substructure_data_train_70.csv","/content/Substructure_data_test_30.csv"),
    ("pubchem_data",     "/content/pubchem_data_train_70.csv",     "/content/pubchem_data_test_30.csv"),
]

# If a dataset has a known target column name, set here; otherwise keep None to auto-detect
FORCE_TARGET = {
    # "KlekotaRoth_data": "pIC50",
    # "MACCS_data": "pIC50",
    # ...
}

# --------------- CONFIG ---------------
CLASS_THRESHOLD = 6.0        # pIC50 cutoff for classification-style metrics
TOP_K_MI = 200               # SelectKBest by mutual information
RANDOM_STATE = 42
RESULTS_DIR = "/content/QSAR_MULTISET_FULL"
ZIP_PATH    = "/content/QSAR_MULTISET_FULL.zip"
np.random.seed(RANDOM_STATE)
os.makedirs(RESULTS_DIR, exist_ok=True)

# --------------- HELPERS ---------------
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def detect_target_column(df, override_name=None):
    if override_name is not None:
        if override_name in df.columns: return override_name
        raise ValueError(f"TARGET_NAME='{override_name}' not in columns.")
    exacts = ["pIC50","IC50","StdValue","standard_value","standardValue","Std Value","Activity","activity"]
    for e in exacts:
        if e in df.columns: return e
    hints = ["pic50","p_ic50","ic50","stdvalue","standard_value","observed","actual","measured","label","activity","response","y"]
    ci = {c.lower(): c for c in df.columns}
    for k,orig in ci.items():
        if "pic50" in k: return orig
    for k,orig in ci.items():
        if "ic50" in k and "pic50" not in k: return orig
    cands = []
    for c in df.columns:
        lc = c.lower().replace(" ","").replace("-","").replace("_","")
        if any(h in lc for h in hints) and pd.api.types.is_numeric_dtype(df[c]):
            cands.append(c)
    if cands:
        return max(cands, key=lambda x: float(pd.to_numeric(df[x], errors="coerce").var()))
    raise ValueError("Could not detect target column. Set FORCE_TARGET[name] if needed.")

def to_pic50(series, name_hint):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name_hint.lower(): return vals
    if ("ic50" in name_hint.lower()) or ("std" in name_hint.lower()) or ("standard" in name_hint.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask], list(X.columns[~mask])

def drop_constant(X):
    if X.shape[1] == 0: return X, []
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    return X.loc[:, X.columns[keep]], X.columns[~keep].tolist()

def metrics_reg(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float); y_pred = np.asarray(y_pred, dtype=float)
    resid = y_true - y_pred
    r2   = float(r2_score(y_true, y_pred))
    rmse = float(np.sqrt(np.mean(resid**2)))
    mae  = float(np.mean(np.abs(resid)))
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0)
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true), np.mean(y_pred)
    var_t, var_p = np.var(y_true), np.var(y_pred)
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p))
    ccc = float((2*cov)/(var_t + var_p + (mu_t - mu_p)**2 + 1e-12))
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
            "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc}

def metrics_clf(y_true_cont, y_pred_cont, thr):
    y_true = (np.asarray(y_true_cont) >= thr).astype(int)
    y_pred = (np.asarray(y_pred_cont) >= thr).astype(int)
    y_score = np.asarray(y_pred_cont)
    acc  = float(accuracy_score(y_true, y_pred))
    prec = float(precision_score(y_true, y_pred, zero_division=0))
    rec  = float(recall_score(y_true, y_pred, zero_division=0))
    f1   = float(f1_score(y_true, y_pred, zero_division=0))
    try: auroc = float(roc_auc_score(y_true, y_score))
    except: auroc = np.nan
    try: aupr  = float(average_precision_score(y_true, y_score))
    except: aupr = np.nan
    return {"ACC": acc, "Precision": prec, "Recall": rec, "F1": f1, "AUROC": auroc, "AUPRC": aupr}

def add_error_cols(df):
    df = df.copy()
    df["residual"] = df["y_true"] - df["y_pred"]
    df["abs_residual"] = df["residual"].abs()
    df["squared_error"] = df["residual"] ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        df["percent_error"] = 100.0 * (df["abs_residual"] / np.clip(np.abs(df["y_true"]), 1e-12, None))
    return df

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

# Composite PRIORITY_SCORE (higher is better)
# Within each dataset: min-max normalize the following (based on TEST metrics across models):
#   +R2 (weight 0.6), +Pearson_r (0.2), -RMSE (0.1), -MAE (0.1)
# Score = 0.6*R2_norm + 0.2*r_norm + 0.1*(1-RMSE_norm) + 0.1*(1-MAE_norm)
def add_priority_score(df_dataset):
    df = df_dataset.copy()
    # avoid div-by-zero: if all values equal, set norm to 0.5
    def mm(col):
        v = df[col].values
        vmin, vmax = np.nanmin(v), np.nanmax(v)
        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax == vmin:
            return np.full_like(v, 0.5, dtype=float)
        return (v - vmin) / (vmax - vmin)
    r2n   = mm("test_R2") if "test_R2" in df.columns else np.zeros(len(df))
    rn    = mm("test_Pearson_r") if "test_Pearson_r" in df.columns else np.zeros(len(df))
    rmsen = mm("test_RMSE") if "test_RMSE" in df.columns else np.zeros(len(df))
    maen  = mm("test_MAE") if "test_MAE" in df.columns else np.zeros(len(df))
    score = 0.6*r2n + 0.2*rn + 0.1*(1.0 - rmn := rmse n) + 0.1*(1.0 - man := mae n)  # will rewrite to avoid walrus clash
    # Rewrite cleanly:
    rmn  = rm sen
    man  = mae n
    score = 0.6*r2n + 0.2*rn + 0.1*(1.0 - rmn) + 0.1*(1.0 - man)
    df["PRIORITY_SCORE"] = score
    return df

# --------------- MODELS ---------------
MODELS = {
    "RF":  RandomForestRegressor(n_estimators=800, max_depth=None, max_features="sqrt",
                                 random_state=RANDOM_STATE, n_jobs=-1),
    "SVR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", SVR(C=10, gamma=0.01, epsilon=0.1))]),
    "KRR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", KernelRidge(alpha=0.3, kernel="rbf", gamma=0.01))]),
    "ElasticNet": Pipeline([("scaler", StandardScaler(with_mean=False)),
                            ("model", ElasticNet(alpha=0.02, l1_ratio=0.4, max_iter=5000, random_state=RANDOM_STATE))]),
}
if HAVE_XGB:
    MODELS["XGB"] = XGBRegressor(
        n_estimators=900, max_depth=4, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=1.0,
        objective="reg:squarederror", random_state=RANDOM_STATE, n_jobs=-1
    )

# --------------- MAIN LOOP ---------------
global_rows = []
per_dataset_priority = []

for name, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"âš ï¸ Missing files for {name}, skipping.")
        continue

    print(f"\n=== DATASET: {name} ===")
    out_dir = os.path.join(RESULTS_DIR, name); os.makedirs(out_dir, exist_ok=True)

    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)

    id_col = detect_id(tr)
    tcol_tr = detect_target_column(tr, FORCE_TARGET.get(name))
    tcol_te = tcol_tr if tcol_tr in te.columns else detect_target_column(te, FORCE_TARGET.get(name))

    y_tr = to_pic50(tr[tcol_tr], tcol_tr)
    y_te = to_pic50(te[tcol_te], tcol_te)

    # Build descriptor matrix (numeric), exclude IDs, target, and SMILES-like
    exclude_tr = [id_col, tcol_tr] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in tr.columns]
    exclude_te = [id_col, tcol_te] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in te.columns]
    Xtr_raw = numeric_X(tr, exclude=exclude_tr)
    Xte_raw = numeric_X(te, exclude=exclude_te)

    # Align columns
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw = Xtr_raw[common]; Xte_raw = Xte_raw[common]

    # Clean: drop all-NaN -> impute (train) -> drop constant -> MI select
    Xtr_nonan, drop_nan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

    Xtr_const, drop_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]

    k_use = min(TOP_K_MI, Xtr_const.shape[1]) if Xtr_const.shape[1] > 0 else 0
    if k_use == 0:
        print(f"âŒ No usable features after cleaning for {name}.")
        continue
    selector = SelectKBest(mutual_info_regression, k=k_use)
    Xtr_sel = selector.fit_transform(Xtr_const, y_tr)
    Xte_sel = selector.transform(Xte_const)
    sel_cols = Xtr_const.columns[selector.get_support()].tolist()
    save_df(pd.DataFrame({"selected_feature": sel_cols}), os.path.join(out_dir, f"{name}_selected_features.csv"))

    # IDs
    ids_tr = tr[[id_col]].reset_index(drop=True)
    ids_te = te[[id_col]].reset_index(drop=True)

    # Per-model training/eval
    per_model_rows = []
    mdl_root = os.path.join(out_dir, "models"); os.makedirs(mdl_root, exist_ok=True)

    for mdl_name, mdl in MODELS.items():
        print(f"  â†’ Training {mdl_name}")
        mdl_dir = os.path.join(mdl_root, mdl_name); os.makedirs(mdl_dir, exist_ok=True)

        mdl.fit(Xtr_sel, y_tr)
        p_tr = mdl.predict(Xtr_sel)
        p_te = mdl.predict(Xte_sel)

        # Regression metrics
        m_tr = metrics_reg(y_tr, p_tr)
        m_te = metrics_reg(y_te, p_te)
        m_cb = metrics_reg(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))

        # Classification metrics
        c_tr = metrics_clf(y_tr, p_tr, CLASS_THRESHOLD)
        c_te = metrics_clf(y_te, p_te, CLASS_THRESHOLD)
        c_cb = metrics_clf(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]), CLASS_THRESHOLD)

        # Save metrics
        metrics_df = pd.DataFrame([
            {"split":"train",    **m_tr, **{f"CLF_{k}": v for k,v in c_tr.items()}},
            {"split":"test",     **m_te, **{f"CLF_{k}": v for k,v in c_te.items()}},
            {"split":"combined", **m_cb, **{f"CLF_{k}": v for k,v in c_cb.items()}},
        ])
        save_df(metrics_df, os.path.join(mdl_dir, f"{mdl_name}_metrics.csv"))

        # Predictions with error columns
        tr_pred = add_error_cols(pd.concat([ids_tr, pd.DataFrame({"y_true": y_tr, "y_pred": p_tr})], axis=1))
        te_pred = add_error_cols(pd.concat([ids_te, pd.DataFrame({"y_true": y_te, "y_pred": p_te})], axis=1))
        comb_pred = pd.concat([tr_pred.assign(split="train"), te_pred.assign(split="test")], axis=0)
        save_df(tr_pred,  os.path.join(mdl_dir, f"{mdl_name}_TRAIN_predictions.csv"))
        save_df(te_pred,  os.path.join(mdl_dir, f"{mdl_name}_TEST_predictions.csv"))
        save_df(comb_pred, os.path.join(mdl_dir, f"{mdl_name}_COMBINED_predictions.csv"))

        # With descriptors used
        save_df(pd.concat([ids_tr, pd.DataFrame(Xtr_sel, columns=sel_cols), tr_pred.drop(columns=[id_col], errors="ignore")], axis=1),
                os.path.join(mdl_dir, f"{mdl_name}_TRAIN_with_descriptors.csv"))
        save_df(pd.concat([ids_te, pd.DataFrame(Xte_sel, columns=sel_cols), te_pred.drop(columns=[id_col], errors="ignore")], axis=1),
                os.path.join(mdl_dir, f"{mdl_name}_TEST_with_descriptors.csv"))

        per_model_rows.append({
            "dataset": name, "model": mdl_name,
            **{f"train_{k}": v for k,v in m_tr.items()},
            **{f"test_{k}": v for k,v in m_te.items()},
            **{f"combined_{k}": v for k,v in m_cb.items()},
            **{f"test_CLF_{k}": v for k,v in c_te.items()}
        })

    # Per-dataset summary + PRIORITY_SCORE
    summary_df = pd.DataFrame(per_model_rows).sort_values("test_R2", ascending=False)
    summary_df = add_priority_score(summary_df)
    summary_df = summary_df.sort_values(["PRIORITY_SCORE","test_R2"], ascending=[False, False])
    save_df(summary_df, os.path.join(out_dir, f"{name}_SUMMARY_models.csv"))

    per_dataset_priority.append(summary_df[["dataset","model","PRIORITY_SCORE","test_R2","test_RMSE","test_MAE","test_Pearson_r"]])
    global_rows.extend(per_model_rows)

# --------------- GLOBAL SUMMARIES + PRIORITIZATION ---------------
if global_rows:
    global_summary = pd.DataFrame(global_rows)
    # Priority per dataset already done; now compute global priorities by averaging per model across datasets
    # First, compute per-dataset PRIORITY_SCOREs and then average by model
    all_priority = []
    for ds in per_dataset_priority:
        all_priority.append(ds)
    if all_priority:
        pri = pd.concat(all_priority, ignore_index=True)
        global_priority = pri.groupby("model", as_index=False).agg(
            MEAN_PRIORITY_SCORE=("PRIORITY_SCORE","mean"),
            MEAN_test_R2=("test_R2","mean"),
            MEAN_test_RMSE=("test_RMSE","mean"),
            MEAN_test_MAE=("test_MAE","mean"),
            MEAN_test_Pearson_r=("test_Pearson_r","mean"),
            N_datasets=("dataset","nunique")
        ).sort_values(["MEAN_PRIORITY_SCORE","MEAN_test_R2"], ascending=[False, False])
    else:
        # fallback if no per-dataset score computed (unlikely)
        global_priority = global_summary.groupby("model", as_index=False).agg(MEAN_test_R2=("test_R2","mean"))
    save_df(global_summary.sort_values(["dataset","test_R2"], ascending=[True, False]),
            os.path.join(RESULTS_DIR, "GLOBAL_summary_all_models_raw.csv"))
    save_df(global_priority, os.path.join(RESULTS_DIR, "GLOBAL_prioritization_by_model.csv"))

# --------------- ZIP EVERYTHING ---------------
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Finished.")
print(f"- Results folder: {RESULTS_DIR}")
print(f"- ZIP:            {ZIP_PATH}")
print("Open each <dataset>_SUMMARY_models.csv to see PRIORITY_SCORE rankings per dataset,")
print("and GLOBAL_prioritization_by_model.csv for the overall best algorithm across all datasets.")

"""##Start with Regressiona AND cLASSIFICATION ANALYSIS"""

# ================== Multi-Dataset QSAR (NaN-safe) â€” Full Stats + Prioritization Score ==================
import os, re, json, zipfile, numpy as np, pandas as pd
from scipy import stats
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score
)
from sklearn.svm import SVR
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor

# Optional XGBoost
try:
    from xgboost import XGBRegressor
    HAVE_XGB = True
except Exception:
    HAVE_XGB = False

# ---------- DATASETS ----------
DATASETS = [
    ("KlekotaRoth_data", "/content/KlekotaRoth_data_train_70.csv", "/content/KlekotaRoth_data_test_30.csv"),
    ("MACCS_data",       "/content/MACCS_data_train_70.csv",       "/content/MACCS_data_test_30.csv"),
    ("Substructure_data","/content/Substructure_data_train_70.csv","/content/Substructure_data_test_30.csv"),
    ("pubchem_data",     "/content/pubchem_data_train_70.csv",     "/content/pubchem_data_test_30.csv"),
]

FORCE_TARGET = {}   # e.g., {"MACCS_data": "pIC50"}

CLASS_THRESHOLD = 6.0
TOP_K_MI = 200
RANDOM_STATE = 42
RESULTS_DIR = "/content/QSAR_MULTISET_FULL_NANSAFE"
ZIP_PATH    = "/content/QSAR_MULTISET_FULL_NANSAFE.zip"
np.random.seed(RANDOM_STATE)
os.makedirs(RESULTS_DIR, exist_ok=True)

# ------------------ HELPERS ------------------
ID_CANDIDATES = [
    "Molecule ChEMBL ID","ChEMBL_ID","chembl_id","molecule_chembl_id",
    "Molecule_ID","CompoundID","Compound_Id","Name","Molecule Name",
    "SMILES","smiles","canonical_smiles"
]

def detect_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def detect_target_column(df, override_name=None):
    if override_name is not None:
        if override_name in df.columns: return override_name
        raise ValueError(f"TARGET_NAME='{override_name}' not in columns.")
    exacts = ["pIC50","IC50","StdValue","standard_value","standardValue","Std Value","Activity","activity"]
    for e in exacts:
        if e in df.columns: return e
    hints = ["pic50","p_ic50","ic50","stdvalue","standard_value","observed","actual","measured","label","activity","response","y"]
    ci = {c.lower(): c for c in df.columns}
    for k,orig in ci.items():
        if "pic50" in k: return orig
    for k,orig in ci.items():
        if "ic50" in k and "pic50" not in k: return orig
    cands = []
    for c in df.columns:
        lc = c.lower().replace(" ","").replace("-","").replace("_","")
        if any(h in lc for h in hints) and pd.api.types.is_numeric_dtype(df[c]):
            cands.append(c)
    if cands:
        return max(cands, key=lambda x: float(pd.to_numeric(df[x], errors="coerce").var()))
    raise ValueError("Could not detect target column.")

def to_pic50(series, name_hint):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name_hint.lower():
        return vals
    if ("ic50" in name_hint.lower()) or ("std" in name_hint.lower()) or ("standard" in name_hint.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan(X):
    mask = X.notna().any(axis=0)
    return X.loc[:, mask]

def drop_constant(X):
    if X.shape[1] == 0: return X
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    keep = vt.get_support()
    return X.loc[:, X.columns[keep]]

def safe_pair_mask(y, yhat):
    y = np.asarray(y, dtype=float); yhat = np.asarray(yhat, dtype=float)
    mask = np.isfinite(y) & np.isfinite(yhat)
    return y[mask], yhat[mask], mask

def metrics_reg(y_true, y_pred):
    y_true, y_pred, _ = safe_pair_mask(y_true, y_pred)
    resid = y_true - y_pred
    r2   = float(r2_score(y_true, y_pred)) if len(y_true)>1 else np.nan
    rmse = float(np.sqrt(np.mean(resid**2))) if len(y_true)>0 else np.nan
    mae  = float(np.mean(np.abs(resid))) if len(y_true)>0 else np.nan
    with np.errstate(divide='ignore', invalid='ignore'):
        mape = float(np.mean(np.abs(resid) / np.clip(np.abs(y_true), 1e-12, None)) * 100.0) if len(y_true)>0 else np.nan
    try: pearson, _ = stats.pearsonr(y_true, y_pred)
    except: pearson = np.nan
    try: spearman, _ = stats.spearmanr(y_true, y_pred)
    except: spearman = np.nan
    mu_t, mu_p = np.mean(y_true) if len(y_true)>0 else np.nan, np.mean(y_pred) if len(y_pred)>0 else np.nan
    var_t, var_p = np.var(y_true) if len(y_true)>0 else np.nan, np.var(y_pred) if len(y_pred)>0 else np.nan
    cov = np.mean((y_true - mu_t) * (y_pred - mu_p)) if len(y_true)>0 else np.nan
    ccc = float((2*cov)/(var_t + var_p + (mu_t - mu_p)**2 + 1e-12)) if np.all(np.isfinite([cov,var_t,var_p,mu_t,mu_p])) else np.nan
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "MAPE_%": mape,
            "Pearson_r": float(pearson), "Spearman_rho": float(spearman), "CCC": ccc}

def metrics_clf(y_true_cont, y_pred_cont, thr):
    y_true, y_pred, _ = safe_pair_mask(y_true_cont, y_pred_cont)
    if len(y_true)==0:
        return {"ACC": np.nan, "Precision": np.nan, "Recall": np.nan, "F1": np.nan, "AUROC": np.nan, "AUPRC": np.nan, "Threshold": float(thr)}
    y_true_b = (y_true >= thr).astype(int)
    y_pred_b = (y_pred >= thr).astype(int)
    acc  = float(accuracy_score(y_true_b, y_pred_b))
    prec = float(precision_score(y_true_b, y_pred_b, zero_division=0))
    rec  = float(recall_score(y_true_b, y_pred_b, zero_division=0))
    f1   = float(f1_score(y_true_b, y_pred_b, zero_division=0))
    try: auroc = float(roc_auc_score(y_true_b, y_pred))
    except: auroc = np.nan
    try: aupr  = float(average_precision_score(y_true_b, y_pred))
    except: aupr = np.nan
    return {"ACC": acc, "Precision": prec, "Recall": rec, "F1": f1, "AUROC": auroc, "AUPRC": aupr, "Threshold": float(thr)}

def add_error_cols_masked(ids_df, y_true, y_pred, id_col):
    y_true, y_pred, mask = safe_pair_mask(y_true, y_pred)
    ids_use = ids_df.loc[mask].reset_index(drop=True)
    df = pd.concat([ids_use, pd.DataFrame({"y_true": y_true, "y_pred": y_pred})], axis=1)
    df["residual"] = df["y_true"] - df["y_pred"]
    df["abs_residual"] = df["residual"].abs()
    df["squared_error"] = df["residual"] ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        df["percent_error"] = 100.0 * (df["abs_residual"] / np.clip(np.abs(df["y_true"]), 1e-12, None))
    return df

def save_df(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)

def add_priority_score(df):
    df = df.copy()
    for col in ["test_R2","test_Pearson_r","test_RMSE","test_MAE"]:
        if col not in df.columns: df[col] = np.nan
    def mm(col):
        v = df[col].values
        vmin, vmax = np.nanmin(v), np.nanmax(v)
        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax == vmin:
            return np.full_like(v, 0.5, dtype=float)
        return (v - vmin) / (vmax - vmin)
    r2n   = mm("test_R2")
    rn    = mm("test_Pearson_r")
    rmsen = mm("test_RMSE")
    maen  = mm("test_MAE")
    score = 0.6*r2n + 0.2*rn + 0.1*(1.0 - rmsen) + 0.1*(1.0 - maen)
    df["PRIORITY_SCORE"] = score
    return df

# ------------------ MODELS ------------------
MODELS = {
    "RF":  RandomForestRegressor(n_estimators=800, max_depth=None, max_features="sqrt",
                                 random_state=RANDOM_STATE, n_jobs=-1),
    "SVR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", SVR(C=10, gamma=0.01, epsilon=0.1))]),
    "KRR": Pipeline([("scaler", StandardScaler(with_mean=False)),
                     ("model", KernelRidge(alpha=0.3, kernel="rbf", gamma=0.01))]),
    "ElasticNet": Pipeline([("scaler", StandardScaler(with_mean=False)),
                            ("model", ElasticNet(alpha=0.02, l1_ratio=0.4, max_iter=5000, random_state=RANDOM_STATE))]),
}
if HAVE_XGB:
    MODELS["XGB"] = XGBRegressor(
        n_estimators=900, max_depth=4, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=1.0,
        objective="reg:squarederror", random_state=RANDOM_STATE, n_jobs=-1
    )

# ------------------ MAIN LOOP ------------------
global_rows = []
per_dataset_priority = []

for name, train_path, test_path in DATASETS:
    print(f"\n=== DATASET: {name} ===")
    if not os.path.exists(train_path) or not os.path.exists(test_path):
        print(f"âš ï¸ Missing files for {name}, skipping.")
        continue

    out_dir = os.path.join(RESULTS_DIR, name); os.makedirs(out_dir, exist_ok=True)
    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)

    id_col = detect_id(tr)
    # target detection
    def _detect(df):
        try: return detect_target_column(df, FORCE_TARGET.get(name))
        except: return detect_target_column(df, None)
    tcol_tr = _detect(tr)
    tcol_te = tcol_tr if tcol_tr in te.columns else _detect(te)

    # convert to pIC50
    y_tr = to_pic50(tr[tcol_tr], tcol_tr)
    y_te = to_pic50(te[tcol_te], tcol_te)

    # drop rows with non-finite targets (NaN/inf) BEFORE feature building
    tr_mask = np.isfinite(y_tr)
    te_mask = np.isfinite(y_te)
    if (~tr_mask).any():
        print(f"  â€¢ Dropping {(~tr_mask).sum()} train rows with non-finite target.")
    if (~te_mask).any():
        print(f"  â€¢ Dropping {(~te_mask).sum()} test rows with non-finite target.")
    tr = tr.loc[tr_mask].reset_index(drop=True); y_tr = y_tr[tr_mask]
    te = te.loc[te_mask].reset_index(drop=True); y_te = y_te[te_mask]

    # build descriptors (numeric only)
    exclude_tr = [id_col, tcol_tr] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in tr.columns]
    exclude_te = [id_col, tcol_te] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in te.columns]
    Xtr_raw = numeric_X(tr, exclude=exclude_tr)
    Xte_raw = numeric_X(te, exclude=exclude_te)

    # align and clean
    common = sorted(list(set(Xtr_raw.columns).intersection(set(Xte_raw.columns))))
    Xtr_raw, Xte_raw = Xtr_raw[common], Xte_raw[common]
    Xtr_nonan = drop_all_nan(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

    Xtr_const = drop_constant(Xtr_imp)
    Xte_const = Xte_imp[Xtr_const.columns]

    k_use = min(TOP_K_MI, Xtr_const.shape[1]) if Xtr_const.shape[1]>0 else 0
    if k_use == 0:
        print("  âŒ No usable features after cleaning.")
        continue

    selector = SelectKBest(mutual_info_regression, k=k_use)
    Xtr_sel = selector.fit_transform(Xtr_const, y_tr)
    Xte_sel = selector.transform(Xte_const)
    sel_cols = Xtr_const.columns[selector.get_support()].tolist()
    save_df(pd.DataFrame({"selected_feature": sel_cols}), os.path.join(out_dir, f"{name}_selected_features.csv"))

    ids_tr = tr[[id_col]].reset_index(drop=True)
    ids_te = te[[id_col]].reset_index(drop=True)

    per_model_rows = []
    mdl_root = os.path.join(out_dir, "models"); os.makedirs(mdl_root, exist_ok=True)

    for mdl_name, mdl in MODELS.items():
        print(f"  â†’ Training {mdl_name}")
        mdl_dir = os.path.join(mdl_root, mdl_name); os.makedirs(mdl_dir, exist_ok=True)

        mdl.fit(Xtr_sel, y_tr)
        p_tr = mdl.predict(Xtr_sel)
        p_te = mdl.predict(Xte_sel)

        # Metrics (NaN-safe)
        m_tr = metrics_reg(y_tr, p_tr)
        m_te = metrics_reg(y_te, p_te)
        m_cb = metrics_reg(np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]))
        c_te = metrics_clf(y_te, p_te, CLASS_THRESHOLD)

        # Save metrics
        metrics_df = pd.DataFrame([
            {"split":"train",**m_tr},
            {"split":"test",**m_te,**{f"CLF_{k}":v for k,v in c_te.items()}},
            {"split":"combined",**m_cb}
        ])
        save_df(metrics_df, os.path.join(mdl_dir, f"{mdl_name}_metrics.csv"))

        # Save predictions with errors (masked)
        tr_pred = add_error_cols_masked(ids_tr, y_tr, p_tr, id_col)
        te_pred = add_error_cols_masked(ids_te, y_te, p_te, id_col)
        save_df(tr_pred, os.path.join(mdl_dir, f"{mdl_name}_TRAIN_predictions.csv"))
        save_df(te_pred, os.path.join(mdl_dir, f"{mdl_name}_TEST_predictions.csv"))

        per_model_rows.append({
            "dataset": name,"model": mdl_name,
            **{f"train_{k}": v for k,v in m_tr.items()},
            **{f"test_{k}": v for k,v in m_te.items()},
            **{f"combined_{k}": v for k,v in m_cb.items()},
            **{f"test_CLF_{k}": v for k,v in c_te.items()}
        })

    # Per-dataset summary + PRIORITY_SCORE
    summary_df = pd.DataFrame(per_model_rows)
    summary_df = add_priority_score(summary_df)
    summary_df = summary_df.sort_values(["PRIORITY_SCORE","test_R2"], ascending=[False,False])
    save_df(summary_df, os.path.join(out_dir, f"{name}_SUMMARY_models.csv"))

    per_dataset_priority.append(summary_df[["dataset","model","PRIORITY_SCORE","test_R2","test_RMSE","test_MAE","test_Pearson_r"]])
    global_rows.extend(per_model_rows)

# ---------- GLOBAL SUMMARY ----------
if global_rows:
    global_summary = pd.DataFrame(global_rows)
    save_df(global_summary, os.path.join(RESULTS_DIR, "GLOBAL_summary_all_models_raw.csv"))
    if per_dataset_priority:
        pri = pd.concat(per_dataset_priority, ignore_index=True)
        global_priority = pri.groupby("model", as_index=False).agg(
            MEAN_PRIORITY_SCORE=("PRIORITY_SCORE","mean"),
            MEAN_test_R2=("test_R2","mean"),
            MEAN_test_RMSE=("test_RMSE","mean"),
            MEAN_test_MAE=("test_MAE","mean"),
            MEAN_test_Pearson_r=("test_Pearson_r","mean"),
            N_datasets=("dataset","nunique")
        ).sort_values(["MEAN_PRIORITY_SCORE","MEAN_test_R2"], ascending=[False,False])
        save_df(global_priority, os.path.join(RESULTS_DIR, "GLOBAL_prioritization_by_model.csv"))

# ---------- ZIP EVERYTHING ----------
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(RESULTS_DIR, zf)

print("\nâœ… Finished (NaN-safe).")
print(f"Results folder: {RESULTS_DIR}")
print(f"ZIP package:    {ZIP_PATH}")
print("If any rows were dropped due to missing/invalid targets, counts were printed above.")

"""##Analysis
##Y-randomization
"""

# ======================= QSAR INTERPRETABILITY SUITE (RF): PubChem + MACCS =======================
# Outputs per dataset:
# - metrics.csv
# - selected_features.csv
# - TRAIN/TEST predictions with residuals
# - Plots: Yobs-Ypred (train/test), Residuals hist, Residuals vs Pred, Williams AD, PCA (train/test), RF importance
# - SHAP: summary_beeswarm.png, summary_bars.png, dependence_topK_*.png   (if shap available)
# A master summary and a ZIP are also saved.

import os, json, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ---- (Optional) SHAP import ----
HAVE_SHAP = False
try:
    import shap
    HAVE_SHAP = True
except Exception:
    HAVE_SHAP = False

# =================== CONFIG: set your four paths ===================
DATASETS = [
    ("PubChem",
     "/content/pubchem_data_train_70.csv",
     "/content/pubchem_data_test_30.csv"),
    ("MACCS",
     "/content/MACCS_data_train_70.csv",
     "/content/MACCS_data_test_30.csv"),
]
TARGET_HINTS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
TOP_K_MI = 200            # feature selection size
RANDOM_STATE = 42
OUTROOT = "/content/QSAR_RF_INTERP"
ZIP_PATH = "/content/QSAR_RF_INTERP.zip"
os.makedirs(OUTROOT, exist_ok=True)
np.random.seed(RANDOM_STATE)

# =================== helpers ===================
ID_CANDIDATES = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles"]

def detect_target_column(df):
    for e in TARGET_HINTS:
        if e in df.columns: return e
    ci = {c.lower(): c for c in df.columns}
    for k,orig in ci.items():
        if "pic50" in k: return orig
    for k,orig in ci.items():
        if "ic50" in k and "pic50" not in k: return orig
    # last resort: a numeric column with a hint
    for c in df.columns:
        lc = c.lower()
        if any(h in lc for h in ["observed","actual","label","response","y","std"]) and pd.api.types.is_numeric_dtype(df[c]):
            return c
    raise ValueError("Could not detect target column (need pIC50/IC50/etc.).")

def to_pic50(series, name):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower():
        return vals
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals

def pick_id(df):
    for c in ID_CANDIDATES:
        if c in df.columns: return c
    df["_row_id"] = np.arange(len(df))
    return "_row_id"

def numeric_X(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    return df[cols].apply(pd.to_numeric, errors="coerce")

def drop_all_nan_cols(X):
    return X.loc[:, X.notna().any(axis=0)]

def drop_constant_cols(X):
    if X.shape[1] == 0: return X
    vt = VarianceThreshold(0.0)
    vt.fit(X.fillna(0))
    return X.loc[:, X.columns[vt.get_support()]]

def metrics(y_true, y_pred):
    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)
    resid = y_true - y_pred
    out = dict(
        R2 = float(r2_score(y_true, y_pred)),
        RMSE = float(np.sqrt(mean_squared_error(y_true, y_pred))),
        MAE = float(mean_absolute_error(y_true, y_pred)),
    )
    try: out["Pearson_r"] = float(stats.pearsonr(y_true, y_pred)[0])
    except: out["Pearson_r"] = np.nan
    return out

def yobs_ypred_plot(y_true, y_pred, title, path_png):
    plt.figure(figsize=(5,5), dpi=140)
    plt.scatter(y_true, y_pred, alpha=0.5)
    lo = float(np.nanmin([y_true.min(), y_pred.min()]))
    hi = float(np.nanmax([y_true.max(), y_pred.max()]))
    plt.plot([lo,hi],[lo,hi], "--", label="45Â°")
    slope, intercept, r_value, _, _ = stats.linregress(y_true, y_pred)
    xs = np.linspace(lo,hi,100)
    plt.plot(xs, slope*xs + intercept, label=f"fit: y={slope:.2f}x+{intercept:.2f}")
    plt.xlabel("Observed pIC50"); plt.ylabel("Predicted pIC50"); plt.title(title+f"\n r={r_value:.3f}")
    plt.legend(); plt.tight_layout(); plt.savefig(path_png, bbox_inches="tight"); plt.close()

def residual_plots(y_true, y_pred, prefix_png):
    resid = y_true - y_pred
    # histogram
    plt.figure(figsize=(5,4), dpi=140)
    plt.hist(resid, bins=30, alpha=0.75)
    plt.axvline(0, color="k", linestyle="--")
    plt.title("Residuals Histogram"); plt.xlabel("Residual (y_true - y_pred)"); plt.ylabel("Count")
    plt.tight_layout(); plt.savefig(prefix_png+"_residual_hist.png", bbox_inches="tight"); plt.close()
    # residual vs predicted
    plt.figure(figsize=(5,4), dpi=140)
    plt.scatter(y_pred, resid, alpha=0.5)
    plt.axhline(0, color="k", linestyle="--")
    plt.title("Residuals vs Predicted"); plt.xlabel("Predicted pIC50"); plt.ylabel("Residual")
    plt.tight_layout(); plt.savefig(prefix_png+"_residual_vs_pred.png", bbox_inches="tight"); plt.close()

def williams_plot(X_train_scaled, y_train, yhat_train, X_all_scaled, y_all, yhat_all, out_png):
    # leverage h = x^T (X'X)^{-1} x  (use scaled features)
    XtX_inv = np.linalg.pinv(X_train_scaled.T @ X_train_scaled)
    h_all = np.einsum("ij,jk,ik->i", X_all_scaled, XtX_inv, X_all_scaled)
    resid_all = y_all - yhat_all
    # standardized residuals (using train RMSE)
    rmse_tr = np.sqrt(np.mean((y_train - yhat_train)**2))
    sr_all = resid_all / (rmse_tr + 1e-12)
    # threshold h*
    p = X_train_scaled.shape[1]
    n = X_train_scaled.shape[0]
    h_star = 3 * p / max(n, 1)

    plt.figure(figsize=(6,5), dpi=140)
    plt.scatter(h_all, sr_all, alpha=0.6)
    plt.axhline(3, color="r", linestyle="--"); plt.axhline(-3, color="r", linestyle="--")
    plt.axvline(h_star, color="r", linestyle="--", label=f"h*={h_star:.3f}")
    plt.xlabel("Leverage (h)"); plt.ylabel("Standardized Residuals")
    plt.title("Williams Plot (Applicability Domain)")
    plt.legend(); plt.tight_layout(); plt.savefig(out_png, bbox_inches="tight"); plt.close()

def pca_plot(X_train, X_test, out_png, n_components=2):
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xtr_s = scaler.fit_transform(X_train)
    Xte_s = scaler.transform(X_test)
    pca = PCA(n_components=n_components, random_state=RANDOM_STATE)
    Ztr = pca.fit_transform(Xtr_s)
    Zte = pca.transform(Xte_s)
    plt.figure(figsize=(6,5), dpi=140)
    plt.scatter(Ztr[:,0], Ztr[:,1], alpha=0.5, label="TRAIN")
    plt.scatter(Zte[:,0], Zte[:,1], alpha=0.5, label="TEST")
    ev = pca.explained_variance_ratio_
    plt.xlabel(f"PC1 ({ev[0]*100:.1f}% var)"); plt.ylabel(f"PC2 ({ev[1]*100:.1f}% var)")
    plt.title("PCA: chemical space (train vs test)")
    plt.legend(); plt.tight_layout(); plt.savefig(out_png, bbox_inches="tight"); plt.close()

def shap_plots(model, X_train_df, outdir, topk=15):
    # TreeExplainer for RF
    explainer = shap.TreeExplainer(model)
    sv = explainer.shap_values(X_train_df)
    # summary beeswarm
    plt.figure(figsize=(7,5), dpi=140)
    shap.summary_plot(sv, X_train_df, show=False)
    plt.tight_layout(); plt.savefig(os.path.join(outdir,"SHAP_summary_beeswarm.png"), bbox_inches="tight"); plt.close()
    # summary bar
    plt.figure(figsize=(7,5), dpi=140)
    shap.summary_plot(sv, X_train_df, plot_type="bar", show=False, max_display=topk)
    plt.tight_layout(); plt.savefig(os.path.join(outdir,"SHAP_summary_bars.png"), bbox_inches="tight"); plt.close()
    # dependence plots for topK
    # compute mean |shap|
    mean_abs = np.abs(sv).mean(axis=0)
    top_idx = np.argsort(-mean_abs)[:topk]
    top_feats = [X_train_df.columns[i] for i in top_idx]
    for f in top_feats:
        plt.figure(figsize=(5,4), dpi=140)
        shap.dependence_plot(f, sv, X_train_df, show=False)
        plt.tight_layout(); plt.savefig(os.path.join(outdir, f"SHAP_dependence_{f}.png"), bbox_inches="tight"); plt.close()

# =================== main ===================
master_rows = []
for ds_name, train_path, test_path in DATASETS:
    if not (os.path.exists(train_path) and os.path.exists(test_path)):
        print(f"âš ï¸ Missing files for {ds_name}; skipping.")
        continue

    print(f"\n=== {ds_name} ===")
    outdir = os.path.join(OUTROOT, ds_name); os.makedirs(outdir, exist_ok=True)

    tr = pd.read_csv(train_path)
    te = pd.read_csv(test_path)

    id_col = pick_id(tr)
    tcol_tr = detect_target_column(tr)
    tcol_te = tcol_tr if tcol_tr in te.columns else detect_target_column(te)

    y_tr = to_pic50(tr[tcol_tr], tcol_tr)
    y_te = to_pic50(te[tcol_te], tcol_te)

    # Drop non-finite targets
    tr = tr[np.isfinite(y_tr)].reset_index(drop=True); y_tr = y_tr[np.isfinite(y_tr)]
    te = te[np.isfinite(y_te)].reset_index(drop=True); y_te = y_te[np.isfinite(y_te)]

    # Build X (numeric), exclude obvious non-features
    exclude_tr = [id_col, tcol_tr] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in tr.columns]
    exclude_te = [id_col, tcol_te] + [c for c in ["SMILES","smiles","canonical_smiles"] if c in te.columns]
    Xtr_raw = numeric_X(tr, exclude_tr)
    Xte_raw = numeric_X(te, exclude_te)

    # Align cols and clean
    common = sorted(list(set(Xtr_raw.columns) & set(Xte_raw.columns)))
    Xtr_raw, Xte_raw = Xtr_raw[common], Xte_raw[common]
    Xtr_nonan = drop_all_nan_cols(Xtr_raw)
    Xte_nonan = Xte_raw[Xtr_nonan.columns]

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_nonan), columns=Xtr_nonan.columns)
    Xte_imp = pd.DataFrame(imp.transform(Xte_nonan), columns=Xtr_nonan.columns)

    Xtr_noconst = drop_constant_cols(Xtr_imp)
    Xte_noconst = Xte_imp[Xtr_noconst.columns]

    k_use = min(TOP_K_MI, Xtr_noconst.shape[1]) if Xtr_noconst.shape[1] > 0 else 0
    if k_use == 0:
        print("  âŒ No usable features after cleaning; skipping dataset.")
        continue

    selector = SelectKBest(mutual_info_regression, k=k_use)
    Xtr_sel = selector.fit_transform(Xtr_noconst, y_tr)
    Xte_sel = selector.transform(Xte_noconst)
    sel_cols = Xtr_noconst.columns[selector.get_support()].tolist()
    pd.DataFrame({"selected_feature": sel_cols}).to_csv(os.path.join(outdir,"selected_features.csv"), index=False)

    # Scale (for AD/PCA computations)
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xtr_s = scaler.fit_transform(Xtr_noconst[sel_cols])
    Xte_s = scaler.transform(Xte_noconst[sel_cols])

    # ----- Train RF -----
    rf = RandomForestRegressor(n_estimators=800, max_features="sqrt", random_state=RANDOM_STATE, n_jobs=-1)
    rf.fit(Xtr_sel, y_tr)
    p_tr = rf.predict(Xtr_sel)
    p_te = rf.predict(Xte_sel)

    # ----- Metrics -----
    m_tr = metrics(y_tr, p_tr)
    m_te = metrics(y_te, p_te)
    pd.DataFrame([{"split":"TRAIN", **m_tr}, {"split":"TEST", **m_te}]).to_csv(os.path.join(outdir,"metrics.csv"), index=False)

    # ----- Save predictions -----
    ids_tr = tr[[id_col]].reset_index(drop=True)
    ids_te = te[[id_col]].reset_index(drop=True)
    df_tr = pd.concat([ids_tr, pd.DataFrame({"y_true": y_tr, "y_pred": p_tr})], axis=1)
    df_te = pd.concat([ids_te, pd.DataFrame({"y_true": y_te, "y_pred": p_te})], axis=1)
    for dfp, tag in [(df_tr,"TRAIN"), (df_te,"TEST")]:
        dfp["residual"] = dfp["y_true"] - dfp["y_pred"]
        dfp["abs_residual"] = dfp["residual"].abs()
        dfp.to_csv(os.path.join(outdir, f"{tag}_predictions.csv"), index=False)

    # ----- Plots: Y-obs vs Y-pred -----
    yobs_ypred_plot(y_tr, p_tr, f"{ds_name} RF â€” TRAIN (R2={m_tr['R2']:.3f})", os.path.join(outdir,"YobsYpred_TRAIN.png"))
    yobs_ypred_plot(y_te, p_te, f"{ds_name} RF â€” TEST (R2={m_te['R2']:.3f})", os.path.join(outdir,"YobsYpred_TEST.png"))
    residual_plots(y_te, p_te, os.path.join(outdir,"TEST"))

    # ----- Williams plot (Applicability Domain) -----
    williams_plot(Xtr_s, y_tr, p_tr, np.vstack([Xtr_s, Xte_s]), np.concatenate([y_tr, y_te]), np.concatenate([p_tr, p_te]),
                  os.path.join(outdir,"Williams_AD.png"))

    # ----- PCA (train vs test) -----
    pca_plot(Xtr_noconst[sel_cols].values, Xte_noconst[sel_cols].values, os.path.join(outdir,"PCA_train_vs_test.png"))

    # ----- Feature importance (RF) -----
    imp_vals = rf.feature_importances_
    imp_df = pd.DataFrame({"feature": sel_cols, "importance": imp_vals}).sort_values("importance", ascending=False)
    imp_df.to_csv(os.path.join(outdir,"RF_feature_importance.csv"), index=False)
    plt.figure(figsize=(6,5), dpi=140)
    top = imp_df.head(25)[::-1]
    plt.barh(top["feature"], top["importance"])
    plt.title("RF Feature Importance (top 25)")
    plt.tight_layout(); plt.savefig(os.path.join(outdir,"RF_feature_importance_top25.png"), bbox_inches="tight"); plt.close()

    # ----- SHAP (if available) -----
    if HAVE_SHAP:
        try:
            shap_plots(rf, pd.DataFrame(Xtr_sel, columns=sel_cols), outdir, topk=15)
        except Exception as e:
            with open(os.path.join(outdir,"_shap_error.txt"), "w") as f:
                f.write(str(e))

    # master summary row
    master_rows.append({
        "dataset": ds_name,
        "n_train": len(y_tr), "n_test": len(y_te),
        "n_features_selected": len(sel_cols),
        **{f"train_{k}": v for k,v in m_tr.items()},
        **{f"test_{k}": v for k,v in m_te.items()}
    })

# ----- Save master summary + ZIP -----
if master_rows:
    ms = pd.DataFrame(master_rows).sort_values(["dataset"])
    ms.to_csv(os.path.join(OUTROOT,"MASTER_summary.csv"), index=False)

def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            full = os.path.join(root, f)
            arc = os.path.relpath(full, path)
            ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nâœ… All done.")
print(f"Results folder: {OUTROOT}")
print(f"ZIP package:    {ZIP_PATH}")
print("Tip: If SHAP plots are missing, install it in Colab with:  !pip -q install shap")

"""##Again analysis"""

# ===================== Step 1: Fingerprint Redundancy Audit (Eigenvalue-Entropy + Correlation), 600 DPI =====================
# Input: ONE combined CSV per dataset with columns: split (TRAIN/TEST), target (pIC50 or IC50), descriptors (numeric), optional IDs/pred/residuals
# Output:
#   - <DATASET>_redundancy_report.csv  (feature, delta_entropy, max_abs_corr, keep_flag)
#   - 600 DPI figures: delta-entropy ranking, delta-entropy vs correlation
#   - <DATASET>_reduced.csv            (same rows/columns as input but redundant descriptor columns removed)
#
# How to use: set DATASET_NAME and INPUT_PATH. Choose KEEP_FRACTION (e.g., 0.80 keeps top 80% most-informative features).

import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import zscore
from numpy.linalg import svd

# ---------------- CONFIG ----------------
DATASET_NAME = "PubChem"  # change to "MACCS" and rerun for MACCS
INPUT_PATH   = "/content/pubchem_testandtrain.csv"  # change to the other file when you switch
OUTDIR       = f"/content/REDUNDANCY_{DATASET_NAME}"
KEEP_FRACTION = 0.80          # keep top 80% by Î”Entropy (adjust 0.7~0.9 as you like)
RANDOM_STATE = 42

os.makedirs(OUTDIR, exist_ok=True)

TARGET_CANDIDATES = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDIDATES = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles","CompoundID","Compound_Id"]

def detect_target(df):
    for c in TARGET_CANDIDATES:
        if c in df.columns: return c
    # fuzzy fallback
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    raise ValueError("Target column (pIC50/IC50/etc.) not found.")

def to_pic50(series, name):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower():
        return vals
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals

def pick_id_cols(df):
    ids = [c for c in ID_CANDIDATES if c in df.columns]
    return ids

def get_descriptor_matrix(df, exclude_cols):
    cols = [c for c in df.columns if c not in exclude_cols]
    X = df[cols].apply(pd.to_numeric, errors="coerce")
    return X, cols

def eigen_entropy(Xs):
    # Xs: standardized matrix (n x p)
    # Use SVD to get singular values -> eigenvalues of covariance ~ s^2
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = (s**2)
    p = lam / np.sum(lam)  # normalized eigenvalues
    # Shannon entropy of eigen-spectrum
    ent = -np.sum(p * np.log(p + 1e-12))
    return ent, lam

def max_abs_corr(X):
    # compute maximum absolute Pearson correlation per feature (fast enough for p<=~1000)
    Xc = (X - X.mean(0)) / (X.std(0) + 1e-12)
    C = np.nan_to_num(np.corrcoef(Xc, rowvar=False), nan=0.0)
    np.fill_diagonal(C, 0.0)
    return np.max(np.abs(C), axis=0)

# ---------------- Load & prepare ----------------
df = pd.read_csv(INPUT_PATH)
if "split" not in df.columns:
    raise ValueError("Input must have a 'split' column with TRAIN/TEST.")

tcol = detect_target(df)
y_all = to_pic50(df[tcol], tcol)
df = df.assign(_y=y_all)

# use TRAIN only to estimate redundancy (avoids test leakage)
dft = df[df["split"].str.upper()=="TRAIN"].reset_index(drop=True)

# Identify columns to exclude from descriptors
exclude_cols = set(["split", "_y", tcol, "pred_pIC50", "residual"])
exclude_cols |= set(pick_id_cols(df))

# Build numeric descriptor matrix
X_raw, feat_cols = get_descriptor_matrix(dft, exclude_cols=exclude_cols)
# Drop all-NaN columns
mask_any = X_raw.notna().any(axis=0)
X_raw = X_raw.loc[:, mask_any]
feat_cols = [c for c,m in zip(feat_cols, mask_any) if m]

# Impute medians and drop constants
imp = SimpleImputer(strategy="median")
X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=feat_cols)
vt = VarianceThreshold(0.0)
vt.fit(X_imp.fillna(0))
keep_mask = vt.get_support()
X_imp = X_imp.loc[:, keep_mask]
feat_cols = [c for c,k in zip(feat_cols, keep_mask) if k]

if X_imp.shape[1] < 3:
    raise ValueError("Too few usable descriptor columns after cleaning.")

# Standardize (mean=0,std=1)
scaler = StandardScaler(with_mean=True, with_std=True)
Xs = scaler.fit_transform(X_imp.values)

# ---------------- Eigenvalue-Entropy full ----------------
H_full, lam_full = eigen_entropy(Xs)

# ---------------- Î”Entropy per feature: H_full - H_minus_j ----------------
# (O(p) SVDs; acceptable for typical PubChem/MACCS sizes)
delta_entropy = []
for j in range(Xs.shape[1]):
    X_minus = np.delete(Xs, j, axis=1)
    H_minus, _ = eigen_entropy(X_minus)
    delta_entropy.append(H_full - H_minus)
delta_entropy = np.array(delta_entropy)

# ---------------- Correlation redundancy per feature ----------------
max_corr = max_abs_corr(pd.DataFrame(Xs, columns=feat_cols))

# ---------------- Rank & decide keep/drop ----------------
rep = pd.DataFrame({
    "feature": feat_cols,
    "delta_entropy": delta_entropy,
    "max_abs_corr": max_corr
}).sort_values("delta_entropy", ascending=False).reset_index(drop=True)

# keep top fraction by delta_entropy
k_keep = max(1, int(np.ceil(KEEP_FRACTION * len(rep))))
rep["keep_flag"] = 0
rep.loc[:k_keep-1, "keep_flag"] = 1

report_path = os.path.join(OUTDIR, f"{DATASET_NAME}_redundancy_report.csv")
rep.to_csv(report_path, index=False)

# ---------------- Plots (600 DPI) ----------------
plt.figure(figsize=(7,4))
plt.plot(np.arange(len(rep))+1, rep["delta_entropy"].values, marker=".", linewidth=1)
plt.xlabel("Feature rank (by Î”Entropy)")
plt.ylabel("Î”Entropy (H_full âˆ’ H_âˆ’j)")
plt.title(f"{DATASET_NAME}: Î”Entropy ranking (keep top {KEEP_FRACTION*100:.0f}%)")
plt.tight_layout()
plt.savefig(os.path.join(OUTDIR, f"{DATASET_NAME}_DeltaEntropyRanking_600dpi.png"), dpi=600, bbox_inches="tight")
plt.close()

plt.figure(figsize=(6,5))
plt.scatter(rep["delta_entropy"].values, rep["max_abs_corr"].values, s=12, alpha=0.7)
plt.axvline(rep["delta_entropy"].iloc[k_keep-1], color="crimson", linestyle="--", label=f"keep cutoff @ {KEEP_FRACTION*100:.0f}%")
plt.xlabel("Î”Entropy")
plt.ylabel("Max |corr| with any other feature")
plt.title(f"{DATASET_NAME}: Î”Entropy vs Correlation")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(OUTDIR, f"{DATASET_NAME}_DeltaEntropy_vs_Corr_600dpi.png"), dpi=600, bbox_inches="tight")
plt.close()

# ---------------- Export reduced combined CSV ----------------
# Apply same column selection to the WHOLE combined dataframe (TRAIN+TEST), preserving all non-descriptor columns
keep_features = rep.loc[rep["keep_flag"]==1, "feature"].tolist()

non_desc_cols = [c for c in df.columns if c not in feat_cols]  # id, split, target, pred, residual, etc.
out_cols = non_desc_cols + keep_features
reduced = df[out_cols].copy()

reduced_path = os.path.join(OUTDIR, f"{DATASET_NAME}_reduced.csv")
reduced.to_csv(reduced_path, index=False)

print("âœ… Redundancy audit complete.")
print("Report:", report_path)
print("Reduced CSV:", reduced_path)
print(f"Kept {len(keep_features)}/{len(rep)} features ({KEEP_FRACTION*100:.0f}%).")

# ===================== Step 1: Redundancy Audit (Eigen-Entropy + Correlation) with FLEXIBLE 'split' =====================
# Works when split column is named like: split / Split / set / Set / SET / subset, with values Train/Test (any case).
# Inputs (combined files):
#   /mnt/data/pubchem_testandtrain.csv
#   /mnt/data/MACCS_testandtrain.csv
# Outputs per dataset under /content/REDUNDANCY_<name>/ :
#   - <name>_redundancy_report.csv  (feature, delta_entropy, max_abs_corr, keep_flag)
#   - <name>_DeltaEntropyRanking_600dpi.png
#   - <name>_DeltaEntropy_vs_Corr_600dpi.png
#   - <name>_reduced.csv  (same rows/columns as input but with redundant descriptor columns removed)

import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from numpy.linalg import svd

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", combined="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   combined="/content/MACCS_testandtrain.csv"),
]
KEEP_FRACTION = 0.80  # keep top 80% by Î”Entropy (adjust 0.7â€“0.9 as you prefer)

TARGET_CANDS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDS     = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles","CompoundID","Compound_Id"]

def detect_target(df):
    for c in TARGET_CANDS:
        if c in df.columns: return c
    # fuzzy fallback
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    raise ValueError("Target column (pIC50/IC50/etc.) not found.")

def to_pic50(series, name):
    vals = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower(): return vals
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(vals, 1e-12, None))
    return vals

def detect_split_col(df):
    # Accept common variants: split / set / subset, any case, strip spaces
    candidates = []
    for c in df.columns:
        key = c.strip().lower()
        if key in ["split","set","subset","partition","fold"]:
            candidates.append(c)
    if candidates:
        # pick the one with Train/Test-like values present
        for c in candidates:
            vals = df[c].astype(str).str.strip().str.lower().unique().tolist()
            if any(v in vals for v in ["train","test","tr","te"]):
                return c
        return candidates[0]
    raise ValueError("Could not find a split column (expected something like 'split'/'Set' with Train/Test).")

def normalize_split(s):
    s = s.astype(str).str.strip().str.lower()
    s = s.replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def id_cols(df):
    return [c for c in ID_CANDS if c in df.columns]

def build_descriptor_matrix(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    X = df[cols].apply(pd.to_numeric, errors="coerce")
    # drop all-NaN columns
    keep = X.notna().any(axis=0)
    return X.loc[:, keep], [c for c,k in zip(cols, keep) if k]

def eigen_entropy(Xs):
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = s**2
    p = lam / np.sum(lam)
    return -np.sum(p * np.log(p + 1e-12))

def max_abs_corr(X):
    Xc = (X - X.mean(0)) / (X.std(0) + 1e-12)
    C = np.corrcoef(Xc, rowvar=False)
    C = np.nan_to_num(C, nan=0.0)
    np.fill_diagonal(C, 0.0)
    return np.max(np.abs(C), axis=0)

def run_one(name, path):
    outdir = f"/content/REDUNDANCY_{name}"
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    tcol = detect_target(df)
    # normalize split
    split_col = detect_split_col(df)
    df["_split"] = normalize_split(df[split_col])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: split column '{split_col}' must contain Train/Test values.")

    # keep original columns; create unified pIC50 vector (not used for redundancy calc beyond subset)
    df["_y"] = to_pic50(df[tcol], tcol)

    # TRAIN subset for redundancy estimation
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)

    # exclude IDs, target, any prediction/residual, SMILES-like
    excl = set(["_split","_y",tcol,"pred_pIC50","residual"])
    excl |= set(id_cols(df))
    for s in ["SMILES","smiles","canonical_smiles"]:
        if s in df.columns: excl.add(s)

    # descriptor matrix
    X_raw, feat_cols = build_descriptor_matrix(dft, exclude=excl)
    if X_raw.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after excluding non-features.")

    # impute + drop constants
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=feat_cols)
    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    keep_mask = vt.get_support()
    X_imp = X_imp.loc[:, keep_mask]
    feat_cols = [c for c,k in zip(feat_cols, keep_mask) if k]
    if X_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after dropping constants.")

    # standardize
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xs = scaler.fit_transform(X_imp.values)

    # full entropy
    H_full = eigen_entropy(Xs)

    # delta entropy per feature
    delta = np.zeros(Xs.shape[1], float)
    for j in range(Xs.shape[1]):
        X_minus = np.delete(Xs, j, axis=1)
        delta[j] = H_full - eigen_entropy(X_minus)

    # max |corr| per feature
    maxcorr = max_abs_corr(pd.DataFrame(Xs, columns=feat_cols))

    rep = pd.DataFrame({"feature": feat_cols,
                        "delta_entropy": delta,
                        "max_abs_corr": maxcorr}).sort_values("delta_entropy", ascending=False).reset_index(drop=True)

    k_keep = max(1, int(np.ceil(KEEP_FRACTION * len(rep))))
    cutoff = rep["delta_entropy"].iloc[k_keep-1]
    rep["keep_flag"] = (rep["delta_entropy"] >= cutoff).astype(int)

    # save report
    rep_path = os.path.join(outdir, f"{name}_redundancy_report.csv")
    rep.to_csv(rep_path, index=False)

    # 600 DPI plots
    plt.figure(figsize=(7,4))
    plt.plot(np.arange(len(rep))+1, rep["delta_entropy"].values, marker=".", linewidth=1)
    plt.xlabel("Feature rank (by Î”Entropy)")
    plt.ylabel("Î”Entropy (H_full âˆ’ H_âˆ’j)")
    plt.title(f"{name}: Î”Entropy ranking (keep top {KEEP_FRACTION*100:.0f}%)")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f"{name}_DeltaEntropyRanking_600dpi.png"), dpi=600, bbox_inches="tight")
    plt.close()

    plt.figure(figsize=(6,5))
    plt.scatter(rep["delta_entropy"].values, rep["max_abs_corr"].values, s=12, alpha=0.7)
    plt.axvline(cutoff, color="crimson", linestyle="--", label=f"keep cutoff @ {KEEP_FRACTION*100:.0f}%")
    plt.xlabel("Î”Entropy")
    plt.ylabel("Max |corr| with any other feature")
    plt.title(f"{name}: Î”Entropy vs Correlation")
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f"{name}_DeltaEntropy_vs_Corr_600dpi.png"), dpi=600, bbox_inches="tight")
    plt.close()

    # build reduced combined CSV (TRAIN+TEST), preserving all non-descriptor cols
    desc_all = [c for c in df.columns if c not in excl]
    keep_set = set(rep.loc[rep["keep_flag"]==1, "feature"].tolist())
    desc_keep = [c for c in desc_all if c in keep_set]
    non_desc_cols = [c for c in df.columns if c not in desc_all]
    out_cols = non_desc_cols + desc_keep
    reduced = df[out_cols].copy()
    reduced_path = os.path.join(outdir, f"{name}_reduced.csv")
    reduced.to_csv(reduced_path, index=False)

    print(f"âœ… {name}: redundancy audit complete.")
    print("  Report:      ", rep_path)
    print("  Î”Entropy fig:", os.path.join(outdir, f"{name}_DeltaEntropyRanking_600dpi.png"))
    print("  Corr fig:    ", os.path.join(outdir, f"{name}_DeltaEntropy_vs_Corr_600dpi.png"))
    print("  Reduced CSV: ", reduced_path)
    print(f"  Kept {len(desc_keep)}/{len(rep)} features ({KEEP_FRACTION*100:.0f}%).")

# ---- Run for both datasets ----
for spec in DATASETS:
    run_one(spec["name"], spec["combined"])

# ===================== Eigenvalue & Entropy Figure (600 DPI) â€” per dataset =====================
# Input: ONE combined CSV per dataset with TRAIN/TEST split, target (pIC50 or IC50), descriptors (numeric).
# Output (per dataset under /content/EIGEN_<name>/):
#   - <name>_panelA_eigenvalues_top10_600dpi.png
#   - <name>_panelB_entropy_distribution_600dpi.png
#   - <name>_eigen_spectrum.csv                  (all normalized eigenvalues)
#   - <name>_entropy_leave1out.csv               (H_full and H_minus_j per feature)
#
# How this matches the paper (Kuwahara & Gao, 2021):
#   (a) plots the first 10 normalized eigenvalues of the TRAIN descriptor matrix.
#   (b) shows the distribution of eigenvalue-based entropy after removing each single feature;
#       the dashed vertical line is the TRAIN "original" entropy (no feature removed).

import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from numpy.linalg import svd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold

# ---------- CONFIG: set your two files ----------
DATASETS = [
    dict(name="MACCS",   combined="/content/MACCS_testandtrain.csv"),
    dict(name="PubChem", combined="/content/pubchem_testandtrain.csv"),
]

TARGET_CANDS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDS     = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles","CompoundID","Compound_Id"]

def detect_target(df):
    for c in TARGET_CANDS:
        if c in df.columns: return c
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    raise ValueError("Target column not found (need pIC50/IC50/etc.).")

def to_pic50(series, name):
    v = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower(): return v
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(v, 1e-12, None))
    return v

def detect_split_col(df):
    for c in df.columns:
        key = c.strip().lower()
        if key in ["set","split","subset","partition","fold"]:
            return c
    raise ValueError("Could not find a split column (e.g., 'Set'/'split').")

def normalize_split(s):
    s = s.astype(str).str.strip().str.lower()
    s = s.replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def id_cols(df):
    return [c for c in ID_CANDS if c in df.columns]

def build_descriptor_matrix(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    X = df[cols].apply(pd.to_numeric, errors="coerce")
    keep = X.notna().any(axis=0)            # drop all-NaN cols
    X = X.loc[:, keep]
    feats = [c for c,k in zip(cols, keep) if k]
    return X, feats

def eigen_entropy_from_Xs(Xs):
    # Xs: standardized matrix (n x p)
    # SVD â†’ singular values s â†’ eigenvalues of covariance âˆ s^2
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = s**2
    p = lam / np.sum(lam)                   # normalized eigenvalues (spectrum)
    H = -np.sum(p * np.log(p + 1e-12))      # Shannon entropy of eigen-spectrum
    return H, p

def make_figures_for_dataset(name, path):
    outdir = f"/content/EIGEN_{name}"
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    tcol = detect_target(df)
    split_col = detect_split_col(df)
    df["_split"] = normalize_split(df[split_col])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: split column must contain Train/Test values.")

    # TRAIN rows only (per paper)
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    y_tr = to_pic50(dft[tcol], tcol)

    # Exclude non-descriptors
    excl = set(["_split", tcol, "pred_pIC50", "residual"])
    excl |= set(id_cols(df))
    for s in ["SMILES","smiles","canonical_smiles"]:
        if s in df.columns: excl.add(s)

    # Descriptor matrix (TRAIN)
    X_raw, feat_cols = build_descriptor_matrix(dft, exclude=excl)
    if X_raw.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after cleaning.")

    # Impute + drop constants
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=feat_cols)
    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    keep_mask = vt.get_support()
    X_imp = X_imp.loc[:, keep_mask]
    feat_cols = [c for c,k in zip(feat_cols, keep_mask) if k]
    if X_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after dropping constants.")

    # Standardize
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xs = scaler.fit_transform(X_imp.values)

    # (a) Full eigen spectrum & top-10 bars
    H_full, p_full = eigen_entropy_from_Xs(Xs)
    eig_df = pd.DataFrame({"component": np.arange(1, len(p_full)+1), "normalized_eigenvalue": p_full})
    eig_df.to_csv(os.path.join(outdir, f"{name}_eigen_spectrum.csv"), index=False)

    # Plot panel (a): first 10 normalized eigenvalues
    top_k = min(10, len(p_full))
    plt.figure(figsize=(6.0, 3.8))
    plt.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    plt.xlabel("component"); plt.ylabel("normalized eigenvalue")
    plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f"{name}_panelA_eigenvalues_top10_600dpi.png"), dpi=600, bbox_inches="tight")
    plt.close()

    # (b) Leave-one-out entropy distribution (remove each feature once)
    H_minus = np.zeros(len(feat_cols), dtype=float)
    for j in range(Xs.shape[1]):
        X_minus = np.delete(Xs, j, axis=1)
        Hj, _ = eigen_entropy_from_Xs(X_minus)
        H_minus[j] = Hj

    ent_df = pd.DataFrame({
        "feature": feat_cols,
        "H_full": H_full,
        "H_minus_j": H_minus
    })
    ent_df.to_csv(os.path.join(outdir, f"{name}_entropy_leave1out.csv"), index=False)

    # Density-like histogram + vertical line at original entropy
    plt.figure(figsize=(6.0, 3.8))
    plt.hist(H_minus, bins=60, density=True, alpha=0.6, color="#f4a3a3", edgecolor="k", linewidth=0.3)
    plt.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5, label=f"original entropy = {H_full:.6f}")
    plt.xlabel("entropy"); plt.ylabel("density"); plt.title(f"{name}")
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f"{name}_panelB_entropy_distribution_600dpi.png"), dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: figures saved in {outdir}")
    print("  â€¢ panel (a):", os.path.join(outdir, f"{name}_panelA_eigenvalues_top10_600dpi.png"))
    print("  â€¢ panel (b):", os.path.join(outdir, f"{name}_panelB_entropy_distribution_600dpi.png"))
    print("  â€¢ data csvs:", os.path.join(outdir, f"{name}_eigen_spectrum.csv"),
          " & ", os.path.join(outdir, f"{name}_entropy_leave1out.csv"))
    print(f"  â€¢ H_full = {H_full:.6f} , n_train = {Xs.shape[0]}, p_features = {Xs.shape[1]}")

# ---- Run for both datasets ----
for spec in DATASETS:
    make_figures_for_dataset(spec["name"], spec["combined"])

# ===================== Eigenvalue & Entropy Analysis (Paper-style) =====================
# Datasets: PubChem + MACCS combined CSVs with a Train/Test column (any of: Set/split/subset/partition/fold).
# Outputs per dataset (under /content/EIGEN_<name>/):
#   - <name>_panelA_eigenvalues_top10_600dpi.png
#   - <name>_panelB_entropy_distribution_600dpi.png  (non-overlapping label)
#   - <name>_figure_combined_2x1_600dpi.png         (paper-like panel a+b)
#   - <name>_eigen_spectrum.csv                     (all normalized eigenvalues)
#   - <name>_entropy_leave1out.csv                  (H_full and H_minus_j per feature)
# Final ZIP: /content/EIGEN_ENTROPY_PACKAGE.zip

import os, io, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from numpy.linalg import svd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", combined="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   combined="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "EIGEN_ENTROPY_PACKAGE.zip")

TARGET_CANDS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDS     = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles","CompoundID","Compound_Id"]

# ---------------- helpers ----------------
def detect_target(df):
    for c in TARGET_CANDS:
        if c in df.columns: return c
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    raise ValueError("Target column not found (need pIC50/IC50/etc.).")

def to_pic50(series, name):
    v = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower(): return v
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(v, 1e-12, None))
    return v

def detect_split_col(df):
    for c in df.columns:
        key = c.strip().lower()
        if key in ["set","split","subset","partition","fold"]:
            return c
    raise ValueError("Split column not found (expected Set/split/subset/partition/fold with Train/Test).")

def normalize_split(s):
    s = s.astype(str).str.strip().str.lower()
    s = s.replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def id_cols(df):
    return [c for c in ID_CANDS if c in df.columns]

def build_descriptor_matrix(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    X = df[cols].apply(pd.to_numeric, errors="coerce")
    keep = X.notna().any(axis=0)            # drop all-NaN cols
    X = X.loc[:, keep]
    feats = [c for c,k in zip(cols, keep) if k]
    return X, feats

def eigen_entropy_from_Xs(Xs):
    # Xs standardized (n x p)
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = s**2
    p = lam / np.sum(lam)                  # normalized eigenvalues
    H = -np.sum(p * np.log(p + 1e-12))     # Shannon entropy of eigen-spectrum
    return H, p

def make_figures_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"EIGEN_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    tcol = detect_target(df)
    split_col = detect_split_col(df)
    df["_split"] = normalize_split(df[split_col])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: split column must contain Train/Test values.")

    # TRAIN rows only (per paper)
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)

    # Exclude non-descriptors
    excl = set(["_split", tcol, "pred_pIC50", "residual"])
    excl |= set(id_cols(df))
    for s in ["SMILES","smiles","canonical_smiles"]:
        if s in df.columns: excl.add(s)

    # Descriptor matrix (TRAIN)
    X_raw, feat_cols = build_descriptor_matrix(dft, exclude=excl)
    if X_raw.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after cleaning.")

    # Impute + drop constants
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=feat_cols)
    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    keep_mask = vt.get_support()
    X_imp = X_imp.loc[:, keep_mask]
    feat_cols = [c for c,k in zip(feat_cols, keep_mask) if k]
    if X_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after dropping constants.")

    # Standardize
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xs = scaler.fit_transform(X_imp.values)

    # (a) Full eigen spectrum & top-10 bars
    H_full, p_full = eigen_entropy_from_Xs(Xs)
    eig_df = pd.DataFrame({"component": np.arange(1, len(p_full)+1), "normalized_eigenvalue": p_full})
    eig_path = os.path.join(outdir, f"{name}_eigen_spectrum.csv")
    eig_df.to_csv(eig_path, index=False)

    # Panel (a): first 10 normalized eigenvalues
    top_k = min(10, len(p_full))
    figA_path = os.path.join(outdir, f"{name}_panelA_eigenvalues_top10_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    plt.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    plt.xlabel("component"); plt.ylabel("normalized eigenvalue")
    plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figA_path, dpi=600, bbox_inches="tight")
    plt.close()

    # (b) Leave-one-out entropy distribution
    H_minus = np.zeros(len(feat_cols), dtype=float)
    for j in range(Xs.shape[1]):
        X_minus = np.delete(Xs, j, axis=1)
        Hj, _ = eigen_entropy_from_Xs(X_minus)
        H_minus[j] = Hj

    ent_df = pd.DataFrame({
        "feature": feat_cols,
        "H_full": H_full,
        "H_minus_j": H_minus
    })
    ent_path = os.path.join(outdir, f"{name}_entropy_leave1out.csv")
    ent_df.to_csv(ent_path, index=False)

    # Panel (b): improved non-overlapping label near the dashed line
    figB_path = os.path.join(outdir, f"{name}_panelB_entropy_distribution_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    counts, bins, patches = plt.hist(H_minus, bins=60, density=True, alpha=0.6,
                                     color="#f4a3a3", edgecolor="k", linewidth=0.3)
    plt.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    # place label slightly to the right of the line and centered vertically
    ymax = plt.ylim()[1]
    x_offset = (bins.max() - bins.min()) * 0.008
    plt.text(H_full + x_offset, ymax*0.85, f"original entropy = {H_full:.6f}",
             color="#1f77b4", fontsize=8, rotation=90, va="center", ha="left")
    plt.xlabel("entropy"); plt.ylabel("density"); plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figB_path, dpi=600, bbox_inches="tight")
    plt.close()

    # Combined 2Ã—1 (paper-like)
    figC_path = os.path.join(outdir, f"{name}_figure_combined_2x1_600dpi.png")
    plt.figure(figsize=(8.0, 6.0))
    # Panel a
    ax1 = plt.subplot(2,1,1)
    ax1.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    ax1.set_xlabel("component"); ax1.set_ylabel("normalized eigenvalue"); ax1.set_title(f"{name}")
    # Panel b
    ax2 = plt.subplot(2,1,2)
    counts, bins, patches = ax2.hist(H_minus, bins=60, density=True, alpha=0.6,
                                     color="#f4a3a3", edgecolor="k", linewidth=0.3)
    ax2.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    ymax2 = ax2.get_ylim()[1]
    x_offset2 = (bins.max() - bins.min()) * 0.008
    ax2.text(H_full + x_offset2, ymax2*0.85, f"original entropy = {H_full:.6f}",
             color="#1f77b4", fontsize=8, rotation=90, va="center", ha="left")
    ax2.set_xlabel("entropy"); ax2.set_ylabel("density")
    plt.tight_layout()
    plt.savefig(figC_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: H_full = {H_full:.6f} | n_train={Xs.shape[0]} | p_features={Xs.shape[1]}")
    print("   â€¢ panel (a):", figA_path)
    print("   â€¢ panel (b):", figB_path)
    print("   â€¢ combined :", figC_path)
    print("   â€¢ CSVs    :", eig_path, " ; ", ent_path)

# ---------------- run both datasets ----------------
for spec in DATASETS:
    make_figures_for_dataset(spec["name"], spec["combined"])

# ---------------- make ZIP ----------------
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("You can download via:")
print("from google.colab import files; files.download('/content/EIGEN_ENTROPY_PACKAGE.zip')")

# ===================== Eigenvalue & Entropy Analysis (Paper-style, 600 DPI) =====================
# Inputs (combined CSVs with a Train/Test column; any of: Set/split/subset/partition/fold):
#   /mnt/data/pubchem_testandtrain.csv
#   /mnt/data/MACCS_testandtrain.csv
# Columns needed: target (pIC50 or IC50), descriptors (numeric). Optional: IDs, SMILES, pred_pIC50, residual.
# Outputs per dataset under /content/EIGEN_<name>/ :
#   - <name>_panelA_eigenvalues_top10_600dpi.png
#   - <name>_panelB_entropy_distribution_600dpi.png   (label â€œoriginal entropyâ€, no numeric value, inside bars)
#   - <name>_figure_combined_2x1_600dpi.png           (paper-like a+b)
#   - <name>_eigen_spectrum.csv                       (all normalized eigenvalues)
#   - <name>_entropy_leave1out.csv                    (H_full and H_minus_j per feature)
# Final ZIP: /content/EIGEN_ENTROPY_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from numpy.linalg import svd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", combined="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   combined="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "EIGEN_ENTROPY_PACKAGE.zip")

TARGET_CANDS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDS     = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles","CompoundID","Compound_Id"]

# ---------------- helpers ----------------
def detect_target(df):
    for c in TARGET_CANDS:
        if c in df.columns: return c
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    raise ValueError("Target column not found (need pIC50/IC50/etc.).")

def to_pic50(series, name):
    v = pd.to_numeric(series, errors="coerce").values
    if "pic50" in name.lower(): return v
    if ("ic50" in name.lower()) or ("std" in name.lower()) or ("standard" in name.lower()):
        return -np.log10(np.clip(v, 1e-12, None))
    return v

def detect_split_col(df):
    for c in df.columns:
        key = c.strip().lower()
        if key in ["set","split","subset","partition","fold"]:
            return c
    raise ValueError("Split column not found (expected Set/split/subset/partition/fold with Train/Test).")

def normalize_split(s):
    s = s.astype(str).str.strip().str.lower()
    s = s.replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def id_cols(df):
    return [c for c in ID_CANDS if c in df.columns]

def build_descriptor_matrix(df, exclude):
    cols = [c for c in df.columns if c not in exclude]
    X = df[cols].apply(pd.to_numeric, errors="coerce")
    keep = X.notna().any(axis=0)            # drop all-NaN cols
    X = X.loc[:, keep]
    feats = [c for c,k in zip(cols, keep) if k]
    return X, feats

def eigen_entropy_from_Xs(Xs):
    # Xs standardized (n x p)
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = s**2
    p = lam / np.sum(lam)                  # normalized eigenvalues
    H = -np.sum(p * np.log(p + 1e-12))     # Shannon entropy of eigen-spectrum
    return H, p

def make_figures_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"EIGEN_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    tcol = detect_target(df)
    split_col = detect_split_col(df)
    df["_split"] = normalize_split(df[split_col])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: split column must contain Train/Test values.")

    # TRAIN rows only (per paper)
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)

    # Exclude non-descriptors
    excl = set(["_split", tcol, "pred_pIC50", "residual"])
    excl |= set(id_cols(df))
    for s in ["SMILES","smiles","canonical_smiles"]:
        if s in df.columns: excl.add(s)

    # Descriptor matrix (TRAIN)
    X_raw, feat_cols = build_descriptor_matrix(dft, exclude=excl)
    if X_raw.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after cleaning.")

    # Impute + drop constants
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=feat_cols)
    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    keep_mask = vt.get_support()
    X_imp = X_imp.loc[:, keep_mask]
    feat_cols = [c for c,k in zip(feat_cols, keep_mask) if k]
    if X_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after dropping constants.")

    # Standardize
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xs = scaler.fit_transform(X_imp.values)

    # (a) Full eigen spectrum & top-10 bars
    H_full, p_full = eigen_entropy_from_Xs(Xs)
    eig_df = pd.DataFrame({"component": np.arange(1, len(p_full)+1), "normalized_eigenvalue": p_full})
    eig_path = os.path.join(outdir, f"{name}_eigen_spectrum.csv")
    eig_df.to_csv(eig_path, index=False)

    # Panel (a): first 10 normalized eigenvalues
    top_k = min(10, len(p_full))
    figA_path = os.path.join(outdir, f"{name}_panelA_eigenvalues_top10_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    plt.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    plt.xlabel("component"); plt.ylabel("normalized eigenvalue")
    plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figA_path, dpi=600, bbox_inches="tight")
    plt.close()

    # (b) Leave-one-out entropy distribution
    H_minus = np.zeros(len(feat_cols), dtype=float)
    for j in range(Xs.shape[1]):
        X_minus = np.delete(Xs, j, axis=1)
        Hj, _ = eigen_entropy_from_Xs(X_minus)
        H_minus[j] = Hj

    ent_df = pd.DataFrame({
        "feature": feat_cols,
        "H_full": H_full,
        "H_minus_j": H_minus
    })
    ent_path = os.path.join(outdir, f"{name}_entropy_leave1out.csv")
    ent_df.to_csv(ent_path, index=False)

    # Panel (b): label â€œoriginal entropyâ€ INSIDE bars, not overlapping, no numeric value
    figB_path = os.path.join(outdir, f"{name}_panelB_entropy_distribution_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    counts, bins, patches = plt.hist(
        H_minus, bins=60, density=True, alpha=0.6,
        color="#f4a3a3", edgecolor="k", linewidth=0.3
    )
    plt.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    ymax = plt.ylim()[1]
    # place label at top along the dashed line, with a small white box for legibility
    plt.text(
        H_full, ymax * 0.9, "original entropy",
        color="#1f77b4", fontsize=9, rotation=90,
        va="top", ha="center", backgroundcolor="white"
    )
    plt.xlabel("entropy"); plt.ylabel("density"); plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figB_path, dpi=600, bbox_inches="tight")
    plt.close()

    # Combined 2Ã—1 (paper-like)
    figC_path = os.path.join(outdir, f"{name}_figure_combined_2x1_600dpi.png")
    plt.figure(figsize=(8.0, 6.0))
    # Panel a
    ax1 = plt.subplot(2,1,1)
    ax1.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    ax1.set_xlabel("component"); ax1.set_ylabel("normalized eigenvalue"); ax1.set_title(f"{name}")
    # Panel b
    ax2 = plt.subplot(2,1,2)
    counts, bins, patches = ax2.hist(
        H_minus, bins=60, density=True, alpha=0.6,
        color="#f4a3a3", edgecolor="k", linewidth=0.3
    )
    ax2.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    ymax2 = ax2.get_ylim()[1]
    ax2.text(
        H_full, ymax2 * 0.9, "original entropy",
        color="#1f77b4", fontsize=9, rotation=90,
        va="top", ha="center", backgroundcolor="white"
    )
    ax2.set_xlabel("entropy"); ax2.set_ylabel("density")
    plt.tight_layout()
    plt.savefig(figC_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: H_full = {H_full:.6f} | n_train={Xs.shape[0]} | p_features={Xs.shape[1]}")
    print("   â€¢ panel (a):", figA_path)
    print("   â€¢ panel (b):", figB_path)
    print("   â€¢ combined :", figC_path)
    print("   â€¢ CSVs    :", eig_path, " ; ", ent_path)

# ---------------- run both datasets ----------------
for spec in DATASETS:
    make_figures_for_dataset(spec["name"], spec["combined"])

# ---------------- make ZIP ----------------
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("You can download via:")
print("from google.colab import files; files.download('/content/EIGEN_ENTROPY_PACKAGE.zip')")

# ===================== DATA AUDIT for combined files (PubChem + MACCS) =====================
# Input: /content/pubchem_testandtrain.csv, /content/MACCS_testandtrain.csv
# Output: per-dataset report in /content/DATA_AUDIT/<name>/
#   - <name>_summary.json  (split/target/ids/pred-cols/descriptors counts)
#   - <name>_columns.csv   (column -> role, dtype, missing_frac)
#   - console printout with highlights

import os, json, numpy as np, pandas as pd

FILES = [
    ("PubChem", "/content/pubchem_testandtrain.csv"),
    ("MACCS",   "/content/MACCS_testandtrain.csv"),
]

OUTROOT = "/content/DATA_AUDIT"
os.makedirs(OUTROOT, exist_ok=True)

# Heuristics
SPLIT_CANDS  = ["split","Split","SET","Set","set","subset","Subset","partition","fold"]
TARGET_CANDS = ["pIC50","IC50","StdValue","standard_value","Activity","activity"]
ID_CANDS     = ["chembl_id","molecule_chembl_id","Name","SMILES","smiles","canonical_smiles",
                "CompoundID","Compound_Id","molecule_id","MoleculeID"]
PRED_CANDS   = ["pred_pIC50","residual","abs_residual","squared_error","percent_error","Pred_pIC50","Residual"]

def detect_split_col(df):
    for c in df.columns:
        if c in SPLIT_CANDS: return c
        if c.strip().lower() in [s.lower() for s in SPLIT_CANDS]: return c
    # fallback: anything with Train/Test-like values
    for c in df.columns:
        vals = df[c].astype(str).str.lower().unique().tolist()
        if any(v in vals for v in ["train","test","tr","te"]): return c
    return None

def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def detect_target(df):
    for c in TARGET_CANDS:
        if c in df.columns: return c
    # fuzzy
    for c in df.columns:
        lc = c.lower()
        if ("pic50" in lc) or ("ic50" in lc) or ("std" in lc) or ("standard" in lc) or (lc in ["activity","response","y"]):
            return c
    return None

def role_of_column(c, split_col, target_col, id_cols, pred_cols, numeric_mask):
    if c == split_col: return "split"
    if c == target_col: return "target"
    if c in id_cols: return "id"
    if c in pred_cols: return "prediction_metric"
    if numeric_mask.get(c, False): return "descriptor_numeric"
    return "other(non-numeric or ignored)"

def audit_one(name, path):
    outdir = os.path.join(OUTROOT, name); os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    n_rows, n_cols = df.shape

    # detect split / normalize
    split_col = detect_split_col(df)
    split_counts = {}
    if split_col:
        split_norm = normalize_split(df[split_col])
        split_counts = split_norm.value_counts().to_dict()
    else:
        split_norm = pd.Series(["UNKNOWN"]*len(df))
    # detect target
    target_col = detect_target(df)

    # id / pred cols present
    id_cols = [c for c in ID_CANDS if c in df.columns]
    pred_cols = [c for c in PRED_CANDS if c in df.columns]

    # numeric mask
    numeric_mask = {}
    for c in df.columns:
        if c == split_col: numeric_mask[c] = False; continue
        try:
            pd.to_numeric(df[c], errors="coerce")
            numeric_mask[c] = True
        except Exception:
            numeric_mask[c] = False

    # descriptor candidates: numeric and NOT in split/target/ids/preds
    blocked = set([split_col, target_col] + id_cols + pred_cols) if split_col or target_col else set(id_cols+pred_cols)
    desc_cols = [c for c in df.columns if (numeric_mask.get(c, False) and c not in blocked)]

    # missing fraction and dtypes
    miss_frac = df.isna().mean()
    dtypes = df.dtypes.astype(str)

    # build columns table
    rows = []
    for c in df.columns:
        rows.append({
            "column": c,
            "role": role_of_column(c, split_col, target_col, id_cols, pred_cols, numeric_mask),
            "dtype": dtypes[c],
            "missing_frac": float(miss_frac[c]),
        })
    coltab = pd.DataFrame(rows).sort_values(["role","column"]).reset_index(drop=True)
    coltab_path = os.path.join(outdir, f"{name}_columns.csv")
    coltab.to_csv(coltab_path, index=False)

    # summary
    summary = {
        "dataset": name,
        "path": path,
        "n_rows": int(n_rows),
        "n_columns": int(n_cols),
        "split_column": split_col,
        "split_counts": split_counts,
        "target_column": target_col,
        "id_columns": id_cols,
        "prediction_metric_columns": pred_cols,
        "n_descriptor_numeric": int(len(desc_cols)),
        "descriptor_examples_first20": desc_cols[:20],
        "has_pred_columns": bool(pred_cols),
        "notes": "Descriptor columns are numeric features excluding split/target/ID/pred-metric columns.",
    }
    with open(os.path.join(outdir, f"{name}_summary.json"), "w") as f:
        json.dump(summary, f, indent=2)

    # Console summary
    print(f"\n=== {name} ===")
    print(f"File: {path}")
    print(f"Rows, Cols: {n_rows}, {n_cols}")
    print(f"Split column: {split_col}  | counts: {split_counts if split_counts else 'N/A'}")
    print(f"Target column: {target_col}")
    print(f"ID columns: {id_cols}")
    print(f"Prediction/diagnostic columns: {pred_cols}")
    print(f"Descriptor (numeric) columns: {len(desc_cols)}")
    if desc_cols:
        print("  e.g. (first 10):", desc_cols[:10])
    print(f"Saved columns table: {coltab_path}")
    print(f"Saved summary JSON: {os.path.join(outdir, f'{name}_summary.json')}")

# Run both
for name, path in FILES:
    if os.path.exists(path):
        audit_one(name, path)
    else:
        print(f"âš ï¸ Missing file: {path}")

# ===================== Eigenvalue & Entropy Analysis (Paper-style, 600 DPI) =====================
# Files (with this column order):
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
#
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
#
# Outputs (per dataset under /content/EIGEN_<name>/):
#   - <name>_panelA_eigenvalues_top10_600dpi.png
#   - <name>_panelB_entropy_distribution_600dpi.png   (label â€œoriginal entropyâ€ inside bars, no numeric value)
#   - <name>_figure_combined_2x1_600dpi.png
#   - <name>_eigen_spectrum.csv                       (all normalized eigenvalues)
#   - <name>_entropy_leave1out.csv                    (H_full and H_minus_j per feature)
# Final ZIP:
#   /content/EIGEN_ENTROPY_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from numpy.linalg import svd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "EIGEN_ENTROPY_PACKAGE.zip")

# ---------------- helpers ----------------
def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def eigen_entropy_from_Xs(Xs):
    # Xs standardized (n x p)
    U, s, Vt = svd(Xs, full_matrices=False)
    lam = s**2
    p = lam / np.sum(lam)                  # normalized eigenvalues
    H = -np.sum(p * np.log(p + 1e-12))     # Shannon entropy of eigen-spectrum
    return H, p

def get_descriptor_block(df):
    """
    Your schema: chembl_id | Set | [DESCRIPTORS...] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
    We grab columns strictly BETWEEN 'Set' and 'Actual pIC50'.
    Fallback: if either is missing, fall back to numeric-exclude logic.
    """
    cols = list(df.columns)
    name_map = {c.lower(): c for c in cols}

    # locate Set and Actual pIC50 by exact names first, then case-insensitive fallback
    set_col = "Set" if "Set" in cols else name_map.get("set", None)
    target_col = "Actual pIC50" if "Actual pIC50" in cols else name_map.get("actual pic50", None)

    if set_col is not None and target_col is not None:
        i_set = cols.index(set_col)
        i_tar = cols.index(target_col)
        if i_tar - i_set > 1:
            desc_cols = cols[i_set+1:i_tar]
            return desc_cols

    # Fallback (shouldnâ€™t trigger for your files): numeric-exclude approach
    block_exclude = {"chembl_id","set","actual pic50","pred pic50","residual","abs_residual","squared_error","percent_error"}
    desc_cols = []
    for c in cols:
        lc = c.lower().strip()
        if lc in block_exclude:
            continue
        # numeric?
        try:
            pd.to_numeric(df[c], errors="coerce")
            desc_cols.append(c)
        except Exception:
            pass
    return desc_cols

def make_figures_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"EIGEN_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)

    # Validate key columns
    for needed in ["Set", "Actual pIC50"]:
        if needed not in df.columns:
            raise ValueError(f"{name}: required column '{needed}' not found.")
    # Normalize split
    df["_split"] = normalize_split(df["Set"])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: 'Set' must contain Train/Test values.")

    # TRAIN only (paper methodology)
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)

    # Descriptor block (strictly between Set and Actual pIC50)
    desc_cols = get_descriptor_block(df)
    if len(desc_cols) < 3:
        raise ValueError(f"{name}: too few descriptor columns detected between 'Set' and 'Actual pIC50'.")

    # Build TRAIN descriptor matrix
    X_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")

    # Impute + drop constants
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    keep_mask = vt.get_support()
    X_imp = X_imp.loc[:, keep_mask]
    kept_cols = [c for c,k in zip(desc_cols, keep_mask) if k]
    if X_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few descriptor columns after dropping constants.")

    # Standardize
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xs = scaler.fit_transform(X_imp.values)

    # (a) Full eigen spectrum & top-10 bars
    H_full, p_full = eigen_entropy_from_Xs(Xs)
    eig_df = pd.DataFrame({"component": np.arange(1, len(p_full)+1),
                           "normalized_eigenvalue": p_full})
    eig_path = os.path.join(outdir, f"{name}_eigen_spectrum.csv")
    eig_df.to_csv(eig_path, index=False)

    # Panel (a): first 10 normalized eigenvalues
    top_k = min(10, len(p_full))
    figA_path = os.path.join(outdir, f"{name}_panelA_eigenvalues_top10_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    plt.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    plt.xlabel("component"); plt.ylabel("normalized eigenvalue")
    plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figA_path, dpi=600, bbox_inches="tight")
    plt.close()

    # (b) Leave-one-out entropy distribution
    H_minus = np.zeros(Xs.shape[1], dtype=float)
    for j in range(Xs.shape[1]):
        X_minus = np.delete(Xs, j, axis=1)
        Hj, _ = eigen_entropy_from_Xs(X_minus)
        H_minus[j] = Hj

    ent_df = pd.DataFrame({
        "feature": kept_cols,
        "H_full": H_full,
        "H_minus_j": H_minus
    })
    ent_path = os.path.join(outdir, f"{name}_entropy_leave1out.csv")
    ent_df.to_csv(ent_path, index=False)

    # Panel (b): label â€œoriginal entropyâ€ INSIDE bars, not overlapping, no numeric value
    figB_path = os.path.join(outdir, f"{name}_panelB_entropy_distribution_600dpi.png")
    plt.figure(figsize=(6.0, 3.8))
    counts, bins, patches = plt.hist(
        H_minus, bins=60, density=True, alpha=0.6,
        color="#f4a3a3", edgecolor="k", linewidth=0.3
    )
    # dashed reference line
    plt.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    # place label on the dashed line, inside the plot
    ymax = plt.ylim()[1]
    plt.text(
        H_full, ymax * 0.9, "original entropy",
        color="#1f77b4", fontsize=9, rotation=90,
        va="top", ha="center", backgroundcolor="white"
    )
    plt.xlabel("entropy"); plt.ylabel("density"); plt.title(f"{name}")
    plt.tight_layout()
    plt.savefig(figB_path, dpi=600, bbox_inches="tight")
    plt.close()

    # Combined 2Ã—1 (paper-like)
    figC_path = os.path.join(outdir, f"{name}_figure_combined_2x1_600dpi.png")
    plt.figure(figsize=(8.0, 6.0))
    # Panel a
    ax1 = plt.subplot(2,1,1)
    ax1.bar(np.arange(1, top_k+1), p_full[:top_k], color="#f08080")
    ax1.set_xlabel("component"); ax1.set_ylabel("normalized eigenvalue"); ax1.set_title(f"{name}")
    # Panel b
    ax2 = plt.subplot(2,1,2)
    counts, bins, patches = ax2.hist(
        H_minus, bins=60, density=True, alpha=0.6,
        color="#f4a3a3", edgecolor="k", linewidth=0.3
    )
    ax2.axvline(H_full, color="#1f77b4", linestyle="--", linewidth=1.5)
    ymax2 = ax2.get_ylim()[1]
    ax2.text(
        H_full, ymax2 * 0.9, "original entropy",
        color="#1f77b4", fontsize=9, rotation=90,
        va="top", ha="center", backgroundcolor="white"
    )
    ax2.set_xlabel("entropy"); ax2.set_ylabel("density")
    plt.tight_layout()
    plt.savefig(figC_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: H_full = {H_full:.6f} | n_train={Xs.shape[0]} | p_features={Xs.shape[1]}")
    print("   â€¢ panel (a):", figA_path)
    print("   â€¢ panel (b):", figB_path)
    print("   â€¢ combined :", figC_path)
    print("   â€¢ CSVs    :", eig_path, " ; ", ent_path)

# ---------------- run both datasets ----------------
for spec in DATASETS:
    make_figures_for_dataset(spec["name"], spec["path"])

# ---------------- make ZIP ----------------
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/EIGEN_ENTROPY_PACKAGE.zip')")

"""##PCA"""

# ===================== Step 1: PCA Analysis (Train vs Test, 600 DPI) =====================
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
# Schema:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
#
# Outputs (per dataset under /content/PCA_<name>/):
#   - <name>_PCA_scatter_train_test_600dpi.png   (colored TRAIN vs TEST)
#   - <name>_PCA_variance_explained.csv          (variance ratios for PCs)
#   - <name>_PCA_scores.csv                      (PC1, PC2, Set)
# Final ZIP: /content/PCA_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "PCA_PACKAGE.zip")

def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    name_map = {c.lower(): c for c in cols}
    set_col = "Set" if "Set" in cols else name_map.get("set", None)
    target_col = "Actual pIC50" if "Actual pIC50" in cols else name_map.get("actual pic50", None)
    if set_col and target_col:
        i_set = cols.index(set_col)
        i_tar = cols.index(target_col)
        if i_tar - i_set > 1:
            return cols[i_set+1:i_tar]
    # fallback: numeric-exclude approach
    block_exclude = {"chembl_id","set","actual pic50","pred pic50","residual","abs_residual","squared_error","percent_error"}
    desc_cols = []
    for c in cols:
        if c.lower().strip() in block_exclude: continue
        try:
            pd.to_numeric(df[c], errors="coerce")
            desc_cols.append(c)
        except: pass
    return desc_cols

def make_pca_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"PCA_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    if "Set" not in df.columns or "Actual pIC50" not in df.columns:
        raise ValueError(f"{name}: Missing required columns 'Set' or 'Actual pIC50'.")

    df["_split"] = normalize_split(df["Set"])
    desc_cols = get_descriptor_block(df)
    if len(desc_cols) < 3:
        raise ValueError(f"{name}: too few descriptor columns detected between 'Set' and 'Actual pIC50'.")

    # Impute + drop constants + standardize
    X_raw = df[desc_cols].apply(pd.to_numeric, errors="coerce")
    imp = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imp.fit_transform(X_raw), columns=desc_cols)
    vt = VarianceThreshold(0.0)
    vt.fit(X_imp.fillna(0))
    X_imp = X_imp.loc[:, vt.get_support()]
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X_imp)

    # PCA
    pca = PCA(n_components=2, random_state=42)
    PCs = pca.fit_transform(Xs)
    var_ratio = pca.explained_variance_ratio_ * 100

    # save variance table
    var_df = pd.DataFrame({
        "PC": ["PC1", "PC2"],
        "explained_variance_%": var_ratio
    })
    var_path = os.path.join(outdir, f"{name}_PCA_variance_explained.csv")
    var_df.to_csv(var_path, index=False)

    # build scores DataFrame
    score_df = pd.DataFrame({
        "chembl_id": df["chembl_id"],
        "Set": df["_split"],
        "PC1": PCs[:,0],
        "PC2": PCs[:,1]
    })
    score_path = os.path.join(outdir, f"{name}_PCA_scores.csv")
    score_df.to_csv(score_path, index=False)

    # Plot PCA (Train vs Test)
    plt.figure(figsize=(6,5))
    colors = {"TRAIN":"#ff6f61", "TEST":"#1f77b4"}
    for split, color in colors.items():
        subset = score_df[score_df["Set"]==split]
        plt.scatter(subset["PC1"], subset["PC2"], s=20, alpha=0.7, color=color, label=split)
    plt.xlabel(f"PC1 ({var_ratio[0]:.2f}% var)")
    plt.ylabel(f"PC2 ({var_ratio[1]:.2f}% var)")
    plt.title(f"{name} â€“ PCA of descriptors")
    plt.legend(frameon=True)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    fig_path = os.path.join(outdir, f"{name}_PCA_scatter_train_test_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: PCA done | PC1={var_ratio[0]:.2f}%, PC2={var_ratio[1]:.2f}%")
    print("   â€¢ Scatter:", fig_path)
    print("   â€¢ Variance:", var_path)
    print("   â€¢ Scores:", score_path)

# ---- Run PCA for both datasets ----
for spec in DATASETS:
    make_pca_for_dataset(spec["name"], spec["path"])

# ---- Make ZIP ----
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/PCA_PACKAGE.zip')")

"""##Willimson plot"""

# ===================== Step 2: Williams Plot (Applicability Domain) â€” 600 DPI, clear axes, no h* number on figure =====================
# Files schema:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
#
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
#
# Outputs (per dataset under /content/WILLIAMS_<name>/):
#   - <name>_WilliamsPlot_600dpi.png                 (no h* text on figure; just lines)
#   - <name>_williams_table.csv                      (chembl_id, split, y_true, y_pred, resid, std_resid, h, flags)
#   - <name>_williams_meta.csv                       (n_train, p_features, h_star, train_RMSE)
# Final ZIP:
#   /content/WILLIAMS_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import mean_squared_error

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "WILLIAMS_PACKAGE.zip")

# ---------------- helpers ----------------
def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    """
    Your schema: chembl_id | Set | [DESCRIPTORS...] | Actual pIC50 | pred pIC50 | residual | abs_residual | squared_error | percent_error
    We take columns strictly BETWEEN 'Set' and 'Actual pIC50'.
    """
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Required columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block found between 'Set' and 'Actual pIC50'.")
    return cols[i_set+1:i_tar]

def williams_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"WILLIAMS_{name}")
    os.makedirs(outdir, exist_ok=True)

    # ----- Load & basic checks -----
    df = pd.read_csv(path)
    needed = ["chembl_id", "Set", "Actual pIC50", "pred pIC50"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"{name}: missing required columns: {missing}")

    df["_split"] = normalize_split(df["Set"])
    if not (set(df["_split"].unique()) >= {"TRAIN","TEST"}):
        raise ValueError(f"{name}: 'Set' must contain Train/Test values.")

    desc_cols = get_descriptor_block(df)
    if len(desc_cols) < 3:
        raise ValueError(f"{name}: too few descriptor columns detected.")

    # ----- Build TRAIN & TEST arrays -----
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    # Descriptor matrices
    Xtr_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")
    Xte_raw = dfe[desc_cols].apply(pd.to_numeric, errors="coerce")

    # Impute (median) and drop constants based on TRAIN only
    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_raw), columns=desc_cols)
    Xte_imp = pd.DataFrame(imp.transform(Xte_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(Xtr_imp.fillna(0))
    keep_mask = vt.get_support()
    keep_cols = [c for c,k in zip(desc_cols, keep_mask) if k]
    Xtr_imp = Xtr_imp.loc[:, keep_mask]
    Xte_imp = Xte_imp.loc[:, keep_mask]

    if Xtr_imp.shape[1] < 3:
        raise ValueError(f"{name}: too few features after dropping constants.")

    # Standardize with TRAIN stats (Williams requires standardized X)
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xtr_s = scaler.fit_transform(Xtr_imp.values)
    Xte_s = scaler.transform(Xte_imp.values)

    # ----- y_true / y_pred and standardized residuals -----
    y_tr = pd.to_numeric(dft["Actual pIC50"], errors="coerce").values
    y_te = pd.to_numeric(dfe["Actual pIC50"], errors="coerce").values
    p_tr = pd.to_numeric(dft["pred pIC50"], errors="coerce").values
    p_te = pd.to_numeric(dfe["pred pIC50"], errors="coerce").values

    # Train RMSE for standardization
    rmse_tr = float(np.sqrt(mean_squared_error(y_tr, p_tr)))
    sr_tr = (y_tr - p_tr) / (rmse_tr + 1e-12)
    sr_te = (y_te - p_te) / (rmse_tr + 1e-12)

    # ----- Leverage (h) using TRAIN only -----
    # h = x^T (X^T X)^(-1) x, computed in TRAIN-standardized space
    XtX_inv = np.linalg.pinv(Xtr_s.T @ Xtr_s)
    h_tr = np.einsum("ij,jk,ik->i", Xtr_s, XtX_inv, Xtr_s)
    h_te = np.einsum("ij,jk,ik->i", Xte_s, XtX_inv, Xte_s)

    n = Xtr_s.shape[0]
    p = Xtr_s.shape[1]
    h_star = 3.0 * p / max(n, 1)  # Williams rule of thumb

    # ----- Collect table & flags -----
    tab_tr = pd.DataFrame({
        "chembl_id": dft["chembl_id"].values,
        "split": "TRAIN",
        "y_true": y_tr,
        "y_pred": p_tr,
        "residual": (p_tr - y_tr),
        "std_residual": sr_tr,
        "leverage_h": h_tr
    })
    tab_te = pd.DataFrame({
        "chembl_id": dfe["chembl_id"].values,
        "split": "TEST",
        "y_true": y_te,
        "y_pred": p_te,
        "residual": (p_te - y_te),
        "std_residual": sr_te,
        "leverage_h": h_te
    })
    tab = pd.concat([tab_tr, tab_te], ignore_index=True)
    tab["flag_abs_sr_gt3"] = (tab["std_residual"].abs() > 3.0).astype(int)
    tab["flag_h_gt_hstar"] = (tab["leverage_h"] > h_star).astype(int)
    tab["flag_out_of_AD"]  = ((tab["std_residual"].abs() > 3.0) | (tab["leverage_h"] > h_star)).astype(int)

    tab_path = os.path.join(outdir, f"{name}_williams_table.csv")
    tab.to_csv(tab_path, index=False)

    meta_path = os.path.join(outdir, f"{name}_williams_meta.csv")
    pd.DataFrame([{
        "n_train": n,
        "p_features": p,
        "h_star": h_star,
        "train_RMSE": rmse_tr
    }]).to_csv(meta_path, index=False)

    # ----- Plot (clear, non-compact, 600 DPI) -----
    plt.figure(figsize=(8,6))
    # Train
    plt.scatter(h_tr, sr_tr, s=24, alpha=0.75, label="TRAIN", color="#1f77b4", edgecolor="none")
    # Test
    plt.scatter(h_te, sr_te, s=24, alpha=0.75, label="TEST", color="#ff7f0e", edgecolor="none")

    # Reference lines (no text labels on h*)
    plt.axhline( 3.0, color="crimson", linestyle="--", linewidth=1.0)
    plt.axhline(-3.0, color="crimson", linestyle="--", linewidth=1.0)
    plt.axvline(h_star, color="crimson", linestyle="--", linewidth=1.0)

    plt.xlabel("Leverage (h)")
    plt.ylabel("Standardized residual")
    plt.title(f"Williams Plot â€” {name}")
    plt.grid(alpha=0.3)
    plt.legend(frameon=True, loc="best")
    plt.tight_layout()
    fig_path = os.path.join(outdir, f"{name}_WilliamsPlot_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: Williams plot ready.")
    print("   â€¢ Figure :", fig_path)
    print("   â€¢ Table  :", tab_path)
    print("   â€¢ Meta   :", meta_path)
    print(f"   â€¢ n_train={n}, p={p}, h* saved in meta CSV, train_RMSE={rmse_tr:.4f}")

# ---- Run for both datasets ----
for spec in DATASETS:
    williams_for_dataset(spec["name"], spec["path"])

# ---- Make ZIP ----
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/WILLIAMS_PACKAGE.zip')")

# ===================== Improved Williams Plot (clear x-axis spread, visible h* line, 600 DPI) =====================
# Files schema:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
#
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
#
# Outputs:
#   - <name>_WilliamsPlot_600dpi.png
#   - <name>_williams_table.csv
#   - <name>_williams_meta.csv
#   - WILLIAMS_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import mean_squared_error

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "WILLIAMS_PACKAGE.zip")

def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block found between 'Set' and 'Actual pIC50'.")
    return cols[i_set+1:i_tar]

def williams_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"WILLIAMS_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    needed = ["chembl_id", "Set", "Actual pIC50", "pred pIC50"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"{name}: missing columns: {missing}")

    df["_split"] = normalize_split(df["Set"])
    desc_cols = get_descriptor_block(df)

    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    # Descriptor matrices
    Xtr_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")
    Xte_raw = dfe[desc_cols].apply(pd.to_numeric, errors="coerce")

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_raw), columns=desc_cols)
    Xte_imp = pd.DataFrame(imp.transform(Xte_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(Xtr_imp.fillna(0))
    keep_mask = vt.get_support()
    Xtr_imp = Xtr_imp.loc[:, keep_mask]
    Xte_imp = Xte_imp.loc[:, keep_mask]

    scaler = StandardScaler()
    Xtr_s = scaler.fit_transform(Xtr_imp)
    Xte_s = scaler.transform(Xte_imp)

    y_tr = pd.to_numeric(dft["Actual pIC50"], errors="coerce").values
    y_te = pd.to_numeric(dfe["Actual pIC50"], errors="coerce").values
    p_tr = pd.to_numeric(dft["pred pIC50"], errors="coerce").values
    p_te = pd.to_numeric(dfe["pred pIC50"], errors="coerce").values

    rmse_tr = float(np.sqrt(mean_squared_error(y_tr, p_tr)))
    sr_tr = (y_tr - p_tr) / (rmse_tr + 1e-12)
    sr_te = (y_te - p_te) / (rmse_tr + 1e-12)

    XtX_inv = np.linalg.pinv(Xtr_s.T @ Xtr_s)
    h_tr = np.einsum("ij,jk,ik->i", Xtr_s, XtX_inv, Xtr_s)
    h_te = np.einsum("ij,jk,ik->i", Xte_s, XtX_inv, Xte_s)

    n, p = Xtr_s.shape
    h_star = 3.0 * p / max(n, 1)

    tab_tr = pd.DataFrame({
        "chembl_id": dft["chembl_id"],
        "split": "TRAIN", "y_true": y_tr, "y_pred": p_tr,
        "residual": p_tr - y_tr, "std_residual": sr_tr, "leverage_h": h_tr
    })
    tab_te = pd.DataFrame({
        "chembl_id": dfe["chembl_id"],
        "split": "TEST", "y_true": y_te, "y_pred": p_te,
        "residual": p_te - y_te, "std_residual": sr_te, "leverage_h": h_te
    })
    tab = pd.concat([tab_tr, tab_te], ignore_index=True)
    tab["flag_out_of_AD"] = ((tab["std_residual"].abs() > 3.0) | (tab["leverage_h"] > h_star)).astype(int)

    tab_path = os.path.join(outdir, f"{name}_williams_table.csv")
    tab.to_csv(tab_path, index=False)
    meta_path = os.path.join(outdir, f"{name}_williams_meta.csv")
    pd.DataFrame([{"n_train": n, "p_features": p, "h_star": h_star, "train_RMSE": rmse_tr}]).to_csv(meta_path, index=False)

    # ---- plot ----
    plt.figure(figsize=(8,6))
    plt.scatter(h_tr, sr_tr, s=30, alpha=0.75, color="#1f77b4", label="TRAIN")
    plt.scatter(h_te, sr_te, s=30, alpha=0.75, color="#ff7f0e", label="TEST")

    plt.axhline( 3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axhline(-3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axvline(h_star, color="crimson", linestyle="--", linewidth=1.2)

    # Extend axes for clarity
    max_h = float(tab["leverage_h"].max())
    plt.xlim(-0.05, max_h * 1.2 if max_h > 0 else 0.1)
    plt.ylim(-8, 8)

    plt.xlabel("Leverage (h)", fontsize=12)
    plt.ylabel("Standardized residual", fontsize=12)
    plt.title(f"Williams Plot â€” {name}", fontsize=13)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    plt.grid(alpha=0.4, linestyle=":")
    plt.legend(frameon=True, fontsize=10)
    plt.tight_layout()

    fig_path = os.path.join(outdir, f"{name}_WilliamsPlot_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: Williams plot ready | h* saved in meta CSV")
    print("   â€¢ Figure:", fig_path)
    print("   â€¢ Table :", tab_path)
    print("   â€¢ Meta  :", meta_path)

# ---- Run for both datasets ----
for spec in DATASETS:
    williams_for_dataset(spec["name"], spec["path"])

# ---- ZIP all results ----
def zipdir(path, ziph):
    for root, _, files in os.walk(path):
        for f in files:
            if f.endswith(".png") or f.endswith(".csv"):
                full = os.path.join(root, f)
                arc  = os.path.relpath(full, path)
                ziph.write(full, arcname=arc)

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    zipdir(OUTROOT, zf)

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/WILLIAMS_PACKAGE.zip')")

# ===================== Williams Plot (clean ticks, no grid, tidy ZIP) =====================
# Schema expected:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
# Outputs per dataset in /content/WILLIAMS_<name>/ :
#   - <name>_WilliamsPlot_600dpi.png
#   - <name>_williams_table.csv
#   - <name>_williams_meta.csv
# Final ZIP (only these assets): /content/WILLIAMS_CLEAN.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import mean_squared_error

# ---------- CONFIG ----------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "WILLIAMS_CLEAN.zip")

# ---------- helpers ----------
def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block found between 'Set' and 'Actual pIC50'.")
    return cols[i_set+1:i_tar]

def nice_limit(v, base=1.0, cap=10.0):
    """Round up to a pleasant axis limit, capped to avoid silly ranges."""
    if v <= 0: return base
    import math
    return min(cap, math.ceil(v / base) * base)

def williams_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"WILLIAMS_{name}")
    os.makedirs(outdir, exist_ok=True)

    # ---- load & checks
    df = pd.read_csv(path)
    for c in ["chembl_id","Set","Actual pIC50","pred pIC50"]:
        if c not in df.columns:
            raise ValueError(f"{name}: missing required column '{c}'.")

    df["_split"] = normalize_split(df["Set"])
    desc_cols = get_descriptor_block(df)
    if len(desc_cols) < 3:
        raise ValueError(f"{name}: too few descriptor columns detected.")

    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    # ---- descriptor matrices
    Xtr_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")
    Xte_raw = dfe[desc_cols].apply(pd.to_numeric, errors="coerce")

    imp = SimpleImputer(strategy="median")
    Xtr_imp = pd.DataFrame(imp.fit_transform(Xtr_raw), columns=desc_cols)
    Xte_imp = pd.DataFrame(imp.transform(Xte_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(Xtr_imp.fillna(0))
    keep_mask = vt.get_support()
    Xtr_imp = Xtr_imp.loc[:, keep_mask]
    Xte_imp = Xte_imp.loc[:, keep_mask]

    scaler = StandardScaler()
    Xtr_s = scaler.fit_transform(Xtr_imp)
    Xte_s = scaler.transform(Xte_imp)

    # ---- y, preds, standardized residuals (use TRAIN RMSE)
    y_tr = pd.to_numeric(dft["Actual pIC50"], errors="coerce").values
    y_te = pd.to_numeric(dfe["Actual pIC50"], errors="coerce").values
    p_tr = pd.to_numeric(dft["pred pIC50"], errors="coerce").values
    p_te = pd.to_numeric(dfe["pred pIC50"], errors="coerce").values

    rmse_tr = float(np.sqrt(mean_squared_error(y_tr, p_tr)))
    sr_tr = (y_tr - p_tr) / (rmse_tr + 1e-12)
    sr_te = (y_te - p_te) / (rmse_tr + 1e-12)

    # ---- leverage (TRAIN space)
    XtX_inv = np.linalg.pinv(Xtr_s.T @ Xtr_s)
    h_tr = np.einsum("ij,jk,ik->i", Xtr_s, XtX_inv, Xtr_s)
    h_te = np.einsum("ij,jk,ik->i", Xte_s, XtX_inv, Xte_s)

    n, p = Xtr_s.shape
    h_star = 3.0 * p / max(n, 1)

    # ---- table & meta
    tab = pd.concat([
        pd.DataFrame({"chembl_id": dft["chembl_id"], "split": "TRAIN",
                      "y_true": y_tr, "y_pred": p_tr,
                      "residual": p_tr - y_tr, "std_residual": sr_tr, "leverage_h": h_tr}),
        pd.DataFrame({"chembl_id": dfe["chembl_id"], "split": "TEST",
                      "y_true": y_te, "y_pred": p_te,
                      "residual": p_te - y_te, "std_residual": sr_te, "leverage_h": h_te})
    ], ignore_index=True)
    tab["flag_out_of_AD"] = ((tab["std_residual"].abs() > 3.0) | (tab["leverage_h"] > h_star)).astype(int)

    tab_path  = os.path.join(outdir, f"{name}_williams_table.csv")
    meta_path = os.path.join(outdir, f"{name}_williams_meta.csv")
    tab.to_csv(tab_path, index=False)
    pd.DataFrame([{"n_train": n, "p_features": p, "h_star": h_star, "train_RMSE": rmse_tr}]
                ).to_csv(meta_path, index=False)

    # ---- figure (no grid; clear, non-overlapping ticks)
    plt.figure(figsize=(8,6))

    plt.scatter(h_tr, sr_tr, s=30, alpha=0.8, color="#1f77b4", label="TRAIN")
    plt.scatter(h_te, sr_te, s=30, alpha=0.8, color="#ff7f0e", label="TEST")

    # reference lines
    plt.axhline( 3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axhline(-3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axvline(h_star, color="crimson", linestyle="--", linewidth=1.2)

    # axis ranges and smart ticks
    max_h = float(tab["leverage_h"].max())
    x_max = max(h_star, max_h) * 1.15
    x_max = nice_limit(x_max, base=0.5, cap= max(1.5, x_max))
    plt.xlim(0.0, x_max)

    y_abs = float(np.nanmax(np.abs(tab["std_residual"].values)))
    y_lim = max(3.5, y_abs*1.15)  # keep Â±3 visible with margin
    y_lim = nice_limit(y_lim, base=1.0, cap=10.0)
    plt.ylim(-y_lim, y_lim)

    # non-overlapping tick locators
    ax = plt.gca()
    ax.xaxis.set_major_locator(MaxNLocator(nbins=7, prune=None))
    ax.yaxis.set_major_locator(MaxNLocator(nbins=9, prune=None))

    # styling
    plt.xlabel("Leverage (h)", fontsize=13)
    plt.ylabel("Standardized residual", fontsize=13)
    plt.title(f"Williams Plot â€” {name}", fontsize=14)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    # no grid
    plt.legend(frameon=True, fontsize=10, loc="best")
    plt.tight_layout()

    fig_path = os.path.join(outdir, f"{name}_WilliamsPlot_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()

    return fig_path, tab_path, meta_path

# ---------- run & make ZIP (only Williams outputs) ----------
assets = []
for spec in DATASETS:
    figp, tabp, metap = williams_for_dataset(spec["name"], spec["path"])
    assets.extend([figp, tabp, metap])

with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        arc = os.path.relpath(f, OUTROOT)
        zf.write(f, arcname=arc)

print("\nðŸ“¦ ZIP ready with Williams-only outputs:", ZIP_PATH)
print("from google.colab import files; files.download('/content/WILLIAMS_CLEAN.zip')")

# ===================== Williams Plot (manual x-axis ticks, stretched low region) =====================
# Manual x ticks: [-2, -1, 0, 3, 6, 9, 12]
# No grid; h*, Â±3 lines only; 600 DPI
# Outputs only this analysis (plot + table + meta) -> /content/WILLIAMS_MANUALTICKS.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import mean_squared_error

DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "WILLIAMS_MANUALTICKS.zip")

def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    return cols[i_set+1:i_tar]

def williams_for_dataset(name, path):
    outdir = os.path.join(OUTROOT, f"WILLIAMS_{name}")
    os.makedirs(outdir, exist_ok=True)
    df = pd.read_csv(path)
    df["_split"] = normalize_split(df["Set"])
    desc_cols = get_descriptor_block(df)

    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    # descriptor block processing
    Xtr = SimpleImputer(strategy="median").fit_transform(dft[desc_cols])
    Xte = SimpleImputer(strategy="median").fit(dft[desc_cols]).transform(dfe[desc_cols])
    vt = VarianceThreshold(0.0)
    vt.fit(Xtr)
    Xtr = Xtr[:, vt.get_support()]
    Xte = Xte[:, vt.get_support()]
    Xtr = StandardScaler().fit_transform(Xtr)
    Xte = StandardScaler().fit(dft[desc_cols].iloc[:, vt.get_support()]).transform(Xte)

    # y/preds
    y_tr, y_te = dft["Actual pIC50"].to_numpy(), dfe["Actual pIC50"].to_numpy()
    p_tr, p_te = dft["pred pIC50"].to_numpy(), dfe["pred pIC50"].to_numpy()

    rmse_tr = float(np.sqrt(mean_squared_error(y_tr, p_tr)))
    sr_tr, sr_te = (y_tr - p_tr)/rmse_tr, (y_te - p_te)/rmse_tr
    XtX_inv = np.linalg.pinv(Xtr.T @ Xtr)
    h_tr = np.einsum("ij,jk,ik->i", Xtr, XtX_inv, Xtr)
    h_te = np.einsum("ij,jk,ik->i", Xte, XtX_inv, Xte)
    n, p = Xtr.shape
    h_star = 3*p/max(n,1)

    # dataframe & save
    tab = pd.concat([
        pd.DataFrame({"chembl_id": dft["chembl_id"], "split":"TRAIN",
                      "y_true":y_tr, "y_pred":p_tr, "residual":p_tr-y_tr,
                      "std_residual":sr_tr, "leverage_h":h_tr}),
        pd.DataFrame({"chembl_id": dfe["chembl_id"], "split":"TEST",
                      "y_true":y_te, "y_pred":p_te, "residual":p_te-y_te,
                      "std_residual":sr_te, "leverage_h":h_te})
    ])
    tab["flag_out_of_AD"] = ((tab["std_residual"].abs()>3) | (tab["leverage_h"]>h_star)).astype(int)
    tab.to_csv(os.path.join(outdir, f"{name}_williams_table.csv"), index=False)
    pd.DataFrame([{"n_train":n,"p_features":p,"h_star":h_star,"train_RMSE":rmse_tr}]
                 ).to_csv(os.path.join(outdir, f"{name}_williams_meta.csv"), index=False)

    # ------------- plot -------------
    plt.figure(figsize=(8,6))
    plt.scatter(h_tr, sr_tr, s=30, alpha=0.8, color="#1f77b4", label="TRAIN")
    plt.scatter(h_te, sr_te, s=30, alpha=0.8, color="#ff7f0e", label="TEST")

    plt.axhline( 3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axhline(-3, color="crimson", linestyle="--", linewidth=1.2)
    plt.axvline(h_star, color="crimson", linestyle="--", linewidth=1.2)

    plt.xlabel("Leverage (h)", fontsize=13)
    plt.ylabel("Standardized residual", fontsize=13)
    plt.title(f"Williams Plot â€” {name}", fontsize=14)

    # manual nonuniform x ticks
    plt.xticks([-2,-1,0,3,6,9,12], fontsize=11)
    plt.yticks(fontsize=11)
    plt.xlim(-2, 12)
    plt.ylim(-10, 10)
    plt.legend(frameon=True, fontsize=10)
    plt.tight_layout()

    fig_path = os.path.join(outdir, f"{name}_WilliamsPlot_ManualTicks_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()
    print(f"âœ… {name}: saved {fig_path}")

# ---- run and zip ----
assets=[]
for spec in DATASETS:
    williams_for_dataset(spec["name"], spec["path"])
    d = os.path.join(OUTROOT, f"WILLIAMS_{spec['name']}")
    for f in os.listdir(d):
        if f.endswith(".png") or f.endswith(".csv"):
            assets.append(os.path.join(d,f))
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        zf.write(f, os.path.relpath(f, OUTROOT))
print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/WILLIAMS_MANUALTICKS.zip')")

"""##SHAP"""

# ===================== Step 3: SHAP Feature Importance (Random Forest, 600 DPI) =====================
# Schema:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error

# !pip -q install shap  # uncomment if shap is missing

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
TOPK = 10   # set to 5 if you prefer
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "SHAP_RF_PACKAGE.zip")

RF_KW = dict(
    n_estimators=1000,
    max_features="sqrt",
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)

# ---------------- helpers ----------------
def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Required columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block between 'Set' and 'Actual pIC50'.")
    return cols[i_set+1:i_tar]

def shap_rf_for_dataset(name, path, topk=10):
    outdir = os.path.join(OUTROOT, f"SHAP_RF_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    df["_split"] = normalize_split(df["Set"])
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    # descriptors
    desc_cols = get_descriptor_block(df)
    X_tr_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")
    X_te_raw = dfe[desc_cols].apply(pd.to_numeric, errors="coerce")
    y_tr = pd.to_numeric(dft["Actual pIC50"], errors="coerce").values
    y_te = pd.to_numeric(dfe["Actual pIC50"], errors="coerce").values

    # impute + drop constants (fit on TRAIN)
    imp = SimpleImputer(strategy="median")
    X_tr_imp = pd.DataFrame(imp.fit_transform(X_tr_raw), columns=desc_cols)
    X_te_imp = pd.DataFrame(imp.transform(X_te_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(X_tr_imp.fillna(0))
    keep_mask = vt.get_support()
    keep_cols = [c for c,k in zip(desc_cols, keep_mask) if k]
    X_tr = X_tr_imp.loc[:, keep_mask].values
    X_te = X_te_imp.loc[:, keep_mask].values

    # RF
    rf = RandomForestRegressor(**RF_KW)
    rf.fit(X_tr, y_tr)

    # quick metrics (fixed RMSE calc)
    pred_tr = rf.predict(X_tr)
    pred_te = rf.predict(X_te)
    r2_tr = r2_score(y_tr, pred_tr)
    r2_te = r2_score(y_te, pred_te)
    rmse_tr = float(np.sqrt(mean_squared_error(y_tr, pred_tr)))
    rmse_te = float(np.sqrt(mean_squared_error(y_te, pred_te)))
    mae_tr = mean_absolute_error(y_tr, pred_tr)
    mae_te = mean_absolute_error(y_te, pred_te)
    print(f"[{name}] RF  | R2_tr={r2_tr:.3f} RMSE_tr={rmse_tr:.3f} MAE_tr={mae_tr:.3f} "
          f"| R2_te={r2_te:.3f} RMSE_te={rmse_te:.3f} MAE_te={mae_te:.3f}")

    # SHAP on TRAIN
    explainer = shap.TreeExplainer(rf)
    shap_values = explainer.shap_values(X_tr)              # (n_train, n_features)
    mean_abs = np.abs(shap_values).mean(axis=0)            # (n_features,)

    imp_df = pd.DataFrame({"feature": keep_cols, "mean_abs_shap": mean_abs})
    imp_df = imp_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)
    csv_all = os.path.join(outdir, f"{name}_shap_mean_abs_all.csv")
    imp_df.to_csv(csv_all, index=False)

    # plots (top-K)
    topk = min(topk, len(keep_cols))
    top_feats = imp_df["feature"].head(topk).tolist()
    top_idx = [keep_cols.index(f) for f in top_feats]

    # bar
    plt.figure(figsize=(6.2, 4.4))
    plt.barh(range(topk), imp_df["mean_abs_shap"].head(topk)[::-1], color="#4C78A8")
    plt.yticks(range(topk), top_feats[::-1], fontsize=8)
    plt.xlabel("mean(|SHAP value|)", fontsize=11)
    plt.title(f"{name} â€” RF SHAP (Top {topk})", fontsize=12)
    plt.tight_layout()
    fig_bar = os.path.join(outdir, f"{name}_shap_top{topk}_bar_600dpi.png")
    plt.savefig(fig_bar, dpi=600, bbox_inches="tight")
    plt.close()

    # beeswarm (use DataFrame for names)
    X_tr_top_df = pd.DataFrame(X_tr[:, top_idx], columns=top_feats)
    plt.figure(figsize=(6.6, 4.8))
    shap.summary_plot(shap_values[:, top_idx], X_tr_top_df, plot_type="dot", show=False, max_display=topk)
    plt.title(f"{name} â€” RF SHAP Beeswarm (Top {topk})", fontsize=12)
    fig_bee = os.path.join(outdir, f"{name}_shap_top{topk}_beeswarm_600dpi.png")
    plt.savefig(fig_bee, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"   â€¢ saved: {csv_all}")
    print(f"   â€¢ saved: {fig_bar}")
    print(f"   â€¢ saved: {fig_bee}")

# ---------------- run ----------------
assets = []
for spec in DATASETS:
    shap_rf_for_dataset(spec["name"], spec["path"], TOPK)
    d = os.path.join(OUTROOT, f"SHAP_RF_{spec['name']}")
    for f in os.listdir(d):
        if f.endswith(".png") or f.endswith(".csv"):
            assets.append(os.path.join(d, f))

# ---------------- zip only SHAP outputs ----------------
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        zf.write(f, os.path.relpath(f, OUTROOT))

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/SHAP_RF_PACKAGE.zip')")

"""##Y-Randomization"""

# ===================== Step 4: Y-Randomization (RF), 600 DPI =====================
# Files expected:
#   chembl_id | Set | [Fingerprint descriptors â€¦] | Actual pIC50 | Pred pIC50 | residual | abs_residual | squared_error | percent_error
# Inputs:
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
# Outputs (per dataset under /content/YRAND_<name>/):
#   - <name>_yrand_scores.csv           (R2_train, R2_test for each permutation)
#   - <name>_yrand_meta.csv             (observed_R2_test, p_value, n_perm, n_train, p_features)
#   - <name>_yrand_hist_600dpi.png      (histogram of permuted R2_test with observed line)
# Final ZIP with only these assets:
#   /content/YRAND_RF_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import r2_score
from numpy.random import default_rng

# ---------------- CONFIG ----------------
DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv"),
]
N_PERM = 50                    # increase to 100 for a paper; 50 is fast and illustrative
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "YRAND_RF_PACKAGE.zip")

RF_KW = dict(
    n_estimators=1000,
    max_features="sqrt",
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)

# ---------------- helpers ----------------
def normalize_split(series):
    s = series.astype(str).str.strip().str.lower().replace({"tr":"train","te":"test"})
    return s.map(lambda x: "TRAIN" if x=="train" else ("TEST" if x=="test" else x))

def get_descriptor_block(df):
    cols = list(df.columns)
    if "Set" not in cols or "Actual pIC50" not in cols:
        raise ValueError("Required columns 'Set' and 'Actual pIC50' not found.")
    i_set = cols.index("Set")
    i_tar = cols.index("Actual pIC50")
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block between 'Set' and 'Actual pIC50'.")
    return cols[i_set+1:i_tar]

def prep_Xy(df):
    df["_split"] = normalize_split(df["Set"])
    dft = df[df["_split"]=="TRAIN"].reset_index(drop=True)
    dfe = df[df["_split"]=="TEST"].reset_index(drop=True)

    desc_cols = get_descriptor_block(df)

    X_tr_raw = dft[desc_cols].apply(pd.to_numeric, errors="coerce")
    X_te_raw = dfe[desc_cols].apply(pd.to_numeric, errors="coerce")

    imp = SimpleImputer(strategy="median")
    X_tr_imp = pd.DataFrame(imp.fit_transform(X_tr_raw), columns=desc_cols)
    X_te_imp = pd.DataFrame(imp.transform(X_te_raw), columns=desc_cols)

    vt = VarianceThreshold(0.0)
    vt.fit(X_tr_imp.fillna(0))
    keep_mask = vt.get_support()
    keep_cols = [c for c,k in zip(desc_cols, keep_mask) if k]

    X_tr = X_tr_imp.loc[:, keep_mask].values
    X_te = X_te_imp.loc[:, keep_mask].values

    y_tr = pd.to_numeric(dft["Actual pIC50"], errors="coerce").values
    y_te = pd.to_numeric(dfe["Actual pIC50"], errors="coerce").values

    return X_tr, X_te, y_tr, y_te, keep_cols

def observed_r2_test(X_tr, X_te, y_tr, y_te):
    rf = RandomForestRegressor(**RF_KW)
    rf.fit(X_tr, y_tr)
    return r2_score(y_te, rf.predict(X_te))

def y_randomization_for_dataset(name, path, n_perm=50, seed=42):
    outdir = os.path.join(OUTROOT, f"YRAND_{name}")
    os.makedirs(outdir, exist_ok=True)

    df = pd.read_csv(path)
    X_tr, X_te, y_tr, y_te, keep_cols = prep_Xy(df)
    n_train, p_feat = X_tr.shape

    # observed R2 on test (non-permuted)
    r2_obs = observed_r2_test(X_tr, X_te, y_tr, y_te)

    # permutations
    rng = default_rng(seed)
    r2_tr_list, r2_te_list = [], []
    for i in range(n_perm):
        y_perm = y_tr.copy()
        rng.shuffle(y_perm)                      # shuffle TRAIN target only
        rf = RandomForestRegressor(**RF_KW)
        rf.fit(X_tr, y_perm)
        r2_tr_list.append(r2_score(y_perm, rf.predict(X_tr)))
        r2_te_list.append(r2_score(y_te, rf.predict(X_te)))

    scores = pd.DataFrame({"perm_id": np.arange(1, n_perm+1),
                           "R2_train": r2_tr_list,
                           "R2_test":  r2_te_list})
    scores_path = os.path.join(outdir, f"{name}_yrand_scores.csv")
    scores.to_csv(scores_path, index=False)

    # empirical p-value: proportion of shuffled R2_test >= observed
    p_value = (np.sum(np.array(r2_te_list) >= r2_obs) + 1.0) / (n_perm + 1.0)
    meta = pd.DataFrame([{
        "observed_R2_test": r2_obs,
        "p_value": p_value,
        "n_perm": n_perm,
        "n_train": n_train,
        "p_features": p_feat
    }])
    meta_path = os.path.join(outdir, f"{name}_yrand_meta.csv")
    meta.to_csv(meta_path, index=False)

    # histogram (600 DPI)
    plt.figure(figsize=(7.0, 4.6))
    plt.hist(r2_te_list, bins=25, color="#f4a3a3", edgecolor="k", alpha=0.85)
    plt.axvline(r2_obs, color="#1f77b4", linestyle="--", linewidth=1.8)
    plt.xlabel("Permuted RÂ² (test)", fontsize=12)
    plt.ylabel("count", fontsize=12)
    plt.title(f"Y-Randomization â€” {name}\nObserved RÂ² (test) marked by dashed line", fontsize=13)
    plt.tight_layout()
    fig_path = os.path.join(outdir, f"{name}_yrand_hist_600dpi.png")
    plt.savefig(fig_path, dpi=600, bbox_inches="tight")
    plt.close()

    print(f"âœ… {name}: observed RÂ²(test)={r2_obs:.3f} | p-valueâ‰ˆ{p_value:.3f} | n_perm={n_perm}")
    print("   â€¢ Scores:", scores_path)
    print("   â€¢ Meta  :", meta_path)
    print("   â€¢ Plot  :", fig_path)

# ---------------- run both datasets ----------------
assets = []
for spec in DATASETS:
    y_randomization_for_dataset(spec["name"], spec["path"], N_PERM)
    d = os.path.join(OUTROOT, f"YRAND_{spec['name']}")
    for f in os.listdir(d):
        if f.endswith(".png") or f.endswith(".csv"):
            assets.append(os.path.join(d, f))

# ---------------- zip only Y-randomization outputs ----------------
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        zf.write(f, os.path.relpath(f, OUTROOT))

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/YRAND_RF_PACKAGE.zip')")

"""##Hit Identification from RF-QSAR Results"""

# ===================== Step 5: Hit Identification from RF-QSAR Results =====================
# Uses: Pred pIC50, residuals, AD flags (from Williams plot), Actual pIC50
# Outputs:
#   <name>_TopHits.csv (top 10 potent in-domain compounds)
#   HIT_SELECTION_PACKAGE.zip

import os, zipfile, pandas as pd

DATASETS = [
    dict(name="PubChem", path="/content/pubchem_testandtrain.csv",
         ad_flag="/content/WILLIAMS_PubChem/PubChem_williams_table.csv"),
    dict(name="MACCS",   path="/content/MACCS_testandtrain.csv",
         ad_flag="/content/WILLIAMS_MACCS/MACCS_williams_table.csv"),
]
TOPN = 10
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "HIT_SELECTION_PACKAGE.zip")

assets=[]
for spec in DATASETS:
    df = pd.read_csv(spec["path"])
    ad = pd.read_csv(spec["ad_flag"])
    merged = pd.merge(df, ad[["chembl_id","flag_out_of_AD"]], on="chembl_id", how="left")

    hits = merged[(merged["flag_out_of_AD"]==0)].copy()
    hits = hits.sort_values("pred pIC50", ascending=False).head(TOPN)
    outpath = os.path.join(OUTROOT, f"{spec['name']}_TopHits.csv")
    hits.to_csv(outpath, index=False)
    assets.append(outpath)
    print(f"âœ… {spec['name']} top {TOPN} hits saved:", outpath)

with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets: zf.write(f, os.path.relpath(f, OUTROOT))

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/HIT_SELECTION_PACKAGE.zip')")

# ===================== Step 3: Consensus Hit Identification =====================
# Uses PubChem and MACCS hit tables + AD flags (from Williams plot)
# Outputs:
#   - Consensus_Hits.csv (common potent compounds)
#   - Unique_PubChem_Hits.csv, Unique_MACCS_Hits.csv
#   - CONSENSUS_HITS_PACKAGE.zip

import pandas as pd, os, zipfile, numpy as np

# Input files
pub_path = "/content/pubchem_testandtrain.csv"
mac_path = "/content/MACCS_testandtrain.csv"
ad_pub = "/content/PubChem_williams_table.csv"
ad_mac = "/content/MACCS_williams_table.csv"
OUTROOT = "/content"
ZIP_PATH = os.path.join(OUTROOT, "CONSENSUS_HITS_PACKAGE.zip")

# Load and merge AD info
pub = pd.read_csv(pub_path)
mac = pd.read_csv(mac_path)
adp = pd.read_csv(ad_pub)
adm = pd.read_csv(ad_mac)
pub = pub.merge(adp[["chembl_id","flag_out_of_AD"]], on="chembl_id", how="left")
mac = mac.merge(adm[["chembl_id","flag_out_of_AD"]], on="chembl_id", how="left")

# Filter: inside AD only
pub = pub[pub["flag_out_of_AD"]==0].copy()
mac = mac[mac["flag_out_of_AD"]==0].copy()

# Activity and residual thresholds
p_act_thr = pub["pred pIC50"].quantile(0.8)
m_act_thr = mac["pred pIC50"].quantile(0.8)
p_res_thr = pub["abs_residual"].median()
m_res_thr = mac["abs_residual"].median()

pub_hits = pub[(pub["pred pIC50"]>=p_act_thr) & (pub["abs_residual"]<=p_res_thr)]
mac_hits = mac[(mac["pred pIC50"]>=m_act_thr) & (mac["abs_residual"]<=m_res_thr)]

# Common & unique hits
common = pub_hits.merge(mac_hits, on="chembl_id", suffixes=("_PubChem","_MACCS"))
unique_pub = pub_hits[~pub_hits["chembl_id"].isin(common["chembl_id"])]
unique_mac = mac_hits[~mac_hits["chembl_id"].isin(common["chembl_id"])]

# Save
common.to_csv(os.path.join(OUTROOT, "Consensus_Hits.csv"), index=False)
unique_pub.to_csv(os.path.join(OUTROOT, "Unique_PubChem_Hits.csv"), index=False)
unique_mac.to_csv(os.path.join(OUTROOT, "Unique_MACCS_Hits.csv"), index=False)

# ZIP
assets = [os.path.join(OUTROOT, f) for f in [
    "Consensus_Hits.csv","Unique_PubChem_Hits.csv","Unique_MACCS_Hits.csv"
]]
with zipfile.ZipFile(ZIP_PATH,"w",zipfile.ZIP_DEFLATED) as zf:
    for f in assets: zf.write(f, os.path.relpath(f,OUTROOT))

print("âœ… Consensus hits:", len(common))
print("âœ… Unique PubChem hits:", len(unique_pub))
print("âœ… Unique MACCS hits:", len(unique_mac))
print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/CONSENSUS_HITS_PACKAGE.zip')")

# ===================== Consensus Ranking + Diverse Docking Panel =====================
# Inputs (from your previous steps):
#   /content/Consensus_Hits.csv
#   /content/pubchem_testandtrain.csv
#   /content/MACCS_testandtrain.csv
# Output (in /content/CONSENSUS_NEXT/):
#   - consensus_ranked.csv                (all 56 ranked by a consensus score)
#   - docking_panel_top20.csv            (top-N by score)
#   - docking_panel_diverse12.csv        (greedy MaxMin diverse subset on PubChemFP)
#   - CONSENSUS_NEXT_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd

# ---------- CONFIG ----------
CONS_PATH = "/content/Consensus_Hits.csv"
PUB_PATH  = "/content/pubchem_testandtrain.csv"
MAC_PATH  = "/content/MACCS_testandtrain.csv"

TOP_N   = 20   # how many to dock by rank
DIV_N   = 12   # how many diverse reps to dock

OUTDIR  = "/content/CONSENSUS_NEXT"
ZIP_PATH= "/content/CONSENSUS_NEXT_PACKAGE.zip"
os.makedirs(OUTDIR, exist_ok=True)

# ---------- helpers ----------
def get_descriptor_block_between_set_and_target(df, target_name="Actual pIC50"):
    cols = list(df.columns)
    if "Set" not in cols or target_name not in cols:
        raise ValueError("Expected columns 'Set' and target present.")
    i_set = cols.index("Set")
    i_tar = cols.index(target_name)
    if i_tar - i_set <= 1:
        raise ValueError("No descriptor block between 'Set' and target.")
    return cols[i_set+1:i_tar]

def zscore(x):
    x = np.asarray(x, dtype=float)
    mu, sd = np.nanmean(x), np.nanstd(x) + 1e-12
    return (x - mu)/sd

def tanimoto_bit(a, b):
    # a, b are 1D arrays of 0/1
    inter = np.sum((a == 1) & (b == 1))
    ua = np.sum(a == 1)
    ub = np.sum(b == 1)
    denom = ua + ub - inter
    if denom <= 0:
        return 0.0
    return inter / denom

def greedy_maxmin_diversity(Xbin, ids, k):
    # Xbin: (n, p) 0/1; ids: list of chembl_id; select k diverse
    n = Xbin.shape[0]
    if k >= n:
        return list(ids)
    # start with best by rank (index 0), then greedy maximize min distance (1 - tanimoto)
    selected = [0]
    remaining = list(range(1, n))
    # precompute similarities
    # (We compute on the fly to save memory)
    while len(selected) < k and remaining:
        best_idx, best_min_dist = None, -1.0
        for i in remaining:
            dists = []
            for j in selected:
                s = tanimoto_bit(Xbin[i], Xbin[j])
                d = 1.0 - s
                dists.append(d)
            mind = min(dists) if dists else 1.0
            if mind > best_min_dist:
                best_min_dist = mind
                best_idx = i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return [ids[i] for i in selected]

# ---------- load data ----------
cons = pd.read_csv(CONS_PATH)
pub  = pd.read_csv(PUB_PATH)
mac  = pd.read_csv(MAC_PATH)

# The merged Consensus_Hits will have suffixed columns from both sides
# We try to detect the prediction and error columns for each side:
def find_col(df, candidates):
    for c in candidates:
        if c in df.columns: return c
    return None

pred_pub = find_col(cons, ["pred pIC50_PubChem","pred pIC50_pubchem","pred pIC50_Pubchem"])
pred_mac = find_col(cons, ["pred pIC50_MACCS","pred pIC50_maccs"])
ares_pub = find_col(cons, ["abs_residual_PubChem","abs_residual_pubchem"])
ares_mac = find_col(cons, ["abs_residual_MACCS","abs_residual_maccs"])

if any(c is None for c in [pred_pub, pred_mac, ares_pub, ares_mac]):
    raise ValueError("Could not find expected columns in Consensus_Hits.csv. "
                     "Expected suffixed: Pred pIC50_* and abs_residual_* for PubChem & MACCS.")

# ---------- build consensus score ----------
cons = cons.copy()
cons["mean_pred"]   = (cons[pred_pub] + cons[pred_mac]) / 2.0
cons["agree_gap"]   = (cons[pred_pub] - cons[pred_mac]).abs()     # smaller is better
cons["mean_absres"] = (cons[ares_pub] + cons[ares_mac]) / 2.0     # smaller is better

# z-normalize components
z_mean_pred   = zscore(cons["mean_pred"])           # maximize
z_agree       = -zscore(cons["agree_gap"])          # minimize gap -> negate
z_err         = -zscore(cons["mean_absres"])        # minimize error -> negate

# Weighted linear score (tweakable weights)
cons["consensus_score"] = 0.6*z_mean_pred + 0.25*z_agree + 0.15*z_err

# rank
cons_ranked = cons.sort_values("consensus_score", ascending=False).reset_index(drop=True)
rank_path = os.path.join(OUTDIR, "consensus_ranked.csv")
cons_ranked.to_csv(rank_path, index=False)

# ---------- choose Top-N by rank ----------
topN = cons_ranked.head(TOP_N).copy()
topN_path = os.path.join(OUTDIR, f"docking_panel_top{TOP_N}.csv")
topN.to_csv(topN_path, index=False)

# ---------- build diversity on PubChem fingerprints ----------
# join PubChem descriptor block for the consensus (weâ€™ll use PubChem FPs for Tanimoto)
desc_cols_pub = get_descriptor_block_between_set_and_target(pub, target_name="Actual pIC50")
# Binary-ize (many PubChemFP* are already 0/1; ensure type)
fp = pub[["chembl_id"] + desc_cols_pub].copy()
for c in desc_cols_pub:
    fp[c] = pd.to_numeric(fp[c], errors="coerce").fillna(0).round().clip(0,1).astype(int)

# align to ranked order
fp_sel = fp.merge(cons_ranked[["chembl_id"]], on="chembl_id", how="inner")
# ensure same order as cons_ranked for greedy selection
fp_sel = fp_sel.set_index("chembl_id").loc[cons_ranked["chembl_id"]].reset_index()
Xbin = fp_sel[desc_cols_pub].to_numpy(dtype=int)
ids  = fp_sel["chembl_id"].tolist()

div_ids = greedy_maxmin_diversity(Xbin, ids, DIV_N)
diverse = cons_ranked[cons_ranked["chembl_id"].isin(div_ids)].copy()
# keep the greedy order
diverse["__ord"] = diverse["chembl_id"].apply(lambda x: div_ids.index(x))
diverse = diverse.sort_values("__ord").drop(columns="__ord")
div_path = os.path.join(OUTDIR, f"docking_panel_diverse{DIV_N}.csv")
diverse.to_csv(div_path, index=False)

print(f"âœ… Ranked consensus saved: {rank_path}")
print(f"âœ… Top-{TOP_N} by score:   {topN_path}")
print(f"âœ… Diverse-{DIV_N} panel:  {div_path}")

# ---------- ZIP (only these 3 CSVs) ----------
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for p in [rank_path, topN_path, div_path]:
        zf.write(p, os.path.relpath(p, "/content"))

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/CONSENSUS_NEXT_PACKAGE.zip')")

# ===================== Step 6 (Updated): Consensus Hits Clustering + Heatmap (600 DPI) =====================
# Input: /content/consensus_ranked.csv
# Output:
#   - Consensus_Hits_ClusterAssignments.csv
#   - Consensus_Hits_Heatmap_600dpi.png
#   - CONSENSUS_CLUSTER_PACKAGE.zip

import os, zipfile, pandas as pd, numpy as np
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist

# ---------------- CONFIG ----------------
INPUT = "/content/consensus_ranked.csv"
OUTROOT = "/content"
OUTDIR = os.path.join(OUTROOT, "CONSENSUS_CLUSTER")
ZIP_PATH = os.path.join(OUTROOT, "CONSENSUS_CLUSTER_PACKAGE.zip")
os.makedirs(OUTDIR, exist_ok=True)

# ---------------- LOAD ----------------
df = pd.read_csv(INPUT)
print(f"Loaded: {INPUT} | shape={df.shape}")

# ---------------- Identify descriptor columns ----------------
# exclude ID and known numeric target/prediction columns
exclude_cols = [
    "chembl_id", "Actual pIC50_PubChem", "pred pIC50_PubChem",
    "Actual pIC50_MACCS", "pred pIC50_MACCS",
    "abs_residual_PubChem", "abs_residual_MACCS",
    "mean_pred", "agree_gap", "mean_absres", "consensus_score"
]
desc_cols = [c for c in df.columns if c not in exclude_cols and pd.api.types.is_numeric_dtype(df[c])]
print(f"Detected descriptor columns: {len(desc_cols)}")

# ---------------- Preprocess descriptors ----------------
X = df[desc_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------- Hierarchical clustering ----------------
Z = linkage(pdist(X_scaled, metric="euclidean"), method="ward")
n_clusters = max(2, int(np.sqrt(len(df))))  # heuristic
cluster_labels = fcluster(Z, n_clusters, criterion="maxclust")
df["Cluster_ID"] = cluster_labels

# ---------------- Save cluster assignments ----------------
assign_path = os.path.join(OUTDIR, "Consensus_Hits_ClusterAssignments.csv")
cols_to_save = [
    "chembl_id", "Actual pIC50_PubChem", "pred pIC50_PubChem",
    "Actual pIC50_MACCS", "pred pIC50_MACCS", "consensus_score", "Cluster_ID"
]
df[cols_to_save].to_csv(assign_path, index=False)
print(f"âœ… Cluster assignments saved: {assign_path}")

# ---------------- Heatmap with consensus_score color bar ----------------
sns.set(style="white", font_scale=0.8)
plt.figure(figsize=(6.8, 5.8))

# order rows by cluster ID for neat grouping
row_order = np.argsort(df["Cluster_ID"].values)
X_ordered = X_scaled[row_order, :]
row_labels = df["chembl_id"].iloc[row_order]

# create cluster heatmap
cg = sns.clustermap(
    X_ordered, method="ward", cmap="vlag", figsize=(6.8,5.8),
    row_cluster=False, col_cluster=False,
    yticklabels=row_labels, cbar_kws={"label": "Standardized descriptor value"}
)
plt.title("Consensus Hits â€” Clustering Heatmap", fontsize=11, pad=70)
heatmap_path = os.path.join(OUTDIR, "Consensus_Hits_Heatmap_600dpi.png")
plt.savefig(heatmap_path, dpi=600, bbox_inches="tight")
plt.close("all")

# ---------------- Zip results ----------------
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in [assign_path, heatmap_path]:
        zf.write(f, os.path.relpath(f, OUTROOT))

print("\nðŸ“¦ ZIP ready:", ZIP_PATH)
print("from google.colab import files; files.download('/content/CONSENSUS_CLUSTER_PACKAGE.zip')")

# ===================== Focused Heatmaps for Consensus Hits (readable, 600 DPI) =====================
# Input:  /content/consensus_ranked.csv  (your 56 consensus hits, ranked)
# Outputs in /content/CONSENSUS_HEATMAP_FOCUSED/ :
#   - heatmap_top10x30_600dpi.png          (Top 10 compounds Ã— Top 30 descriptors)
#   - heatmap_allx30_600dpi.png            (All 56 compounds Ã— Top 30 descriptors)  [optional]
#   - selected_top_compounds.csv
#   - selected_top_descriptors.csv
#   - CONSENSUS_HEATMAP_FOCUSED.zip

import os, zipfile, numpy as np, pandas as pd
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# -------- settings you can tweak --------
INPUT = "/content/consensus_ranked.csv"
TOP_COMPOUNDS = 10     # you asked for ~top 10
TOP_DESCRIPTORS = 30   # keep readable; raise/lower as needed
MAKE_ALLxTOPM   = True # also export (all compounds Ã— top M descriptors)
OUTDIR = "/content/CONSENSUS_HEATMAP_FOCUSED"
ZIP_PATH = "/content/CONSENSUS_HEATMAP_FOCUSED.zip"
os.makedirs(OUTDIR, exist_ok=True)

# -------- load --------
df = pd.read_csv(INPUT)

# columns to exclude from descriptor block (adjusted to your file)
exclude_cols = {
    "chembl_id","Actual pIC50_PubChem","pred pIC50_PubChem",
    "Actual pIC50_MACCS","pred pIC50_MACCS",
    "abs_residual_PubChem","abs_residual_MACCS",
    "mean_pred","agree_gap","mean_absres","consensus_score","Cluster_ID"
}
desc_cols = [c for c in df.columns
             if (c not in exclude_cols) and pd.api.types.is_numeric_dtype(df[c])]

# sanity
if len(desc_cols) < 5:
    raise ValueError("No descriptor columns detected. Check the file/column names.")

# -------- pick top-N compounds by consensus_score --------
df_sorted = df.sort_values("consensus_score", ascending=False).reset_index(drop=True)
top_cmp = df_sorted.head(TOP_COMPOUNDS).copy()
top_cmp_ids = top_cmp["chembl_id"].tolist()
pd.DataFrame({"chembl_id": top_cmp_ids}).to_csv(
    os.path.join(OUTDIR, "selected_top_compounds.csv"), index=False
)

# -------- select top-M descriptors by variance (within top compounds) --------
X_top = top_cmp[desc_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
var_series = X_top.var(axis=0)
top_desc = var_series.sort_values(ascending=False).head(TOP_DESCRIPTORS).index.tolist()
pd.DataFrame({"descriptor": top_desc}).to_csv(
    os.path.join(OUTDIR, "selected_top_descriptors.csv"), index=False
)

def plot_heatmap(data_df, title, outpath, figsize=(10,7)):
    # z-score per column (descriptor) for better contrast
    scaler = StandardScaler()
    X = scaler.fit_transform(data_df)
    # rows = compounds; columns = descriptors
    sns.set(style="white", font_scale=0.9)
    g = sns.clustermap(
        X, method="average", metric="correlation",
        cmap="vlag", figsize=figsize,
        row_cluster=True, col_cluster=True,
        xticklabels=data_df.columns, yticklabels=data_df.index
    )
    # improve label visibility
    g.ax_heatmap.set_xlabel("Descriptors", fontsize=12)
    g.ax_heatmap.set_ylabel("Compounds (chembl_id)", fontsize=12)
    g.fig.suptitle(title, fontsize=13, y=1.02)
    # rotate x labels
    for lbl in g.ax_heatmap.get_xmajorticklabels():
        lbl.set_rotation(90)
        lbl.set_fontsize(8)
    for lbl in g.ax_heatmap.get_ymajorticklabels():
        lbl.set_fontsize(9)
    plt.savefig(outpath, dpi=600, bbox_inches="tight")
    plt.close("all")

# -------- heatmap: Top-10 compounds Ã— Top-30 descriptors --------
HM1_df = top_cmp.set_index("chembl_id")[top_desc]
plot_heatmap(
    HM1_df,
    title=f"Consensus Hits â€” Top {TOP_COMPOUNDS} Ã— Top {TOP_DESCRIPTORS}",
    outpath=os.path.join(OUTDIR, f"heatmap_top{TOP_COMPOUNDS}x{TOP_DESCRIPTORS}_600dpi.png"),
    figsize=(10,7)  # wider figure for readability
)

# -------- optional: All compounds Ã— Top-30 descriptors --------
if MAKE_ALLxTOPM:
    ALL_df = df_sorted.set_index("chembl_id")[top_desc]
    plot_heatmap(
        ALL_df,
        title=f"Consensus Hits â€” All ({len(df_sorted)}) Ã— Top {TOP_DESCRIPTORS}",
        outpath=os.path.join(OUTDIR, f"heatmap_allx{TOP_DESCRIPTORS}_600dpi.png"),
        figsize=(11,10)
    )

# -------- zip only these outputs --------
assets = [os.path.join(OUTDIR, f) for f in os.listdir(OUTDIR)]
with zipfile.ZipFile(ZIP_PATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        zf.write(f, os.path.relpath(f, "/content"))

print("\nâœ… Saved focused heatmaps and selections in:", OUTDIR)
print("ðŸ“¦ ZIP:", ZIP_PATH)
print("from google.colab import files; files.download('/content/CONSENSUS_HEATMAP_FOCUSED.zip')")

# ===================== Consensus Hits: Focused Clustering + Tanimoto (600 DPI) =====================
# Input
#   /content/consensus_ranked.csv     (your 56 consensus hits with descriptors)
#
# Outputs in /content/CONSENSUS_HEATMAP_FOCUSED/ :
#   - heatmap_top10x30_600dpi.png                 (Top 10 compounds Ã— Top 30 descriptors)
#   - heatmap_allx30_600dpi.png                   (All 56 Ã— Top 30 descriptors)   [optional toggle]
#   - tanimoto_top10_matrix.csv                   (Top 10 pairwise Tanimoto)
#   - tanimoto_top10_heatmap_600dpi.png
#   - selected_top_compounds.csv
#   - selected_top_descriptors.csv
#   - CONSENSUS_HEATMAP_FOCUSED.zip               (only the above files)
#
# Notes
#   â€¢ Clustering uses ONLY descriptor columns (no residuals / errors / predictions).
#   â€¢ Descriptor selection = highest variance among the Top-10 compounds.
#   â€¢ Tanimoto uses PubChem fingerprint bits; falls back to MACCS if PubChem bits not present.

import os, zipfile, numpy as np, pandas as pd
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist, squareform

# ---------------- settings ----------------
INPUT = "/content/consensus_ranked.csv"
TOP_COMPOUNDS   = 10
TOP_DESCRIPTORS = 30
MAKE_ALLxTOPM   = True  # also save all-compounds Ã— top-M descriptors heatmap

OUTDIR  = "/content/CONSENSUS_HEATMAP_FOCUSED"
ZIPPATH = "/content/CONSENSUS_HEATMAP_FOCUSED.zip"
os.makedirs(OUTDIR, exist_ok=True)

# ---------------- helpers ----------------
def is_descriptor_col(colname: str) -> bool:
    """Keep numeric descriptor columns only; drop IDs, targets, preds, errors, meta."""
    c = colname.lower()
    bad_substr = [
        "chembl_id", "set", "consensus_score", "mean_pred", "agree_gap", "mean_absres",
        "actual pic50", "pred pic50", "residual", "abs_residual", "squared_error",
        "percent_error", "flag_out_of_ad", "cluster_id"
    ]
    return not any(s in c for s in bad_substr)

def to_numeric_df(df: pd.DataFrame) -> pd.DataFrame:
    return df.apply(pd.to_numeric, errors="coerce").fillna(0)

def plot_cluster_heatmap(data_df: pd.DataFrame, title: str, outpath: str, figsize=(10,7)):
    """Z-score columns and plot clustered heatmap (correlation distance), 600 DPI."""
    scaler = StandardScaler()
    X = scaler.fit_transform(data_df)
    sns.set(style="white", font_scale=0.9)
    g = sns.clustermap(
        X, method="average", metric="correlation",
        cmap="vlag", figsize=figsize,
        row_cluster=True, col_cluster=True,
        xticklabels=data_df.columns, yticklabels=data_df.index
    )
    g.ax_heatmap.set_xlabel("Descriptors", fontsize=12)
    g.ax_heatmap.set_ylabel("Compounds (chembl_id)", fontsize=12)
    g.fig.suptitle(title, fontsize=13, y=1.02)
    for lbl in g.ax_heatmap.get_xmajorticklabels():
        lbl.set_rotation(90); lbl.set_fontsize(8)
    for lbl in g.ax_heatmap.get_ymajorticklabels():
        lbl.set_fontsize(9)
    plt.savefig(outpath, dpi=600, bbox_inches="tight")
    plt.close("all")

def tanimoto_bit(a: np.ndarray, b: np.ndarray) -> float:
    inter = np.sum((a == 1) & (b == 1))
    ua = np.sum(a == 1); ub = np.sum(b == 1)
    denom = ua + ub - inter
    return 0.0 if denom <= 0 else inter / denom

def make_tanimoto_matrix(Xbin: np.ndarray, ids):
    n = Xbin.shape[0]
    M = np.zeros((n, n), dtype=float)
    for i in range(n):
        M[i, i] = 1.0
        for j in range(i+1, n):
            s = tanimoto_bit(Xbin[i], Xbin[j])
            M[i, j] = M[j, i] = s
    return pd.DataFrame(M, index=ids, columns=ids)

# ---------------- load ----------------
df = pd.read_csv(INPUT)
if "chembl_id" not in df.columns:
    raise ValueError("Expected 'chembl_id' in the input file.")

# detect descriptor columns (numeric + not in excluded list)
num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
desc_cols = [c for c in num_cols if is_descriptor_col(c)]
if len(desc_cols) < 5:
    raise ValueError("Too few descriptor columns detected. Check file or exclusions.")
# focus selection columns saved for traceability
meta_cols = [c for c in df.columns if not is_descriptor_col(c)]

# ---- select top compounds by consensus_score (fall back to Pred pIC50_PubChem if absent) ----
score_col = "consensus_score" if "consensus_score" in df.columns else \
            ("pred pIC50_PubChem" if "pred pIC50_PubChem" in df.columns else None)
if score_col is None:
    raise ValueError("No 'consensus_score' or 'pred pIC50_PubChem' found to rank compounds.")

df_sorted = df.sort_values(score_col, ascending=False).reset_index(drop=True)
top_cmp = df_sorted.head(TOP_COMPOUNDS).copy()
top_cmp_ids = top_cmp["chembl_id"].tolist()
pd.DataFrame({"chembl_id": top_cmp_ids}).to_csv(os.path.join(OUTDIR, "selected_top_compounds.csv"), index=False)

# ---- pick top descriptors by variance within the top compounds ----
X_top = to_numeric_df(top_cmp[desc_cols])
var_series = X_top.var(axis=0)
top_desc = var_series.sort_values(ascending=False).head(TOP_DESCRIPTORS).index.tolist()
pd.DataFrame({"descriptor": top_desc}).to_csv(os.path.join(OUTDIR, "selected_top_descriptors.csv"), index=False)

# ---- heatmap: Top-10 Ã— Top-30 ----
HM1_df = top_cmp.set_index("chembl_id")[top_desc]
plot_cluster_heatmap(HM1_df, f"Consensus Hits â€” Top {TOP_COMPOUNDS} Ã— Top {TOP_DESCRIPTORS}",
                     os.path.join(OUTDIR, f"heatmap_top{TOP_COMPOUNDS}x{TOP_DESCRIPTORS}_600dpi.png"),
                     figsize=(10,7))

# ---- optional: All compounds Ã— Top-30 ----
if MAKE_ALLxTOPM:
    ALL_df = df_sorted.set_index("chembl_id")[top_desc]
    plot_cluster_heatmap(ALL_df, f"Consensus Hits â€” All ({len(df_sorted)}) Ã— Top {TOP_DESCRIPTORS}",
                         os.path.join(OUTDIR, f"heatmap_allx{TOP_DESCRIPTORS}_600dpi.png"),
                         figsize=(11,10))

# ===================== Tanimoto among Top-10 (PubChem bits preferred) =====================
# find PubChem bit columns; if none, fall back to MACCS bits
pub_bits = [c for c in desc_cols if c.lower().startswith("pubchemfp")]
maccs_bits = [c for c in desc_cols if c.lower().startswith("maccsfp")]
fp_cols = pub_bits if len(pub_bits) > 0 else maccs_bits
if len(fp_cols) == 0:
    print("âš ï¸ No PubChem/MACCS fingerprint bit columns found; skipping Tanimoto.")
else:
    fp_top = to_numeric_df(top_cmp[fp_cols]).round().clip(0,1).astype(int)
    Xbin = fp_top.to_numpy(dtype=int)
    ids = top_cmp["chembl_id"].tolist()
    tan = make_tanimoto_matrix(Xbin, ids)
    tan_path = os.path.join(OUTDIR, "tanimoto_top10_matrix.csv")
    tan.to_csv(tan_path)

    # heatmap of similarity
    sns.set(style="white", font_scale=0.9)
    plt.figure(figsize=(7.2,6.4))
    ax = sns.heatmap(tan, cmap="viridis", vmin=0, vmax=1, square=True,
                     cbar_kws={"label":"Tanimoto similarity"})
    plt.title("Top-10 Consensus Hits â€” Tanimoto Similarity", fontsize=13)
    plt.xticks(rotation=90); plt.yticks(rotation=0)
    plt.tight_layout()
    tan_png = os.path.join(OUTDIR, "tanimoto_top10_heatmap_600dpi.png")
    plt.savefig(tan_png, dpi=600, bbox_inches="tight")
    plt.close()
    print(f"âœ… Tanimoto matrix saved: {tan_path}")

# ---------------- zip (only outputs from this step) ----------------
assets = [os.path.join(OUTDIR, f) for f in os.listdir(OUTDIR)]
with zipfile.ZipFile(ZIPPATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in assets:
        zf.write(f, os.path.relpath(f, "/content"))

print("\nâœ… Outputs folder:", OUTDIR)
print("ðŸ“¦ ZIP:", ZIPPATH)
print("from google.colab import files; files.download('/content/CONSENSUS_HEATMAP_FOCUSED.zip')")

# ===================== SHAP-based Heatmap (robust, auto-recover, 600 DPI) =====================
# Input:  /content/consensus_ranked.csv
# Output (in /content/SHAP_HEATMAP/):
#   - shap_top_features.csv
#   - dropped_constant_features.csv         (if any)
#   - added_variance_features.csv           (if any augmentation used)
#   - shap_heatmap_final_600dpi.png
#   - selected_compounds_used.csv
#   - SHAP_HEATMAP_PACKAGE.zip

import os, zipfile, numpy as np, pandas as pd
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import shap

# ---------------- settings ----------------
INPUT = "/content/consensus_ranked.csv"
TOP_N_COMPOUNDS   = 10        # target row count
TOPK_PUBCHEM      = 10        # target SHAP features per family
TOPK_MACCS        = 10
MAX_ROWS_EXPAND   = 20        # auto-expand up to this many rows if needed
MIN_DESC_REQUIRED = 12        # we try to have at least this many varying descriptors
OUTDIR  = "/content/SHAP_HEATMAP"
ZIPPATH = "/content/SHAP_HEATMAP_PACKAGE.zip"
os.makedirs(OUTDIR, exist_ok=True)

# ---------------- helpers ----------------
def is_descriptor(c: str) -> bool:
    lc = c.lower()
    bad = ["chembl_id","consensus_score","mean_pred","agree_gap","mean_absres",
           "actual pic50","pred pic50","residual","abs_residual","squared_error",
           "percent_error","flag_out_of_ad","cluster_id","set"]
    return (lc.startswith("pubchemfp") or lc.startswith("maccsfp")) and not any(b in lc for b in bad)

def pick_target(df: pd.DataFrame) -> pd.Series:
    acts = [c for c in df.columns if c.lower().startswith("actual pic50")]
    if acts:
        y = df[acts].apply(pd.to_numeric, errors="coerce").mean(axis=1)
        if np.isfinite(y).sum() >= 2:   # need >=2 samples
            return y
    preds = [c for c in df.columns if c.lower().startswith("pred pic50")]
    if not preds:
        raise ValueError("Need Actual pIC50_* or Pred pIC50_* columns.")
    return df[preds].apply(pd.to_numeric, errors="coerce").mean(axis=1)

def rf_shap_importance(X: np.ndarray, y: np.ndarray, feature_names):
    rf = RandomForestRegressor(n_estimators=600, max_features="sqrt",
                               random_state=42, n_jobs=-1)
    rf.fit(X, y)
    expl = shap.TreeExplainer(rf)
    sv = expl.shap_values(X)  # (n, p)
    mean_abs = np.abs(sv).mean(axis=0)
    imp = pd.DataFrame({"feature": feature_names, "mean_abs_shap": mean_abs})
    return imp.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)

def numeric(df):
    return df.apply(pd.to_numeric, errors="coerce").replace([np.inf,-np.inf], np.nan).fillna(0.0)

# ---------------- load ----------------
df = pd.read_csv(INPUT)
if "chembl_id" not in df.columns:
    raise ValueError("Expected 'chembl_id' in consensus_ranked.csv")

rank_col = "consensus_score" if "consensus_score" in df.columns else \
           ("pred pIC50_PubChem" if "pred pIC50_PubChem" in df.columns else None)
if rank_col is None:
    raise ValueError("Need 'consensus_score' or 'pred pIC50_PubChem' to rank compounds.")

df_ranked = df.sort_values(rank_col, ascending=False).reset_index(drop=True)
desc_all = [c for c in df.columns if is_descriptor(c)]
if len(desc_all) < 5:
    raise ValueError("Too few descriptor columns detected (need PubChemFP*/MACCSFP*).")

# ---- SHAP on all rows (stable importance)
X_all = numeric(df[desc_all])
X_all = SimpleImputer(strategy="most_frequent").fit_transform(X_all)
y_all = pick_target(df).to_numpy()
shap_imp = rf_shap_importance(X_all, y_all, desc_all)

# split per family
imp_pub  = shap_imp[shap_imp["feature"].str.lower().str.startswith("pubchemfp")]
imp_macc = shap_imp[shap_imp["feature"].str.lower().str.startswith("maccsfp")]
top_pub  = imp_pub.head(TOPK_PUBCHEM)
top_macc = imp_macc.head(TOPK_MACCS)
top_union = pd.concat([top_pub, top_macc], ignore_index=True).drop_duplicates("feature")
top_union_path = os.path.join(OUTDIR, "shap_top_features.csv")
top_union.to_csv(top_union_path, index=False)

# ---- build initial matrix (rows: Top N, cols: union SHAP)
N = min(TOP_N_COMPOUNDS, len(df_ranked))
rows_df = df_ranked.head(max(N,2)).copy()  # ensure >=2 rows
sel_cols = top_union["feature"].tolist()
hm_df = numeric(rows_df.set_index("chembl_id")[sel_cols])

# ---- drop constant columns
col_var = hm_df.var(axis=0)
const_cols = col_var[col_var <= 1e-12].index.tolist()
if const_cols:
    pd.DataFrame({"dropped_constant_feature": const_cols}).to_csv(
        os.path.join(OUTDIR, "dropped_constant_features.csv"), index=False
    )
    hm_df = hm_df.drop(columns=const_cols)

# ---- if not enough varying descriptors, auto-expand rows & add variance features
added_variance = []
if hm_df.shape[1] < MIN_DESC_REQUIRED:
    # expand rows up to MAX_ROWS_EXPAND
    N2 = min(MAX_ROWS_EXPAND, len(df_ranked))
    rows_df = df_ranked.head(max(N2,2)).copy()
    hm_df = numeric(rows_df.set_index("chembl_id")[sel_cols])
    col_var = hm_df.var(axis=0)
    const_cols = col_var[col_var <= 1e-12].index.tolist()
    hm_df = hm_df.drop(columns=const_cols)
    # if still short, add highest-variance descriptors from ALL descriptor set
    if hm_df.shape[1] < MIN_DESC_REQUIRED:
        remaining = [c for c in desc_all if c not in hm_df.columns]
        if remaining:
            var_all = numeric(rows_df[remaining]).var(axis=0).sort_values(ascending=False)
            need = MIN_DESC_REQUIRED - hm_df.shape[1]
            add_cols = var_all.head(max(need,0)).index.tolist()
            if add_cols:
                added_variance = add_cols
                hm_df = pd.concat([hm_df, numeric(rows_df.set_index("chembl_id")[add_cols])], axis=1)
                pd.DataFrame({"added_variance_feature": added_variance}).to_csv(
                    os.path.join(OUTDIR, "added_variance_features.csv"), index=False
                )

# final sanity
if hm_df.shape[0] < 2 or hm_df.shape[1] < 2:
    raise ValueError("Not enough varying data to plot. Try increasing TOPK_PUBCHEM/MACCS or MAX_ROWS_EXPAND.")

# ---- z-score columns; cluster with Euclidean (robust)
Z = StandardScaler().fit_transform(hm_df)

sns.set(style="white", font_scale=0.9)
g = sns.clustermap(
    Z, method="average", metric="euclidean",
    cmap="vlag", figsize=(10.8,7.8),
    row_cluster=True, col_cluster=True,
    xticklabels=hm_df.columns, yticklabels=hm_df.index
)
g.ax_heatmap.set_xlabel("Descriptors (SHAP union + optional high-variance)", fontsize=12)
g.ax_heatmap.set_ylabel("Compounds (Top consensus)", fontsize=12)
title = f"SHAP-based Heatmap â€” {hm_df.shape[0]} compounds Ã— {hm_df.shape[1]} descriptors"
g.fig.suptitle(title, fontsize=13, y=1.02)
for lbl in g.ax_heatmap.get_xmajorticklabels():
    lbl.set_rotation(90); lbl.set_fontsize(8)
for lbl in g.ax_heatmap.get_ymajorticklabels():
    lbl.set_fontsize(9)

png_path = os.path.join(OUTDIR, "shap_heatmap_final_600dpi.png")
plt.savefig(png_path, dpi=600, bbox_inches="tight")
plt.close("all")

# save which compounds were used
rows_df[["chembl_id", rank_col]].to_csv(os.path.join(OUTDIR, "selected_compounds_used.csv"), index=False)

# ---- zip (only this step's assets)
with zipfile.ZipFile(ZIPPATH, "w", zipfile.ZIP_DEFLATED) as zf:
    for f in os.listdir(OUTDIR):
        zf.write(os.path.join(OUTDIR, f), f)

print("âœ… Figure:", png_path)
print("âœ… Features:", top_union_path)
if const_cols:
    print("âœ… Dropped constants: /content/SHAP_HEATMAP/dropped_constant_features.csv")
if added_variance:
    print("âœ… Added variance features:", len(added_variance))
print("âœ… Rows used saved to: /content/SHAP_HEATMAP/selected_compounds_used.csv")
print("\nðŸ“¦ ZIP ready:", ZIPPATH)
print("from google.colab import files; files.download('/content/SHAP_HEATMAP_PACKAGE.zip')")

"""##final Heatmap_Consenus"""

# Paths you already have
PUB_TOP = "/content/PubChem_TopHits.csv"
MAC_TOP = "/content/MACCS_TopHits.csv"

# Full matrices with descriptors
PUB_FULL = "/content/pubchem_testandtrain.csv"
MAC_FULL = "/content/MACCS_testandtrain.csv"

OUTROOT = "/content/HITS_HEATMAPS"
DPI = 600

import os, re, json, zipfile
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm

os.makedirs(OUTROOT, exist_ok=True)
sns.set(context="notebook", style="white", font_scale=0.9)
print("Output root:", OUTROOT)

def detect_id_col(df):
    for c in ["chembl_id","molecule_chembl_id","id"]:
        if c in df.columns: return c
    # fallback
    for c in df.columns:
        if df[c].dtype==object: return c
    return None

def detect_pred_col(df):
    keys = ["pred pIC50","pred_pIC50","Pred pIC50","predicted pIC50","prediction"]
    for k in keys:
        if k in df.columns: return k
    for c in df.columns:
        if re.search(r"pred.*pic50", c, flags=re.I): return c
    return None

def detect_actual_col(df):
    keys = ["Actual pIC50","actual pIC50","pIC50"]
    for k in keys:
        if k in df.columns: return k
    for c in df.columns:
        if re.search(r"(actual|exp|obs).*pic50", c, flags=re.I): return c
    return None

def detect_error_cols(df):
    out = {}
    for k in ["residual","abs_residual","squared_error","percent_error","flag_out_of_AD","flag_out_of_ad","ad_flag"]:
        for c in df.columns:
            if k.lower() in c.lower():
                out[k] = c
                break
    return out

def get_descriptor_block_cols(df):
    # descriptors are between 'Set' and 'Actual pIC50' in your full files
    cols = df.columns.tolist()
    if "Set" not in cols:
        return []
    # pick first column that looks like target
    tar = detect_actual_col(df)
    if tar is None:
        return []
    i_set = cols.index("Set")
    i_tar = cols.index(tar)
    if i_tar - i_set <= 1:
        return []
    return cols[i_set+1:i_tar]

def to_binary_array(df_desc):
    X = df_desc.copy().apply(pd.to_numeric, errors="coerce").fillna(0.0).values
    return (X >= 0.5).astype(np.uint8)

def tanimoto_bit(a, b):
    inter = np.sum((a==1) & (b==1))
    ua, ub = np.sum(a==1), np.sum(b==1)
    denom = ua + ub - inter
    return 0.0 if denom<=0 else inter/denom

def tanimoto_matrix(Xbin):
    n = Xbin.shape[0]
    M = np.eye(n)
    for i in range(n):
        ai = Xbin[i]
        for j in range(i+1, n):
            s = tanimoto_bit(ai, Xbin[j])
            M[i,j] = M[j,i] = s
    return pd.DataFrame(M)

def audit(name, df, outdir, desc_cols_found):
    os.makedirs(outdir, exist_ok=True)
    idc = detect_id_col(df)
    act = detect_actual_col(df)
    pred = detect_pred_col(df)
    errs = detect_error_cols(df)
    print(f"\n=== AUDIT: {name} ===")
    print("Rows, Cols:", df.shape)
    print("ID:", idc)
    print("Actual pIC50:", act)
    print("Pred pIC50:", pred)
    print("Errors/meta:", {k:v for k,v in errs.items() if v})
    print("Descriptor count:", len(desc_cols_found))
    with open(os.path.join(outdir, f"{name}_audit.json"), "w") as f:
        json.dump({"shape": df.shape, "id": idc, "actual": act, "pred": pred,
                   "errors": errs, "n_desc": len(desc_cols_found),
                   "sample_desc": desc_cols_found[:10]}, f, indent=2)
    return idc, act, pred, errs

# --- Step 3: Load & audit (TopHits ONLY, no descriptors needed) ---

import os, re, json, zipfile
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import cm

PUB_TOP   = "/content/PubChem_TopHits.csv"
MAC_TOP   = "/content/MACCS_TopHits.csv"
OUTROOT   = "/content/HITS_HEATMAPS"
PUB_DIR   = os.path.join(OUTROOT, "PubChem")
MAC_DIR   = os.path.join(OUTROOT, "MACCS")
CONS_DIR  = os.path.join(OUTROOT, "CONSENSUS")
for d in [OUTROOT, PUB_DIR, MAC_DIR, CONS_DIR]:
    os.makedirs(d, exist_ok=True)

DPI = 600
sns.set(context="notebook", style="white", font_scale=0.9)

def detect_id_col(df):
    for c in ["chembl_id","molecule_chembl_id","id"]:
        if c in df.columns: return c
    for c in df.columns:
        if df[c].dtype==object: return c
    raise ValueError("ID column not found (expected chembl_id/molecule_chembl_id/id).")

def detect_pred_col(df):
    keys = ["pred pIC50","Pred pIC50","pred_pIC50","prediction"]
    for k in keys:
        if k in df.columns: return k
    for c in df.columns:
        if re.search(r"pred.*pic50", c, flags=re.I): return c
    return None

def detect_actual_col(df):
    keys = ["Actual pIC50","actual pIC50","pIC50"]
    for k in keys:
        if k in df.columns: return k
    for c in df.columns:
        if re.search(r"(actual|exp|obs).*pic50", c, flags=re.I): return c
    return None

def detect_error_cols(df):
    found = {}
    for key in ["residual","abs_residual","squared_error","percent_error","flag_out_of_AD","flag_out_of_ad","ad_flag"]:
        for c in df.columns:
            if key.lower() in c.lower():
                found[key] = c
                break
    return found

def audit_top(df, name, outdir):
    idc  = detect_id_col(df)
    act  = detect_actual_col(df)
    pred = detect_pred_col(df)
    errs = detect_error_cols(df)
    info = {
        "shape": df.shape,
        "id_col": idc,
        "actual_col": act,
        "pred_col": pred,
        "error_cols": errs
    }
    with open(os.path.join(outdir, f"{name}_audit.json"), "w") as f:
        json.dump(info, f, indent=2)
    print(f"\n=== AUDIT: {name} ===")
    print("Rows, Cols:", df.shape)
    print("ID:", idc)
    print("Actual pIC50:", act)
    print("Pred pIC50:", pred)
    print("Errors/meta:", {k:v for k,v in errs.items() if v})
    return idc, act, pred, errs

pub = pd.read_csv(PUB_TOP)
mac = pd.read_csv(MAC_TOP)

pub_id, pub_act, pub_pred, pub_errs = audit_top(pub, "PubChem_TopHits", PUB_DIR)
mac_id, mac_act, mac_pred, mac_errs = audit_top(mac, "MACCS_TopHits",  MAC_DIR)

# Build slim tables (IDs + predictions + errors)
def slim(df, idc, predc, actc, errs):
    keep = [idc]
    if predc: keep.append(predc)
    if actc:  keep.append(actc)
    for k in ["abs_residual","percent_error","residual","squared_error","flag_out_of_AD","flag_out_of_ad","ad_flag"]:
        if errs.get(k): keep.append(errs[k])
    out = df[keep].copy().drop_duplicates(subset=[idc])
    out = out.rename(columns={idc:"chembl_id",
                              predc:"Pred_pIC50",
                              actc:"Actual_pIC50"} | {v:k for k,v in errs.items() if v})
    return out

pub_s = slim(pub, pub_id, pub_pred, pub_act, pub_errs).rename(columns={
    "Pred_pIC50":"Pred_pIC50_PubChem",
    "Actual_pIC50":"Actual_pIC50_PubChem",
    "abs_residual":"abs_residual_PubChem",
    "percent_error":"percent_error_PubChem",
    "residual":"residual_PubChem",
    "squared_error":"squared_error_PubChem",
    "flag_out_of_AD":"flag_out_of_AD_PubChem",
    "flag_out_of_ad":"flag_out_of_AD_PubChem",
    "ad_flag":"flag_out_of_AD_PubChem"
})

mac_s = slim(mac, mac_id, mac_pred, mac_act, mac_errs).rename(columns={
    "Pred_pIC50":"Pred_pIC50_MACCS",
    "Actual_pIC50":"Actual_pIC50_MACCS",
    "abs_residual":"abs_residual_MACCS",
    "percent_error":"percent_error_MACCS",
    "residual":"residual_MACCS",
    "squared_error":"squared_error_MACCS",
    "flag_out_of_AD":"flag_out_of_AD_MACCS",
    "flag_out_of_ad":"flag_out_of_AD_MACCS",
    "ad_flag":"flag_out_of_AD_MACCS"
})

# Consensus union
cons = pd.merge(pub_s, mac_s, on="chembl_id", how="outer")

def src_tag(r):
    p = not pd.isna(r.get("Pred_pIC50_PubChem"))
    m = not pd.isna(r.get("Pred_pIC50_MACCS"))
    if p and m: return "Both"
    if p: return "PubChem"
    if m: return "MACCS"
    return "Unknown"

cons["Source"] = cons.apply(src_tag, axis=1)
cons["mean_pred"]   = cons[["Pred_pIC50_PubChem","Pred_pIC50_MACCS"]].mean(axis=1, skipna=True)
cons["agree_gap"]   = (cons["Pred_pIC50_PubChem"] - cons["Pred_pIC50_MACCS"]).abs()
cons["mean_absres"] = cons[["abs_residual_PubChem","abs_residual_MACCS"]].mean(axis=1, skipna=True)

def rank_minmax(x, ascend=True):
    s = x.rank(method="average", ascending=ascend)
    return (s - s.min())/(s.max()-s.min() + 1e-12)

r_mean = rank_minmax(cons["mean_pred"],  ascend=False)  # higher better
r_gap  = rank_minmax(cons["agree_gap"],  ascend=True)   # smaller better
r_err  = rank_minmax(cons["mean_absres"], ascend=True)  # smaller better
cons["Consensus_Score"] = 0.6*r_mean + 0.25*(1.0 - r_gap) + 0.15*(1.0 - r_err)

cons = cons.sort_values("Consensus_Score", ascending=False).reset_index(drop=True)
cons_path = os.path.join(CONS_DIR, "Consensus_Union_Table.csv")
cons.to_csv(cons_path, index=False)
print("âœ… Consensus table saved:", cons_path)

# --- Step 4 (fixed): Meta-feature clustering without descriptors, with proper index alignment ---

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import cm

DPI = 600
sns.set(context="notebook", style="white", font_scale=0.9)

def zscore(df):
    return (df - df.mean()) / (df.std(ddof=0) + 1e-12)

def make_row_colors(source, score):
    cmap_cat = {"Both":"#1b9e77","PubChem":"#7570b3","MACCS":"#d95f02","Unknown":"#999999"}
    row1 = pd.Series([cmap_cat.get(s,"#999999") for s in source], name="Source")
    sc = (score - np.nanmin(score))/(np.nanmax(score)-np.nanmin(score)+1e-12)
    row2 = pd.Series([cm.get_cmap("viridis")(v) for v in sc], name="Consensus")
    return [row1, row2]

def clustermap_from_features(name, feat_df, meta_df, outdir):
    """
    feat_df: DataFrame indexed by chembl_id with numeric meta-features (no descriptors).
    meta_df: DataFrame with columns ['chembl_id','Source','Consensus_Score'].
    """
    os.makedirs(outdir, exist_ok=True)

    # Align metadata to features by chembl_id
    meta = meta_df.set_index("chembl_id").reindex(feat_df.index)

    # Drop rows with all-NaN features
    feat = feat_df.copy()
    mask_good = ~feat.isna().all(axis=1)
    feat = feat.loc[mask_good]
    meta = meta.loc[mask_good]

    # If too small, skip gracefully
    if (feat.shape[0] < 2) or (feat.shape[1] < 1):
        print(f"âš ï¸ Skipping {name}: not enough data (rows={feat.shape[0]}, cols={feat.shape[1]}).")
        return None

    # Fill NaNs columnwise (mean), then z-score
    feat_filled = feat.apply(pd.to_numeric, errors="coerce")
    feat_filled = feat_filled.fillna(feat_filled.mean())
    Z = zscore(feat_filled)

    # Row color bars
    row_colors = make_row_colors(meta["Source"].tolist(), meta["Consensus_Score"].values)

    g = sns.clustermap(
        Z, method="average", metric="euclidean", cmap="vlag",
        figsize=(8,8), row_colors=row_colors, col_cluster=True,
        xticklabels=True, yticklabels=False
    )
    g.ax_heatmap.set_title(f"{name} â€” meta-feature clustering", pad=10)
    png_path = os.path.join(outdir, f"{name}_MetaFeatures_Clustermap_600dpi.png")
    plt.savefig(png_path, dpi=DPI, bbox_inches="tight"); plt.close()

    # Save dendrogram order and z-scores with chembl_id index
    order = g.dendrogram_row.reordered_ind if hasattr(g, "dendrogram_row") else list(range(Z.shape[0]))
    ordered_ids = Z.index.to_series().iloc[order].tolist()
    pd.DataFrame({"chembl_id": ordered_ids, "order_index": range(len(order))}) \
      .to_csv(os.path.join(outdir, f"{name}_Cluster_Order.csv"), index=False)

    Z.to_csv(os.path.join(outdir, f"{name}_MetaFeatures_Zscores.csv"))
    print(f"âœ… Saved {name} clustermap:", png_path)
    return png_path

# ---------- Build feature matrices (index = chembl_id) ----------

# PubChem-only meta columns present
pub_cols = [c for c in cons.columns if any(k in c for k in
    ["Pred_pIC50_PubChem","residual_PubChem","abs_residual_PubChem","percent_error_PubChem","squared_error_PubChem"])]
feat_pub = cons.set_index("chembl_id")[pub_cols]

# MACCS-only
mac_cols = [c for c in cons.columns if any(k in c for k in
    ["Pred_pIC50_MACCS","residual_MACCS","abs_residual_MACCS","percent_error_MACCS","squared_error_MACCS"])]
feat_mac = cons.set_index("chembl_id")[mac_cols]

# CONSENSUS union (all numeric meta columns available)
meta_keys = ["Pred_pIC50_PubChem","Pred_pIC50_MACCS",
             "abs_residual_PubChem","abs_residual_MACCS",
             "percent_error_PubChem","percent_error_MACCS",
             "residual_PubChem","residual_MACCS",
             "squared_error_PubChem","squared_error_MACCS",
             "Consensus_Score"]
meta_cols = [c for c in meta_keys if c in cons.columns]
feat_all  = cons.set_index("chembl_id")[meta_cols]

# Common metadata frame (chembl_id, Source, Consensus_Score)
meta_common = cons[["chembl_id","Source","Consensus_Score"]].copy()

# ---------- Plot panels ----------
pub_png = clustermap_from_features("PubChem",       feat_pub, meta_common, PUB_DIR)
mac_png = clustermap_from_features("MACCS",         feat_mac, meta_common, MAC_DIR)
all_png = clustermap_from_features("ConsensusUnion",feat_all, meta_common, CONS_DIR)

# ==== 0) Imports & config ====
import os, pandas as pd, numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

sns.set(style="white", context="notebook", font_scale=0.9)
DPI = 600
TOPK = 30   # number of most-informative descriptors (by variance) to show

PUB_PATH = "/content/PubChem_TopHits.csv"
MAC_PATH = "/content/MACCS_TopHits.csv"
OUTDIR   = "/content/CONSENSUS_FIGS"
os.makedirs(OUTDIR, exist_ok=True)

# ==== 1) Helper: detect descriptor columns ====
META_KEYS = {
    "chembl_id","Chembl_ID","ChEMBL_ID","ChEMBL","id","ID",
    "Set","Actual pIC50","Actual_pIC50","actual_pIC50",
    "pred pIC50","Pred pIC50","pred_pIC50","Pred_pIC50",
    "residual","abs_residual","squared_error","percent_error","flag_out_of_AD",
    "Pred_pIC50_PubChem","Pred_pIC50_MACCS",
    "residual_PubChem","residual_MACCS",
    "abs_residual_PubChem","abs_residual_MACCS",
    "squared_error_PubChem","squared_error_MACCS",
    "percent_error_PubChem","percent_error_MACCS",
    "Consensus_Score","Source"
}

def find_id_col(df):
    for c in df.columns:
        lc = c.lower()
        if lc in ("chembl_id","chembl","id") or "chembl" in lc:
            return c
    # fallback: first column
    return df.columns[0]

def detect_descriptor_cols(df, expect_prefix=None):
    cols = list(df.columns)
    # 1) If a prefix was given, prefer it
    if expect_prefix is not None:
        pref = expect_prefix.lower()
        cand = [c for c in cols if c.lower().startswith(pref)]
        if len(cand) > 0:
            return cand

    # 2) Otherwise: numeric & not meta-like
    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    desc = [c for c in numeric_cols if c not in META_KEYS]
    return desc

def topk_by_variance(df_num, k=TOPK):
    v = df_num.var(axis=0)
    order = v.sort_values(ascending=False).index
    return list(order[:min(k, len(order))])

def build_clustermap(df, id_col, desc_cols, tag, outdir):
    # keep only id + desc
    work = df[[id_col] + desc_cols].copy()
    work = work.drop_duplicates(subset=[id_col]).set_index(id_col)

    # impute (per column median), then z-score
    imputer = SimpleImputer(strategy="median")
    X_imp = pd.DataFrame(imputer.fit_transform(work), index=work.index, columns=work.columns)
    scaler = StandardScaler(with_mean=True, with_std=True)
    Xz = pd.DataFrame(scaler.fit_transform(X_imp), index=X_imp.index, columns=X_imp.columns)

    # keep most-variable descriptors for readability
    keep = topk_by_variance(Xz, TOPK)
    Xz = Xz[keep]

    # clustermap
    g = sns.clustermap(
        Xz, method="average", metric="euclidean",
        cmap="RdBu_r", center=0, figsize=(10,8),
        cbar_kws={"label":"z-scored value"},
        xticklabels=True, yticklabels=True
    )
    # tidy labels (no big title; rotate x tick labels)
    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=40, ha="right")

    # save
    png = os.path.join(outdir, f"{tag}_Descriptor_Clustermap_{TOPK}bits_600dpi.png")
    plt.savefig(png, dpi=DPI, bbox_inches="tight"); plt.close()

    # also save the ordered ids and the exact descriptor set used
    ordered_idx = Xz.index if not hasattr(g, "dendrogram_row") else \
                  Xz.index.to_series().iloc[g.dendrogram_row.reordered_ind]
    pd.DataFrame({"chembl_id": ordered_idx}).to_csv(
        os.path.join(outdir, f"{tag}_RowOrder.csv"), index=False
    )
    pd.Series(keep, name="descriptor").to_csv(
        os.path.join(outdir, f"{tag}_DescriptorsUsed.csv"), index=False
    )
    print(f"âœ… Saved: {png}")
    return png

# ==== 2) Load, detect descriptors, plot ====
def run_panel(path, expect_prefix, tag):
    df = pd.read_csv(path)
    id_col = find_id_col(df)
    desc_cols = detect_descriptor_cols(df, expect_prefix=expect_prefix)

    if len(desc_cols) == 0:
        raise ValueError(
            f"No descriptor columns detected for {tag}. "
            f"Expected prefix like '{expect_prefix}*' or numeric columns not in META_KEYS."
        )

    print(f"[{tag}] ID column: {id_col}")
    print(f"[{tag}] Descriptors detected: {len(desc_cols)} (showing top {TOPK} by variance)")

    return build_clustermap(df, id_col, desc_cols, tag, OUTDIR)

pub_png = run_panel(PUB_PATH, expect_prefix="PubchemFP", tag="PubChem_TopHits")
mac_png = run_panel(MAC_PATH, expect_prefix="MACCSFP",  tag="MACCS_TopHits")

# -*- coding: utf-8 -*-
# Heatmap + clustering with SHAP-selected descriptors (PubChem & MACCS)
# Expected files:
#   /content/PubChem_TopHits.csv
#   /content/MACCS_TopHits.csv

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# --------- paths & output ----------
PUB_PATH = "/content/PubChem_TopHits.csv"
MAC_PATH = "/content/MACCS_TopHits.csv"
OUTDIR   = "/content/SHAP_CLUSTERS"
os.makedirs(OUTDIR, exist_ok=True)

# --------- SHAP-selected top-10 features (from your plots) ----------
PUBCHEM_TOP10 = [
    "PubchemFP690","PubchemFP150","PubchemFP646","PubchemFP378","PubchemFP438",
    "PubchemFP199","PubchemFP617","PubchemFP623","PubchemFP691","PubchemFP659"
]
MACCS_TOP10 = [
    "MACCSFP38","MACCSFP54","MACCSFP152","MACCSFP100","MACCSFP72",
    "MACCSFP132","MACCSFP139","MACCSFP62","MACCSFP138","MACCSFP126"
]

ID_COL  = "chembl_id"             # will label rows by this
Y_COL   = "Actual pIC50"          # not plotted, but kept for audit
YP_COL  = "pred pIC50"            # not plotted, but kept for audit

def clustermap_on_features(name, df, feat_list, outdir):
    """
    Make a clustered heatmap using the provided feature list.
    Saves a 600 DPI PNG and a CSV of the standardized matrix actually plotted.
    """
    # Keep only features that actually exist in the file (robust to casing/typos)
    cols_lower = {c.lower(): c for c in df.columns}
    present = []
    for f in feat_list:
        # match exact name first; fall back to case-insensitive
        if f in df.columns:
            present.append(f)
        elif f.lower() in cols_lower:
            present.append(cols_lower[f.lower()])
    if len(present) == 0:
        raise ValueError(f"[{name}] None of the requested features were found in the file.")

    # Build matrix (rows = compounds, cols = selected descriptors)
    # Make sure values are numeric
    X = df[present].apply(pd.to_numeric, errors="coerce").copy()
    X.index = df[ID_COL].astype(str)

    # Standardize (z-score) for fair color scaling
    Z = StandardScaler().fit_transform(X.values)
    Z_df = pd.DataFrame(Z, index=X.index, columns=X.columns)

    # Save the data that are shown in the heatmap
    csv_out = os.path.join(outdir, f"{name}_Top10_SHAP_StandardizedMatrix.csv")
    Z_df.to_csv(csv_out)

    # Plot clustered heatmap
    sns.set(context="talk", style="white")
    g = sns.clustermap(
        Z_df,
        method="average", metric="euclidean",
        cmap="vlag", center=0,
        linewidths=0.2, linecolor="white",
        figsize=(11, 8),
        cbar_kws={"label": "standardized descriptor value"}
    )

    # Axis labels & tidy ticks
    g.ax_heatmap.set_xlabel("Descriptors", fontsize=14)
    g.ax_heatmap.set_ylabel("Compounds (ChEMBL ID)", fontsize=14)
    g.ax_heatmap.set_title(f"{name} â€” SHAP-selected descriptors", pad=12, fontsize=16)
    g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=45, ha="right", fontsize=10)
    g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), fontsize=10)

    # Save high-res image
    png_out = os.path.join(outdir, f"{name}_SHAP_Clustermap_600dpi.png")
    g.fig.savefig(png_out, dpi=600, bbox_inches="tight")
    plt.close(g.fig)

    print(f"âœ“ {name}: using {len(present)}/{len(feat_list)} requested features.")
    print("  Saved:", png_out)
    print("  Data:", csv_out)
    return png_out, csv_out


# --------- Load files & sanity prints ----------
pub = pd.read_csv(PUB_PATH)
mac = pd.read_csv(MAC_PATH)

print("=== PubChem_TopHits ===")
print("Shape:", pub.shape)
print("Columns example:", list(pub.columns[:10]))
print()

print("=== MACCS_TopHits ===")
print("Shape:", mac.shape)
print("Columns example:", list(mac.columns[:10]))
print()

# --------- Make figures ----------
pub_png, pub_csv = clustermap_on_features("PubChem", pub, PUBCHEM_TOP10, OUTDIR)
mac_png, mac_csv = clustermap_on_features("MACCS", mac, MACCS_TOP10, OUTDIR)

print("\nAll done.")
print("PubChem figure:", pub_png)
print("MACCS  figure:", mac_png)

# -*- coding: utf-8 -*-
# Heatmap + clustering with SHAP-selected descriptors (PubChem & MACCS)
# Expected files:
#   /content/PubChem_TopHits.csv
#   /content/MACCS_TopHits.csv

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# --------- paths & output ----------
PUB_PATH = "/content/PubChem_TopHits.csv"
MAC_PATH = "/content/MACCS_TopHits.csv"
OUTDIR   = "/content/SHAP_CLUSTERS"
os.makedirs(OUTDIR, exist_ok=True)

# --------- SHAP-selected top-10 features (from your plots) ----------
PUBCHEM_TOP10 = [
    "PubchemFP690","PubchemFP150","PubchemFP646","PubchemFP378","PubchemFP438",
    "PubchemFP199","PubchemFP623",
    "PubchemFP698","PubchemFP338","PubchemFP690","PubchemFP699","PubchemFP712",
    "PubchemFP646","PubchemFP645","PubchemFP420","PubchemFP818"
]
MACCS_TOP10 = [
    "MACCSFP38","MACCSFP54","MACCSFP152","MACCSFP100","MACCSFP72",
    "MACCSFP132","MACCSFP139","MACCSFP62","MACCSFP138","MACCSFP84","MACCSFP146",
    "MACCSFP86","MACCSFP92","MACCSFP117","MACCSFP110","MACCSFP154"
]

ID_COL  = "chembl_id"             # will label rows by this
Y_COL   = "Actual pIC50"          # not plotted, but kept for audit
YP_COL  = "pred pIC50"            # not plotted, but kept for audit

def clustermap_on_features(name, df, feat_list, outdir):
    """
    Make a clustered heatmap using the provided feature list.
    Saves a 600 DPI PNG and a CSV of the standardized matrix actually plotted.
    """
    # Keep only features that actually exist in the file (robust to casing/typos)
    cols_lower = {c.lower(): c for c in df.columns}
    present = []
    for f in feat_list:
        # match exact name first; fall back to case-insensitive
        if f in df.columns:
            present.append(f)
        elif f.lower() in cols_lower:
            present.append(cols_lower[f.lower()])
    if len(present) == 0:
        raise ValueError(f"[{name}] None of the requested features were found in the file.")

    # Build matrix (rows = compounds, cols = selected descriptors)
    # Make sure values are numeric
    X = df[present].apply(pd.to_numeric, errors="coerce").copy()
    X.index = df[ID_COL].astype(str)

    # Standardize (z-score) for fair color scaling
    Z = StandardScaler().fit_transform(X.values)
    Z_df = pd.DataFrame(Z, index=X.index, columns=X.columns)

    # Save the data that are shown in the heatmap
    csv_out = os.path.join(outdir, f"{name}_Top10_SHAP_StandardizedMatrix.csv")
    Z_df.to_csv(csv_out)

    # Plot clustered heatmap
    sns.set(context="talk", style="white")
    g = sns.clustermap(
        Z_df,
        method="average", metric="euclidean",
        cmap="vlag", center=0,
        linewidths=0.2, linecolor="white",
        figsize=(11, 8),
        cbar_kws={"label": "standardized descriptor value"}
    )

    # Axis labels & tidy ticks
    g.ax_heatmap.set_xlabel("Descriptors", fontsize=14)
    g.ax_heatmap.set_ylabel("Compounds (ChEMBL ID)", fontsize=14)
    g.ax_heatmap.set_title(f"{name} â€” SHAP-selected descriptors", pad=12, fontsize=16)
    g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=45, ha="right", fontsize=10)
    g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), fontsize=10)

    # Save high-res image
    png_out = os.path.join(outdir, f"{name}_SHAP_Clustermap_600dpi.png")
    g.fig.savefig(png_out, dpi=600, bbox_inches="tight")
    plt.close(g.fig)

    print(f"âœ“ {name}: using {len(present)}/{len(feat_list)} requested features.")
    print("  Saved:", png_out)
    print("  Data:", csv_out)
    return png_out, csv_out


# --------- Load files & sanity prints ----------
pub = pd.read_csv(PUB_PATH)
mac = pd.read_csv(MAC_PATH)

print("=== PubChem_TopHits ===")
print("Shape:", pub.shape)
print("Columns example:", list(pub.columns[:10]))
print()

print("=== MACCS_TopHits ===")
print("Shape:", mac.shape)
print("Columns example:", list(mac.columns[:10]))
print()

# --------- Make figures ----------
pub_png, pub_csv = clustermap_on_features("PubChem", pub, PUBCHEM_TOP10, OUTDIR)
mac_png, mac_csv = clustermap_on_features("MACCS", mac, MACCS_TOP10, OUTDIR)

print("\nAll done.")
print("PubChem figure:", pub_png)
print("MACCS  figure:", mac_png)

# ============================
# Consensus clustering package
# ============================
# Inputs (update paths if needed):
PUB_PATH = "/content/PubChem_TopHits.csv"
MAC_PATH = "/content/MACCS_TopHits.csv"

# --- SHAP-selected descriptors you asked to keep ---
PUB_DESIRED = [
    "PubchemFP690","PubchemFP150","PubchemFP646","PubchemFP378","PubchemFP438",
    "PubchemFP199","PubchemFP623","PubchemFP698","PubchemFP338","PubchemFP699",
    "PubchemFP712","PubchemFP645","PubchemFP420","PubchemFP818"
]
MAC_DESIRED = [
    "MACCSFP38","MACCSFP54","MACCSFP152","MACCSFP100","MACCSFP72",
    "MACCSFP132","MACCSFP139","MACCSFP62","MACCSFP138","MACCSFP84",
    "MACCSFP146","MACCSFP86","MACCSFP92","MACCSFP117","MACCSFP110","MACCSFP154"
]

# ============================
# Code starts here
# ============================
import os, zipfile, json, textwrap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

OUTROOT = "/content/CONSENSUS_CLUSTER"
os.makedirs(OUTROOT, exist_ok=True)

def read_csv_safe(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path)

def audit(df):
    cols = df.columns.tolist()
    id_col = "chembl_id" if "chembl_id" in cols else None
    act_col = "Actual pIC50" if "Actual pIC50" in cols else None
    # permissive detection of predicted column
    pred_candidates = ["pred pIC50","Pred pIC50","pred_pIC50","predpic50","Predicted pIC50"]
    pred_col = next((c for c in pred_candidates if c in cols), None)
    return id_col, act_col, pred_col

def zscore_df(df):
    if df.shape[1] == 0:
        return df.copy()
    Z = StandardScaler().fit_transform(df.values)
    return pd.DataFrame(Z, index=df.index, columns=df.columns)

def tanimoto_bit(a, b):
    inter = np.sum((a==1) & (b==1))
    ua = np.sum(a==1); ub = np.sum(b==1)
    denom = ua + ub - inter
    return 0.0 if denom <= 0 else inter/denom

def tanimoto_matrix(Xbin):
    n = Xbin.shape[0]
    M = np.zeros((n,n), dtype=float)
    for i in range(n):
        M[i,i] = 1.0
        for j in range(i+1, n):
            s = tanimoto_bit(Xbin[i], Xbin[j])
            M[i,j] = M[j,i] = s
    return M

def clustermap_features(X, ids, title, outpath):
    Z = zscore_df(X)
    sns.set(context="notebook", font_scale=0.95)
    g = sns.clustermap(
        Z, cmap="vlag", figsize=(12, 9),
        metric="euclidean", method="average",
        cbar_kws={"label":"Standardized descriptor value"},
        xticklabels=True, yticklabels=ids
    )
    g.ax_heatmap.set_xlabel("Descriptors")
    g.ax_heatmap.set_ylabel("Compounds (ChEMBL ID)")
    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=45, ha="right")
    plt.title(title, y=1.05)
    plt.savefig(outpath, dpi=600, bbox_inches="tight")
    plt.close()

def clustermap_tanimoto(M, ids, title, outpath):
    # cluster and plot a similarity matrix
    sns.set(context="notebook", font_scale=0.95)
    dfM = pd.DataFrame(M, index=ids, columns=ids)
    g = sns.clustermap(
        dfM, cmap="viridis", figsize=(10, 8),
        metric="euclidean", method="average",
        cbar_kws={"label":"Tanimoto similarity"},
        xticklabels=True, yticklabels=True
    )
    g.ax_heatmap.set_xlabel("Compounds (ChEMBL ID)")
    g.ax_heatmap.set_ylabel("Compounds (ChEMBL ID)")
    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=45, ha="right")
    plt.title(title, y=1.05)
    plt.savefig(outpath, dpi=600, bbox_inches="tight")
    plt.close()

def rank_consensus(pub_pred, mac_pred):
    # rank-averaged consensus (robust, scale-free)
    # convert to z-scores then take mean
    zp = (pub_pred - pub_pred.mean())/pub_pred.std(ddof=0)
    zm = (mac_pred - mac_pred.mean())/mac_pred.std(ddof=0)
    return (zp + zm) / 2.0

# --- Load & audit ---
pub_df = read_csv_safe(PUB_PATH)
mac_df = read_csv_safe(MAC_PATH)

pub_id, pub_act, pub_pred = audit(pub_df)
mac_id, mac_act, mac_pred = audit(mac_df)

assert pub_id and pub_act and pub_pred, "Check PubChem_TopHits columns (chembl_id / Actual pIC50 / pred pIC50)."
assert mac_id and mac_act and mac_pred, "Check MACCS_TopHits columns (chembl_id / Actual pIC50 / pred pIC50)."

# --- keep only common compounds ---
common_ids = sorted(set(pub_df[pub_id]).intersection(set(mac_df[mac_id])))
pub = pub_df[pub_df[pub_id].isin(common_ids)].copy().sort_values(pub_id).reset_index(drop=True)
mac = mac_df[mac_df[mac_id].isin(common_ids)].copy().sort_values(mac_id).reset_index(drop=True)
ids = pub[pub_id].tolist()

# --- keep only the descriptors you asked for (and that actually exist) ---
pub_keep = [c for c in PUB_DESIRED if c in pub.columns]
mac_keep = [c for c in MAC_DESIRED if c in mac.columns]

# report coverage
print(f"Common compounds: {len(ids)}")
print(f"PubChem descriptors found: {len(pub_keep)} / {len(PUB_DESIRED)}")
print(f"MACCS descriptors found:   {len(mac_keep)} / {len(MAC_DESIRED)}")

# --- Feature matrices (numeric, NaN->0) ---
pub_feat = pub[pub_keep].apply(pd.to_numeric, errors="coerce").fillna(0.0)
mac_feat = mac[mac_keep].apply(pd.to_numeric, errors="coerce").fillna(0.0)

# --- Consensus score ---
cons = rank_consensus(pub[pub_pred].astype(float), mac[mac_pred].astype(float))
cons_df = pd.DataFrame({
    "chembl_id": ids,
    "Pred_pIC50_PubChem": pub[pub_pred].astype(float).values,
    "Pred_pIC50_MACCS":   mac[mac_pred].astype(float).values,
    "Consensus_Score":    cons.values
})
cons_df_path = os.path.join(OUTROOT, "Consensus_Score.csv")
cons_df.to_csv(cons_df_path, index=False)

# --- Descriptor heatmaps ---
clustermap_features(pub_feat.set_index(pd.Index(ids)), ids,
                    "PubChem â€” selected descriptors",
                    os.path.join(OUTROOT, "Heatmap_PubChem_Selected_600dpi.png"))

clustermap_features(mac_feat.set_index(pd.Index(ids)), ids,
                    "MACCS â€” selected descriptors",
                    os.path.join(OUTROOT, "Heatmap_MACCS_Selected_600dpi.png"))

comb_feat = pd.concat([pub_feat, mac_feat], axis=1)
clustermap_features(comb_feat.set_index(pd.Index(ids)), ids,
                    "Combined â€” PubChem + MACCS (selected)",
                    os.path.join(OUTROOT, "Heatmap_Combined_Selected_600dpi.png"))

# --- Tanimoto (threshold 0.5 -> binarize; works for 0/1 fingerprints or 0/1/float) ---
pub_bin = (pub_feat.values >= 0.5).astype(int)
mac_bin = (mac_feat.values >= 0.5).astype(int)
comb_bin = np.concatenate([pub_bin, mac_bin], axis=1)

pub_tani = tanimoto_matrix(pub_bin)
mac_tani = tanimoto_matrix(mac_bin)
comb_tani = tanimoto_matrix(comb_bin)

clustermap_tanimoto(pub_tani, ids, "PubChem â€” Tanimoto similarity (selected)",
                    os.path.join(OUTROOT, "Tanimoto_PubChem_Selected_600dpi.png"))
clustermap_tanimoto(mac_tani, ids, "MACCS â€” Tanimoto similarity (selected)",
                    os.path.join(OUTROOT, "Tanimoto_MACCS_Selected_600dpi.png"))
clustermap_tanimoto(comb_tani, ids, "Combined â€” Tanimoto similarity (selected)",
                    os.path.join(OUTROOT, "Tanimoto_Combined_Selected_600dpi.png"))

# --- Save a compact README and ZIP everything ---
readme = f"""
Generated in: {OUTROOT}

Included:
  â€¢ Consensus_Score.csv â€” chembl_id + predicted pIC50s and rank-averaged Consensus_Score.
  â€¢ Heatmap_PubChem_Selected_600dpi.png â€” z-scored clustering of selected PubChem bits.
  â€¢ Heatmap_MACCS_Selected_600dpi.png â€” z-scored clustering of selected MACCS bits.
  â€¢ Heatmap_Combined_Selected_600dpi.png â€” concatenated PubChem+MACCS heatmap.
  â€¢ Tanimoto_*_Selected_600dpi.png â€” clustered similarity maps from binarized descriptors.

Counts:
  â€¢ Compounds: {len(ids)}
  â€¢ PubChem descriptors used: {len(pub_keep)} / {len(PUB_DESIRED)}
  â€¢ MACCS descriptors used:   {len(mac_keep)} / {len(MAC_DESIRED)}

Notes:
  â€¢ Axes are labeled; xtick labels are rotated to avoid overlap.
  â€¢ All figures saved at 600 DPI for publication quality.
"""
with open(os.path.join(OUTROOT, "README.txt"), "w") as f:
    f.write(textwrap.dedent(readme).strip()+"\n")

zip_path = "/content/CONSENSUS_CLUSTER_RESULTS.zip"
with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
    for fn in os.listdir(OUTROOT):
        zf.write(os.path.join(OUTROOT, fn), arcname=fn)

print("ZIP ready:", zip_path)

# ================================
# SHAP-selected descriptor bit tables (0/1)
# ================================
PUB_PATH = "/content/PubChem_TopHits.csv"
MAC_PATH = "/content/MACCS_TopHits.csv"

PUB_DESIRED = [
    "PubchemFP690","PubchemFP150","PubchemFP646","PubchemFP378","PubchemFP438",
    "PubchemFP199","PubchemFP623","PubchemFP698","PubchemFP338","PubchemFP699",
    "PubchemFP712","PubchemFP645","PubchemFP420","PubchemFP818"
]
MAC_DESIRED = [
    "MACCSFP38","MACCSFP54","MACCSFP152","MACCSFP100","MACCSFP72",
    "MACCSFP132","MACCSFP139","MACCSFP62","MACCSFP138","MACCSFP84",
    "MACCSFP146","MACCSFP86","MACCSFP92","MACCSFP117","MACCSFP110","MACCSFP154"
]

BIT_THRESHOLD = 0.5      # >= 0.5 -> 1, else 0 (change if your files are strictly 0/1)

import os, numpy as np, pandas as pd, textwrap, zipfile
from sklearn.preprocessing import binarize

OUTDIR = "/content/SHAP_BITS"
os.makedirs(OUTDIR, exist_ok=True)

def load_df(path):
    if not os.path.exists(path):
        raise FileNotFoundError(path)
    return pd.read_csv(path)

def find_cols(df, wanted):
    present = [c for c in wanted if c in df.columns]
    missing = [c for c in wanted if c not in df.columns]
    return present, missing

def mk_bits_table(df, id_col, keep_cols, prefix=None):
    X = df[keep_cols].apply(pd.to_numeric, errors="coerce").fillna(0.0).values
    # binarize by threshold (works for 0/1 or float fingerprints)
    B = (X >= BIT_THRESHOLD).astype(int)
    cols = keep_cols if prefix is None else [f"{prefix}:{c}" for c in keep_cols]
    return pd.DataFrame(B, index=df[id_col].values, columns=cols)

def tidy_long(bits_df, id_name="chembl_id"):
    return (bits_df
            .reset_index(names=[id_name])
            .melt(id_vars=[id_name], var_name="descriptor", value_name="bit"))

# -------- load & prepare --------
pub = load_df(PUB_PATH)
mac = load_df(MAC_PATH)

# detect the ID column
id_col = "chembl_id" if "chembl_id" in pub.columns else None
assert id_col and id_col in mac.columns, "chembl_id column is required in both files."

# keep SHAP-listed descriptors that actually exist
pub_keep, pub_missing = find_cols(pub, PUB_DESIRED)
mac_keep, mac_missing = find_cols(mac, MAC_DESIRED)

print(f"PubChem: using {len(pub_keep)} descriptors, missing {len(pub_missing)} -> {pub_missing[:5]}")
print(f"MACCS  : using {len(mac_keep)} descriptors, missing {len(mac_missing)} -> {mac_missing[:5]}")

# -------- per-file binary tables --------
pub_bits = mk_bits_table(pub, id_col, pub_keep, prefix="PubChem")
mac_bits = mk_bits_table(mac, id_col, mac_keep, prefix="MACCS")

pub_bits_path = os.path.join(OUTDIR, "PubChem_SHAP_bits_binary.csv")
mac_bits_path = os.path.join(OUTDIR, "MACCS_SHAP_bits_binary.csv")
pub_bits.to_csv(pub_bits_path)
mac_bits.to_csv(mac_bits_path)

# per-compound counts & per-descriptor prevalence (per file)
pub_counts = pub_bits.sum(axis=1).rename("PubChem_bit_count").reset_index()
mac_counts = mac_bits.sum(axis=1).rename("MACCS_bit_count").reset_index()

pub_prev = (pub_bits.sum(axis=0)
            .rename("n_compounds_with_bit")
            .to_frame()
            .assign(prevalence_pct=lambda d: 100*d["n_compounds_with_bit"]/pub_bits.shape[0]))
mac_prev = (mac_bits.sum(axis=0)
            .rename("n_compounds_with_bit")
            .to_frame()
            .assign(prevalence_pct=lambda d: 100*d["n_compounds_with_bit"]/mac_bits.shape[0]))

pub_counts.to_csv(os.path.join(OUTDIR, "PubChem_per_compound_bit_counts.csv"), index=False)
mac_counts.to_csv(os.path.join(OUTDIR, "MACCS_per_compound_bit_counts.csv"), index=False)
pub_prev.to_csv(os.path.join(OUTDIR, "PubChem_descriptor_prevalence.csv"))
mac_prev.to_csv(os.path.join(OUTDIR, "MACCS_descriptor_prevalence.csv"))

# tidy long versions (nice for plotting/joins)
tidy_pub = tidy_long(pub_bits)
tidy_mac = tidy_long(mac_bits)
tidy_pub.to_csv(os.path.join(OUTDIR, "PubChem_SHAP_bits_long.csv"), index=False)
tidy_mac.to_csv(os.path.join(OUTDIR, "MACCS_SHAP_bits_long.csv"), index=False)

# -------- combined table only for common compounds --------
common_ids = sorted(set(pub[id_col]).intersection(set(mac[id_col])))
pub_common = pub_bits.loc[common_ids]
mac_common = mac_bits.loc[common_ids]
comb_bits = pd.concat([pub_common, mac_common], axis=1)
comb_bits_path = os.path.join(OUTDIR, "Combined_SHAP_bits_binary_common.csv")
comb_bits.to_csv(comb_bits_path)

# also provide a compact summary per compound across both sets
comb_counts = (comb_bits
               .sum(axis=1)
               .rename("Total_bit_count")
               .reset_index(names=[id_col]))
comb_counts.to_csv(os.path.join(OUTDIR, "Combined_per_compound_total_bits.csv"), index=False)

# -------- zip everything for download --------
zip_path = "/content/SHAP_BITS_TABLES.zip"
with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
    for fn in os.listdir(OUTDIR):
        zf.write(os.path.join(OUTDIR, fn), arcname=fn)

print("Done.")
print("Folder:", OUTDIR)
print("ZIP   :", zip_path)

# -------- combined table only for common compounds --------
common_ids = sorted(set(pub[id_col]).intersection(set(mac[id_col])))
pub_common = pub_bits.loc[common_ids]
mac_common = mac_bits.loc[common_ids]
comb_bits = pd.concat([pub_common, mac_common], axis=1)
comb_bits_path = os.path.join(OUTDIR, "Combined_SHAP_bits_binary_common.csv")
comb_bits.to_csv(comb_bits_path)

# also provide a compact summary per compound across both sets
comb_counts = comb_bits.sum(axis=1).rename("Total_bit_count").reset_index()
comb_counts.columns = [id_col, "Total_bit_count"]
comb_counts.to_csv(os.path.join(OUTDIR, "Combined_per_compound_total_bits.csv"), index=False)

# -------- zip everything for download --------
zip_path = "/content/SHAP_BITS_TABLES.zip"
with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
    for fn in os.listdir(OUTDIR):
        zf.write(os.path.join(OUTDIR, fn), arcname=fn)

print("Done.")
print("Folder:", OUTDIR)
print("ZIP   :", zip_path)